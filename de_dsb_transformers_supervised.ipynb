{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "de_dsb_transformers_supervised.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# create a seperate folder to store everything\n",
        "# !mkdir wmt\n",
        "# %cd wmt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:45.876120Z",
          "iopub.execute_input": "2022-07-24T19:01:45.876582Z",
          "iopub.status.idle": "2022-07-24T19:01:46.194317Z",
          "shell.execute_reply.started": "2022-07-24T19:01:45.876499Z",
          "shell.execute_reply": "2022-07-24T19:01:46.192944Z"
        },
        "trusted": true,
        "id": "J9o_gWJIdwcc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BFmSadnVoSa",
        "outputId": "ec357a59-9651-41cc-f26a-fd9f0cdb26dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tree\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:47.842831Z",
          "iopub.execute_input": "2022-07-24T19:01:47.843475Z",
          "iopub.status.idle": "2022-07-24T19:03:13.957220Z",
          "shell.execute_reply.started": "2022-07-24T19:01:47.843443Z",
          "shell.execute_reply": "2022-07-24T19:03:13.955881Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sULkb42Jdwci",
        "outputId": "ffaa2ed4-fd06-4740-83a0-84949de83238"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (145 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 27.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 60.2 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 72.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 63.3 MB/s \n",
            "\u001b[?25hCollecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=43632e6f4b2276d2a7000697102a56a732b697dd7aac20c8f08afe4efd48e70e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, portalocker, morfessor, colorama, tensorboardX, sacremoses, sacrebleu, mock, indic-nlp-library\n",
            "Successfully installed colorama-0.4.5 indic-nlp-library-0.81 mock-4.0.3 morfessor-2.0.6 portalocker-2.5.1 sacrebleu-2.2.0 sacremoses-0.0.53 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0 tensorboardX-2.5.1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 32239, done.\u001b[K\n",
            "remote: Total 32239 (delta 0), reused 0 (delta 0), pack-reused 32239\u001b[K\n",
            "Receiving objects: 100% (32239/32239), 22.42 MiB | 30.99 MiB/s, done.\n",
            "Resolving deltas: 100% (23642/23642), done.\n",
            "/content/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.32)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 25.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.1+cu113)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 63.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2.2.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.1+cu113)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.9.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 75.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.8.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=0be5ce977f2535f24e5c24fc85f90fb0541d2bdb77fb4ab4686465322a7d92b0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 fairseq hydra-core-1.0.7 omegaconf-2.0.6\n",
            "/content\n",
            "/env/python\n",
            "/env/python:/content/fairseq/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:13.960095Z",
          "iopub.execute_input": "2022-07-24T19:03:13.960531Z",
          "iopub.status.idle": "2022-07-24T19:03:14.252882Z",
          "shell.execute_reply.started": "2022-07-24T19:03:13.960500Z",
          "shell.execute_reply": "2022-07-24T19:03:14.251592Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XleJnPzidwck",
        "outputId": "332c5a0b-7d36-440e-d38c-ebfae00b9313"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U --no-cache-dir gdown --pre"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:14.254120Z",
          "iopub.execute_input": "2022-07-24T19:03:14.254498Z",
          "iopub.status.idle": "2022-07-24T19:03:36.192512Z",
          "shell.execute_reply.started": "2022-07-24T19:03:14.254462Z",
          "shell.execute_reply": "2022-07-24T19:03:36.191399Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIuDbtOgdwcm",
        "outputId": "75d63dd5-b49a-4594-9dd0-9fe7675829b8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=70734d313abf240cab11cf1af139a544051219e015b679dbb96b320ac07f08ac\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-in384yg4/wheels/3d/ec/b0/a96d1d126183f98570a785e6bf8789fca559853a9260e928e1\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNjo_Q8_IlXz",
        "outputId": "85669407-e40a-4b23-c5ad-e8bd63fe67cf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir drive\n",
        "%cd drive"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.194917Z",
          "iopub.execute_input": "2022-07-24T19:03:36.195189Z",
          "iopub.status.idle": "2022-07-24T19:03:36.494103Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.195162Z",
          "shell.execute_reply": "2022-07-24T19:03:36.492910Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AopzC4DBdwcn",
        "outputId": "2747b29a-08cf-4b5c-83e6-e40658337ee5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 10E82WQI8AFC0YygAQL6ZyYUvDiuLk2Lp --folder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.495274Z",
          "iopub.execute_input": "2022-07-24T19:03:36.495519Z",
          "iopub.status.idle": "2022-07-24T19:03:47.662284Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.495496Z",
          "shell.execute_reply": "2022-07-24T19:03:47.661258Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa1K1LT9dwcn",
        "outputId": "9828a621-cd6a-45e1-abc9-aca1d3ef7b6c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Retrieving folder list\n",
            "Retrieving folder 1_Yikicq46ZMM1U71iEX6SCcizcIEXG_k monolingual\n",
            "Processing file 1YHpVt_mvDrtzOh8d5CeSFBXkINV9Bd3z 8815_DSB_wikipedia_2021.txt\n",
            "Processing file 1itDHrLXQU5YzSFEZEjKfGgkcwFM4lnT6 66408_DSB_monolingual.txt\n",
            "Processing file 1eqidIw9muaTEXXvEd8D9sL2DC9R4nAbY de.tok\n",
            "Processing file 12rX2uWXDG-PD0V4GmO5eEvLNeBCAwLxT mono.dsb\n",
            "Retrieving folder 1q0fbh2GXp4wNTYNM_leJOHqUK4ePNNcz train\n",
            "Processing file 1yFTGUXguH-DootoOSXSbh0XBuQ4G-xgH train.de\n",
            "Processing file 1frDMVTIizN-dQAfmOMhnE82b3DG4YIh0 train.dsb\n",
            "Retrieving folder 1RgvNWMZKpa_0YZDTDWD3WsRFRUqDqq3O valid\n",
            "Processing file 1zm6cCUoUCiYhAoZKmM3Xd7YH24M9GMWU valid.de\n",
            "Processing file 1Q5Un8CyV0wCpyEta_kG5K9w438bJnsOw valid.dsb\n",
            "Retrieving folder 1PUrRdk1KRu2xEzNw-rMWgx1ST3GquQ9J valid2\n",
            "Processing file 1c1QXYUwCUKLF-bHGmZ7ddqM45pCz51Hn valid2.de\n",
            "Processing file 15GR-6QX_I_tPZ5KXit6174YYgyRwgVYo valid2.dsb\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1YHpVt_mvDrtzOh8d5CeSFBXkINV9Bd3z\n",
            "To: /content/drive/dsb-de/monolingual/8815_DSB_wikipedia_2021.txt\n",
            "100% 735k/735k [00:00<00:00, 160MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1itDHrLXQU5YzSFEZEjKfGgkcwFM4lnT6\n",
            "To: /content/drive/dsb-de/monolingual/66408_DSB_monolingual.txt\n",
            "100% 6.32M/6.32M [00:00<00:00, 61.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1eqidIw9muaTEXXvEd8D9sL2DC9R4nAbY\n",
            "To: /content/drive/dsb-de/monolingual/de.tok\n",
            "100% 7.37M/7.37M [00:00<00:00, 117MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12rX2uWXDG-PD0V4GmO5eEvLNeBCAwLxT\n",
            "To: /content/drive/dsb-de/monolingual/mono.dsb\n",
            "100% 13.7M/13.7M [00:00<00:00, 105MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1yFTGUXguH-DootoOSXSbh0XBuQ4G-xgH\n",
            "To: /content/drive/dsb-de/train/train.de\n",
            "100% 3.68M/3.68M [00:00<00:00, 300MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1frDMVTIizN-dQAfmOMhnE82b3DG4YIh0\n",
            "To: /content/drive/dsb-de/train/train.dsb\n",
            "100% 3.47M/3.47M [00:00<00:00, 28.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zm6cCUoUCiYhAoZKmM3Xd7YH24M9GMWU\n",
            "To: /content/drive/dsb-de/valid/valid.de\n",
            "100% 64.5k/64.5k [00:00<00:00, 100MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Q5Un8CyV0wCpyEta_kG5K9w438bJnsOw\n",
            "To: /content/drive/dsb-de/valid/valid.dsb\n",
            "100% 58.9k/58.9k [00:00<00:00, 69.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1c1QXYUwCUKLF-bHGmZ7ddqM45pCz51Hn\n",
            "To: /content/drive/dsb-de/valid2/valid2.de\n",
            "100% 60.4k/60.4k [00:00<00:00, 89.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15GR-6QX_I_tPZ5KXit6174YYgyRwgVYo\n",
            "To: /content/drive/dsb-de/valid2/valid2.dsb\n",
            "100% 56.1k/56.1k [00:00<00:00, 95.0MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUdiQyJdJBdt",
        "outputId": "ea633e27-e35f-4ea0-abd4-6e5fb64a9a0a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:47.663751Z",
          "iopub.execute_input": "2022-07-24T19:03:47.664065Z",
          "iopub.status.idle": "2022-07-24T19:03:47.671365Z",
          "shell.execute_reply.started": "2022-07-24T19:03:47.664036Z",
          "shell.execute_reply": "2022-07-24T19:03:47.670044Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXt8nnHHdwco",
        "outputId": "1523df9a-1b9c-4180-cf00-f595f4abbe16"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H751-LW3XII0",
        "outputId": "a107fab4-faed-4b6e-e00e-f69d7cce9ecb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:04:11.972219Z",
          "iopub.execute_input": "2022-07-24T19:04:11.972913Z",
          "iopub.status.idle": "2022-07-24T19:04:12.256665Z",
          "shell.execute_reply.started": "2022-07-24T19:04:11.972888Z",
          "shell.execute_reply": "2022-07-24T19:04:12.255254Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ihwjB6Gdwcp",
        "outputId": "79f149b0-c843-4ea8-fd85-ba0e744ad6d4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdsb-de\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8F3ZpFtX6Vo",
        "outputId": "cce900bf-04d6-4179-a22c-d425d9d44974"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-preprocess \\\n",
        "    --source-lang de --target-lang dsb \\\n",
        "    --trainpref /content/drive/dsb-de/train/train --validpref /content/drive/dsb-de/valid/valid \\\n",
        "    --destdir data-bin/wmt_de_dsb --thresholdtgt 0 --thresholdsrc 0 \\\n",
        "    --workers 20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:14:48.909980Z",
          "iopub.execute_input": "2022-07-24T19:14:48.910354Z",
          "iopub.status.idle": "2022-07-24T19:15:18.024931Z",
          "shell.execute_reply.started": "2022-07-24T19:14:48.910330Z",
          "shell.execute_reply": "2022-07-24T19:15:18.024044Z"
        },
        "trusted": true,
        "id": "XxPJ-2-zdwcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00ae2f44-c2f0-4b88-f959-ffe53151e503"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 21:04:46 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wmt_de_dsb', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='de', srcdict=None, suppress_crashes=False, target_lang='dsb', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='/content/drive/dsb-de/train/train', use_plasma_view=False, user_dir=None, validpref='/content/drive/dsb-de/valid/valid', wandb_project=None, workers=20)\n",
            "2022-08-28 21:04:51 | INFO | fairseq_cli.preprocess | [de] Dictionary: 62320 types\n",
            "2022-08-28 21:04:56 | INFO | fairseq_cli.preprocess | [de] /content/drive/dsb-de/train/train.de: 40194 sents, 555037 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-28 21:04:56 | INFO | fairseq_cli.preprocess | [de] Dictionary: 62320 types\n",
            "2022-08-28 21:04:57 | INFO | fairseq_cli.preprocess | [de] /content/drive/dsb-de/valid/valid.de: 1353 sents, 11205 tokens, 14.6% replaced (by <unk>)\n",
            "2022-08-28 21:04:57 | INFO | fairseq_cli.preprocess | [dsb] Dictionary: 80848 types\n",
            "2022-08-28 21:05:03 | INFO | fairseq_cli.preprocess | [dsb] /content/drive/dsb-de/train/train.dsb: 40194 sents, 508703 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-28 21:05:03 | INFO | fairseq_cli.preprocess | [dsb] Dictionary: 80848 types\n",
            "2022-08-28 21:05:04 | INFO | fairseq_cli.preprocess | [dsb] /content/drive/dsb-de/valid/valid.dsb: 1353 sents, 9777 tokens, 21.9% replaced (by <unk>)\n",
            "2022-08-28 21:05:04 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt_de_dsb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uol2ncymJWFr",
        "outputId": "34893749-b296-49bf-ce5b-fe92c0a08134"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 27.5 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 49.1 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.4 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 79.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 80.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=bdfd21435379983086700ad4f7cb4ba0beb9bde6fe6d4d320b6561c241ef3896\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"wmt2022_de-dsb_transformers_supervised\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:18.026892Z",
          "iopub.execute_input": "2022-07-24T19:15:18.027225Z",
          "iopub.status.idle": "2022-07-24T19:15:26.733355Z",
          "shell.execute_reply.started": "2022-07-24T19:15:18.027200Z",
          "shell.execute_reply": "2022-07-24T19:15:26.732531Z"
        },
        "trusted": true,
        "id": "hoUXe8Kndwcr",
        "outputId": "f979e9ea-3f77-4338-b730-77714d8463e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220828_210540-2ir1543v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/rahul2023_usa/wmt2022_de-dsb_transformers_supervised/runs/2ir1543v\" target=\"_blank\">gallant-lion-1</a></strong> to <a href=\"https://wandb.ai/rahul2023_usa/wmt2022_de-dsb_transformers_supervised\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rahul2023_usa/wmt2022_de-dsb_transformers_supervised/runs/2ir1543v?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f2c6a73e190>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Finetuning the model\n",
        "\n",
        "# # pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
        "\n",
        "\n",
        "# # some notable args:\n",
        "# # --max-update=1000     -> for this example, to demonstrate how to finetune we are only training for 1000 steps. You should increase this when finetuning\n",
        "# # --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# # --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# # --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# # --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\n",
        "# # --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\n",
        "# # --max_tokns           -> this is max tokens per batch\n",
        "\n",
        "\n",
        "# !( fairseq-train data-bin/wmt_dsb_de \\\n",
        "# --max-source-positions=210 \\\n",
        "# --max-target-positions=210 \\\n",
        "# --max-update=1000 \\\n",
        "# --save-interval=1 \\\n",
        "# --arch=transformer \\\n",
        "# --criterion=label_smoothed_cross_entropy \\\n",
        "# --source-lang=dsb \\\n",
        "# --lr-scheduler=inverse_sqrt \\\n",
        "# --target-lang=de \\\n",
        "# --label-smoothing=0.1 \\\n",
        "# --optimizer adam \\\n",
        "# --adam-betas \"(0.9, 0.98)\" \\\n",
        "# --clip-norm 1.0 \\\n",
        "# --warmup-init-lr 1e-07 \\\n",
        "# --warmup-updates 4000 \\\n",
        "# --dropout 0.2 \\\n",
        "# --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "# --save-dir checkpoints/model \\\n",
        "# --keep-last-epochs 5 \\\n",
        "# --patience 5 \\\n",
        "# --skip-invalid-size-inputs-valid-test \\\n",
        "# --fp16 \\\n",
        "# --update-freq=2 \\\n",
        "# --distributed-world-size 1 \\\n",
        "# --max-tokens 1024 \\\n",
        "# --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "# --lr 5e-4 \\\n",
        "# --reset-lr-scheduler \\\n",
        "# --reset-meters \\\n",
        "# --reset-dataloader \\\n",
        "# --reset-optimizer \\\n",
        "# --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w4cGb93Bdwct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!( fairseq-train data-bin/wmt_de_dsb \\\n",
        "    --arch transformer \\\n",
        "    --max-epoch=50 \\\n",
        "    --criterion=label_smoothed_cross_entropy \\\n",
        "    --source-lang=de \\\n",
        "    --lr-scheduler=inverse_sqrt \\\n",
        "    --target-lang=dsb \\\n",
        "    --label-smoothing=0.1 \\\n",
        "    --optimizer adam \\\n",
        "    --adam-betas \"(0.9, 0.98)\" \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --dropout 0.2 \\\n",
        "    --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "    --wandb-project 'wmt2022_de-dsb_transformers_supervised' \\\n",
        "    --save-dir checkpoints/model \\\n",
        "    --keep-last-epochs 5 \\\n",
        "    --fp16 \\\n",
        "    --update-freq=2 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --lr 5e-4 \\\n",
        "    --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "    --reset-lr-scheduler \\\n",
        "    --reset-meters \\\n",
        "    --reset-dataloader \\\n",
        "    --reset-optimizer \\\n",
        "    --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:42.672724Z",
          "iopub.execute_input": "2022-07-24T19:15:42.673143Z",
          "iopub.status.idle": "2022-07-24T19:16:00.902554Z",
          "shell.execute_reply.started": "2022-07-24T19:15:42.673112Z",
          "shell.execute_reply": "2022-07-24T19:16:00.901385Z"
        },
        "trusted": true,
        "id": "U2IouMGudwcv",
        "outputId": "599effec-1f73-4ed3-df1f-36fbfbdfabe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 21:07:36 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../../../tmp/tensorboard-wandb', 'wandb_project': 'wmt2022_de-dsb_transformers_supervised', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/model', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wmt_de_dsb', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=True, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=50, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='checkpoints/model', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='dsb', task='translation', tensorboard_logdir='../../../tmp/tensorboard-wandb', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='wmt2022_de-dsb_transformers_supervised', warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/wmt_de_dsb', 'source_lang': 'de', 'target_lang': 'dsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 21:07:36 | INFO | fairseq.tasks.translation | [de] dictionary: 62320 types\n",
            "2022-08-28 21:07:36 | INFO | fairseq.tasks.translation | [dsb] dictionary: 80848 types\n",
            "2022-08-28 21:07:38 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(62320, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(80848, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=80848, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-08-28 21:07:38 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-08-28 21:07:38 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-08-28 21:07:38 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-08-28 21:07:38 | INFO | fairseq_cli.train | num. shared model params: 158,834,688 (num. trained: 158,834,688)\n",
            "2022-08-28 21:07:38 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-08-28 21:07:38 | INFO | fairseq.data.data_utils | loaded 1,353 examples from: data-bin/wmt_de_dsb/valid.de-dsb.de\n",
            "2022-08-28 21:07:38 | INFO | fairseq.data.data_utils | loaded 1,353 examples from: data-bin/wmt_de_dsb/valid.de-dsb.dsb\n",
            "2022-08-28 21:07:38 | INFO | fairseq.tasks.translation | data-bin/wmt_de_dsb valid de-dsb 1353 examples\n",
            "2022-08-28 21:07:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-28 21:07:42 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-08-28 21:07:42 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-28 21:07:42 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-08-28 21:07:42 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-08-28 21:07:42 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/model/checkpoint_last.pt\n",
            "2022-08-28 21:07:42 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/model/checkpoint_last.pt\n",
            "2022-08-28 21:07:42 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-08-28 21:07:42 | INFO | fairseq.data.data_utils | loaded 40,194 examples from: data-bin/wmt_de_dsb/train.de-dsb.de\n",
            "2022-08-28 21:07:42 | INFO | fairseq.data.data_utils | loaded 40,194 examples from: data-bin/wmt_de_dsb/train.de-dsb.dsb\n",
            "2022-08-28 21:07:42 | INFO | fairseq.tasks.translation | data-bin/wmt_de_dsb train de-dsb 40194 examples\n",
            "2022-08-28 21:07:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 001:   0% 0/75 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul2023_usa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220828_210742-2t4hq1kv\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_de-dsb_transformers_supervised\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_de-dsb_transformers_supervised/runs/2t4hq1kv\u001b[0m\n",
            "2022-08-28 21:07:43 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-08-28 21:07:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2022-08-28 21:07:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   1% 1/75 [00:03<04:38,  3.77s/it]2022-08-28 21:07:46 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:  99% 74/75 [00:34<00:00,  2.38it/s]2022-08-28 21:08:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  17% 1/6 [00:00<00:04,  1.07it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 2/6 [00:01<00:02,  1.34it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 3/6 [00:02<00:01,  1.57it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 4/6 [00:02<00:01,  1.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  83% 5/6 [00:02<00:00,  2.54it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:08:22 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-08-28 21:08:22 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.285 | nll_loss 13.962 | ppl 15963.4 | bleu 0 | wps 4628.4 | wpb 1629.5 | bsz 225.5 | num_updates 73\n",
            "2022-08-28 21:08:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 73 updates\n",
            "2022-08-28 21:08:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-28 21:08:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-28 21:08:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint1.pt (epoch 1 @ 73 updates, score 14.285) (writing took 27.97694473499996 seconds)\n",
            "2022-08-28 21:08:53 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-08-28 21:08:53 | INFO | train | epoch 001 | loss 15.902 | nll_loss 15.773 | ppl 55998.3 | wps 7416.5 | ups 1.1 | wpb 6759.8 | bsz 526.2 | num_updates 73 | lr 9.125e-06 | gnorm 3.974 | loss_scale 32 | train_wall 34 | gb_free 7.6 | wall 70\n",
            "2022-08-28 21:08:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 002:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:08:53 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-08-28 21:08:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 74/75 [00:31<00:00,  2.42it/s, loss=15.576, nll_loss=15.408, ppl=43493, wps=8719.9, ups=1.28, wpb=6814.6, bsz=537.5, num_updates=100, lr=1.25e-05, gnorm=3.399, loss_scale=32, train_wall=45, gb_free=7.6, wall=82]2022-08-28 21:09:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  17% 1/6 [00:00<00:04,  1.24it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 2/6 [00:01<00:02,  1.39it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 3/6 [00:02<00:01,  1.57it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 4/6 [00:02<00:01,  1.55it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  83% 5/6 [00:02<00:00,  2.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:09:27 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.657 | nll_loss 13.26 | ppl 9811.73 | bleu 0.06 | wps 3758.4 | wpb 1629.5 | bsz 225.5 | num_updates 148 | best_loss 13.657\n",
            "2022-08-28 21:09:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 148 updates\n",
            "2022-08-28 21:09:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-28 21:09:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-28 21:09:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint2.pt (epoch 2 @ 148 updates, score 13.657) (writing took 28.869815130000006 seconds)\n",
            "2022-08-28 21:09:56 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-08-28 21:09:56 | INFO | train | epoch 002 | loss 14.523 | nll_loss 14.238 | ppl 19315.9 | wps 7983 | ups 1.18 | wpb 6782.7 | bsz 535.9 | num_updates 148 | lr 1.85e-05 | gnorm 1.742 | loss_scale 32 | train_wall 31 | gb_free 7.5 | wall 134\n",
            "2022-08-28 21:09:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 003:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:09:56 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-08-28 21:09:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  88% 66/75 [00:28<00:03,  2.36it/s, loss=14.138, nll_loss=13.812, ppl=14384.9, wps=9039.4, ups=1.34, wpb=6753.3, bsz=532.8, num_updates=200, lr=2.5e-05, gnorm=1.886, loss_scale=32, train_wall=42, gb_free=7.3, wall=157]2022-08-28 21:10:25 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 003:  99% 74/75 [00:32<00:00,  2.33it/s, loss=14.138, nll_loss=13.812, ppl=14384.9, wps=9039.4, ups=1.34, wpb=6753.3, bsz=532.8, num_updates=200, lr=2.5e-05, gnorm=1.886, loss_scale=32, train_wall=42, gb_free=7.3, wall=157]2022-08-28 21:10:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  17% 1/6 [00:00<00:04,  1.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 2/6 [00:01<00:03,  1.02it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.03s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.10s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.01it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.34it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:10:34 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.3 | nll_loss 12.882 | ppl 7550.45 | bleu 0.07 | wps 1813.8 | wpb 1629.5 | bsz 225.5 | num_updates 222 | best_loss 13.3\n",
            "2022-08-28 21:10:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 222 updates\n",
            "2022-08-28 21:10:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-28 21:10:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-28 21:11:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint3.pt (epoch 3 @ 222 updates, score 13.3) (writing took 29.017938533999995 seconds)\n",
            "2022-08-28 21:11:03 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-08-28 21:11:03 | INFO | train | epoch 003 | loss 13.817 | nll_loss 13.457 | ppl 11248.5 | wps 7470.1 | ups 1.1 | wpb 6768.6 | bsz 536.5 | num_updates 222 | lr 2.775e-05 | gnorm 2.517 | loss_scale 16 | train_wall 32 | gb_free 7.5 | wall 201\n",
            "2022-08-28 21:11:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 004:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:11:03 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-08-28 21:11:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 74/75 [00:33<00:00,  2.19it/s]2022-08-28 21:11:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.07s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.06s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.04s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 4/6 [00:04<00:01,  1.01it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  83% 5/6 [00:04<00:00,  1.26it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 6/6 [00:04<00:00,  1.73it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:11:42 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.939 | nll_loss 12.45 | ppl 5596.89 | bleu 0.12 | wps 2193.4 | wpb 1629.5 | bsz 225.5 | num_updates 297 | best_loss 12.939\n",
            "2022-08-28 21:11:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 297 updates\n",
            "2022-08-28 21:11:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-28 21:11:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-28 21:12:11 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint4.pt (epoch 4 @ 297 updates, score 12.939) (writing took 29.03408720099992 seconds)\n",
            "2022-08-28 21:12:11 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-08-28 21:12:11 | INFO | train | epoch 004 | loss 13.122 | nll_loss 12.682 | ppl 6571.06 | wps 7549.5 | ups 1.11 | wpb 6782.7 | bsz 535.9 | num_updates 297 | lr 3.7125e-05 | gnorm 2.119 | loss_scale 16 | train_wall 33 | gb_free 8 | wall 268\n",
            "2022-08-28 21:12:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 005:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:12:11 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-08-28 21:12:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 74/75 [00:33<00:00,  2.32it/s, loss=13.22, nll_loss=12.791, ppl=7088.87, wps=5942.1, ups=0.88, wpb=6731, bsz=533.3, num_updates=300, lr=3.75e-05, gnorm=2.423, loss_scale=16, train_wall=44, gb_free=7.4, wall=270]2022-08-28 21:12:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  17% 1/6 [00:00<00:04,  1.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 2/6 [00:01<00:03,  1.04it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 3/6 [00:02<00:02,  1.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 4/6 [00:03<00:01,  1.02it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  83% 5/6 [00:04<00:00,  1.26it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 6/6 [00:04<00:00,  1.68it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:12:49 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.844 | nll_loss 12.3 | ppl 5043.4 | bleu 0.14 | wps 2185.8 | wpb 1629.5 | bsz 225.5 | num_updates 372 | best_loss 12.844\n",
            "2022-08-28 21:12:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 372 updates\n",
            "2022-08-28 21:12:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-28 21:12:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-28 21:13:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint5.pt (epoch 5 @ 372 updates, score 12.844) (writing took 27.821560823000027 seconds)\n",
            "2022-08-28 21:13:17 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-08-28 21:13:17 | INFO | train | epoch 005 | loss 12.593 | nll_loss 12.07 | ppl 4300.99 | wps 7637.3 | ups 1.13 | wpb 6782.7 | bsz 535.9 | num_updates 372 | lr 4.65e-05 | gnorm 2.344 | loss_scale 16 | train_wall 34 | gb_free 7.5 | wall 335\n",
            "2022-08-28 21:13:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 006:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:13:17 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-08-28 21:13:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 74/75 [00:34<00:00,  2.07it/s, loss=12.512, nll_loss=11.975, ppl=4026.2, wps=8756, ups=1.29, wpb=6811.8, bsz=530.8, num_updates=400, lr=5e-05, gnorm=2.124, loss_scale=16, train_wall=45, gb_free=7.5, wall=348]2022-08-28 21:13:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.07s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.34s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.12s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.13s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.03s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:13:58 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 12.709 | nll_loss 12.128 | ppl 4477.15 | bleu 0.15 | wps 1595.2 | wpb 1629.5 | bsz 225.5 | num_updates 447 | best_loss 12.709\n",
            "2022-08-28 21:13:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 447 updates\n",
            "2022-08-28 21:13:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-28 21:14:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-28 21:14:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint6.pt (epoch 6 @ 447 updates, score 12.709) (writing took 29.046555743 seconds)\n",
            "2022-08-28 21:14:27 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-08-28 21:14:27 | INFO | train | epoch 006 | loss 12.247 | nll_loss 11.658 | ppl 3231.08 | wps 7291.4 | ups 1.07 | wpb 6782.7 | bsz 535.9 | num_updates 447 | lr 5.5875e-05 | gnorm 2.034 | loss_scale 16 | train_wall 34 | gb_free 7.4 | wall 405\n",
            "2022-08-28 21:14:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 007:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:14:27 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-08-28 21:14:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 74/75 [00:34<00:00,  2.08it/s, loss=12.093, nll_loss=11.473, ppl=2843.14, wps=8278.4, ups=1.23, wpb=6749.8, bsz=548.3, num_updates=500, lr=6.25e-05, gnorm=2.227, loss_scale=16, train_wall=46, gb_free=7.7, wall=429]2022-08-28 21:15:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.24s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.27s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.23s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.27s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.12s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.08it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:15:09 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 12.716 | nll_loss 12.129 | ppl 4479.17 | bleu 0.24 | wps 1511.2 | wpb 1629.5 | bsz 225.5 | num_updates 522 | best_loss 12.709\n",
            "2022-08-28 21:15:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 522 updates\n",
            "2022-08-28 21:15:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-28 21:15:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-28 21:15:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint7.pt (epoch 7 @ 522 updates, score 12.716) (writing took 15.989113977999978 seconds)\n",
            "2022-08-28 21:15:25 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-08-28 21:15:25 | INFO | train | epoch 007 | loss 11.992 | nll_loss 11.353 | ppl 2614.87 | wps 8846.6 | ups 1.3 | wpb 6782.7 | bsz 535.9 | num_updates 522 | lr 6.525e-05 | gnorm 2.09 | loss_scale 16 | train_wall 34 | gb_free 7.3 | wall 462\n",
            "2022-08-28 21:15:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 008:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:15:25 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-08-28 21:15:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 74/75 [00:34<00:00,  2.41it/s]2022-08-28 21:16:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.17s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.16s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.16s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.21s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.08s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:16:06 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 12.319 | nll_loss 11.669 | ppl 3255.57 | bleu 0.33 | wps 1576.4 | wpb 1629.5 | bsz 225.5 | num_updates 597 | best_loss 12.319\n",
            "2022-08-28 21:16:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 597 updates\n",
            "2022-08-28 21:16:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-28 21:16:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-28 21:16:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint8.pt (epoch 8 @ 597 updates, score 12.319) (writing took 28.169919474999915 seconds)\n",
            "2022-08-28 21:16:34 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-08-28 21:16:34 | INFO | train | epoch 008 | loss 11.756 | nll_loss 11.078 | ppl 2161.71 | wps 7306.9 | ups 1.08 | wpb 6782.7 | bsz 535.9 | num_updates 597 | lr 7.4625e-05 | gnorm 2.018 | loss_scale 16 | train_wall 35 | gb_free 7.5 | wall 532\n",
            "2022-08-28 21:16:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 009:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:16:34 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-08-28 21:16:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 74/75 [00:35<00:00,  2.11it/s, loss=11.801, nll_loss=11.13, ppl=2240.54, wps=6543.4, ups=0.96, wpb=6821.4, bsz=528.2, num_updates=600, lr=7.5e-05, gnorm=2.003, loss_scale=16, train_wall=46, gb_free=7.6, wall=534]2022-08-28 21:17:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.31s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.21s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.23s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.38s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.21s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:17:17 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 12.197 | nll_loss 11.53 | ppl 2957.23 | bleu 0.27 | wps 1432.4 | wpb 1629.5 | bsz 225.5 | num_updates 672 | best_loss 12.197\n",
            "2022-08-28 21:17:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 672 updates\n",
            "2022-08-28 21:17:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-28 21:17:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-28 21:17:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint9.pt (epoch 9 @ 672 updates, score 12.197) (writing took 27.947800959999995 seconds)\n",
            "2022-08-28 21:17:45 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-08-28 21:17:45 | INFO | train | epoch 009 | loss 11.487 | nll_loss 10.768 | ppl 1743.66 | wps 7205.6 | ups 1.06 | wpb 6782.7 | bsz 535.9 | num_updates 672 | lr 8.4e-05 | gnorm 1.983 | loss_scale 16 | train_wall 35 | gb_free 7.8 | wall 603\n",
            "2022-08-28 21:17:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 010:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:17:45 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-08-28 21:17:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 74/75 [00:34<00:00,  2.14it/s, loss=11.434, nll_loss=10.706, ppl=1670.69, wps=8275.7, ups=1.22, wpb=6777.9, bsz=534, num_updates=700, lr=8.75e-05, gnorm=2.074, loss_scale=16, train_wall=46, gb_free=7.3, wall=616]2022-08-28 21:18:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.20s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.25s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.30s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.36s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.20s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.01it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:18:27 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 12.013 | nll_loss 11.309 | ppl 2536.32 | bleu 0.37 | wps 1412.1 | wpb 1629.5 | bsz 225.5 | num_updates 747 | best_loss 12.013\n",
            "2022-08-28 21:18:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 747 updates\n",
            "2022-08-28 21:18:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-28 21:18:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-28 21:18:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint10.pt (epoch 10 @ 747 updates, score 12.013) (writing took 28.98597219599992 seconds)\n",
            "2022-08-28 21:18:56 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-08-28 21:18:56 | INFO | train | epoch 010 | loss 11.224 | nll_loss 10.465 | ppl 1413.51 | wps 7178.3 | ups 1.06 | wpb 6782.7 | bsz 535.9 | num_updates 747 | lr 9.3375e-05 | gnorm 2.005 | loss_scale 16 | train_wall 34 | gb_free 7.8 | wall 674\n",
            "2022-08-28 21:18:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 011:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:18:56 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2022-08-28 21:18:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 74/75 [00:34<00:00,  2.11it/s, loss=11.016, nll_loss=10.228, ppl=1199.05, wps=8272.7, ups=1.21, wpb=6830.3, bsz=548.1, num_updates=800, lr=0.0001, gnorm=1.914, loss_scale=16, train_wall=46, gb_free=8, wall=698]2022-08-28 21:19:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.33s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.46s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 3/6 [00:04<00:04,  1.49s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.47s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.27s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 6/6 [00:07<00:00,  1.06s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:19:38 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 11.996 | nll_loss 11.282 | ppl 2490.69 | bleu 0.46 | wps 1298.3 | wpb 1629.5 | bsz 225.5 | num_updates 822 | best_loss 11.996\n",
            "2022-08-28 21:19:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 822 updates\n",
            "2022-08-28 21:19:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-28 21:19:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-28 21:20:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint11.pt (epoch 11 @ 822 updates, score 11.996) (writing took 28.286884848 seconds)\n",
            "2022-08-28 21:20:06 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-08-28 21:20:06 | INFO | train | epoch 011 | loss 10.888 | nll_loss 10.08 | ppl 1082.74 | wps 7192.5 | ups 1.06 | wpb 6782.7 | bsz 535.9 | num_updates 822 | lr 0.00010275 | gnorm 1.912 | loss_scale 16 | train_wall 35 | gb_free 7.4 | wall 744\n",
            "2022-08-28 21:20:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 012:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:20:07 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2022-08-28 21:20:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 74/75 [00:34<00:00,  2.17it/s]2022-08-28 21:20:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.04s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.21s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.18s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.25s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.13s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.06it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:20:48 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 11.616 | nll_loss 10.848 | ppl 1843.2 | bleu 0.82 | wps 1480.9 | wpb 1629.5 | bsz 225.5 | num_updates 897 | best_loss 11.616\n",
            "2022-08-28 21:20:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 897 updates\n",
            "2022-08-28 21:20:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-28 21:20:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-28 21:21:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint12.pt (epoch 12 @ 897 updates, score 11.616) (writing took 28.48091063700008 seconds)\n",
            "2022-08-28 21:21:16 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-08-28 21:21:16 | INFO | train | epoch 012 | loss 10.544 | nll_loss 9.688 | ppl 825.03 | wps 7284.8 | ups 1.07 | wpb 6782.7 | bsz 535.9 | num_updates 897 | lr 0.000112125 | gnorm 1.853 | loss_scale 16 | train_wall 34 | gb_free 7.5 | wall 814\n",
            "2022-08-28 21:21:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 013:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:21:16 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2022-08-28 21:21:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 74/75 [00:34<00:00,  2.06it/s, loss=10.61, nll_loss=9.764, ppl=869.32, wps=5733.8, ups=0.85, wpb=6756.5, bsz=528.6, num_updates=900, lr=0.0001125, gnorm=1.851, loss_scale=16, train_wall=46, gb_free=7.7, wall=816]2022-08-28 21:21:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.05s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.11s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.13s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.17s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.05s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.13it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:21:58 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 11.567 | nll_loss 10.793 | ppl 1773.72 | bleu 1.07 | wps 1591.5 | wpb 1629.5 | bsz 225.5 | num_updates 972 | best_loss 11.567\n",
            "2022-08-28 21:21:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 972 updates\n",
            "2022-08-28 21:21:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-28 21:22:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-28 21:23:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint13.pt (epoch 13 @ 972 updates, score 11.567) (writing took 78.63354844600008 seconds)\n",
            "2022-08-28 21:23:16 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2022-08-28 21:23:16 | INFO | train | epoch 013 | loss 10.191 | nll_loss 9.287 | ppl 624.66 | wps 4242.3 | ups 0.63 | wpb 6782.7 | bsz 535.9 | num_updates 972 | lr 0.0001215 | gnorm 1.938 | loss_scale 16 | train_wall 35 | gb_free 8.3 | wall 934\n",
            "2022-08-28 21:23:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 014:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:23:16 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2022-08-28 21:23:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 74/75 [00:34<00:00,  2.09it/s, loss=10.113, nll_loss=9.199, ppl=587.84, wps=5104.8, ups=0.76, wpb=6674.8, bsz=529.4, num_updates=1000, lr=0.000125, gnorm=1.941, loss_scale=16, train_wall=45, gb_free=7.6, wall=947]2022-08-28 21:23:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.14s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.16s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.15s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.20s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.07s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:23:58 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 11.563 | nll_loss 10.775 | ppl 1752.58 | bleu 1.26 | wps 1576.2 | wpb 1629.5 | bsz 225.5 | num_updates 1047 | best_loss 11.563\n",
            "2022-08-28 21:23:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1047 updates\n",
            "2022-08-28 21:23:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-28 21:24:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-28 21:25:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint14.pt (epoch 14 @ 1047 updates, score 11.563) (writing took 84.2750501559999 seconds)\n",
            "2022-08-28 21:25:22 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2022-08-28 21:25:22 | INFO | train | epoch 014 | loss 9.832 | nll_loss 8.878 | ppl 470.62 | wps 4048.4 | ups 0.6 | wpb 6782.7 | bsz 535.9 | num_updates 1047 | lr 0.000130875 | gnorm 2.013 | loss_scale 16 | train_wall 35 | gb_free 7.3 | wall 1060\n",
            "2022-08-28 21:25:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 015:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:25:22 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2022-08-28 21:25:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 74/75 [00:34<00:00,  2.21it/s, loss=9.625, nll_loss=8.643, ppl=399.8, wps=4989.1, ups=0.73, wpb=6851.3, bsz=545.5, num_updates=1100, lr=0.0001375, gnorm=2.057, loss_scale=16, train_wall=46, gb_free=7.3, wall=1084]2022-08-28 21:25:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.11s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.23s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.16s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.19s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.05s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:26:03 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 11.311 | nll_loss 10.478 | ppl 1425.83 | bleu 1.82 | wps 1571.6 | wpb 1629.5 | bsz 225.5 | num_updates 1122 | best_loss 11.311\n",
            "2022-08-28 21:26:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1122 updates\n",
            "2022-08-28 21:26:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-28 21:26:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-28 21:27:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint15.pt (epoch 15 @ 1122 updates, score 11.311) (writing took 85.129950818 seconds)\n",
            "2022-08-28 21:27:28 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2022-08-28 21:27:28 | INFO | train | epoch 015 | loss 9.466 | nll_loss 8.463 | ppl 352.79 | wps 4040.9 | ups 0.6 | wpb 6782.7 | bsz 535.9 | num_updates 1122 | lr 0.00014025 | gnorm 2.042 | loss_scale 16 | train_wall 34 | gb_free 7.3 | wall 1186\n",
            "2022-08-28 21:27:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 016:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:27:28 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2022-08-28 21:27:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 74/75 [00:33<00:00,  2.14it/s]2022-08-28 21:28:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.19s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.22s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.20s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.25s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.11s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.08it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:28:08 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 11.44 | nll_loss 10.61 | ppl 1563.35 | bleu 1.59 | wps 1520.6 | wpb 1629.5 | bsz 225.5 | num_updates 1197 | best_loss 11.311\n",
            "2022-08-28 21:28:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1197 updates\n",
            "2022-08-28 21:28:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-28 21:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-28 21:28:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint16.pt (epoch 16 @ 1197 updates, score 11.44) (writing took 41.23456181300003 seconds)\n",
            "2022-08-28 21:28:50 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2022-08-28 21:28:50 | INFO | train | epoch 016 | loss 9.12 | nll_loss 8.068 | ppl 268.38 | wps 6206.5 | ups 0.92 | wpb 6782.7 | bsz 535.9 | num_updates 1197 | lr 0.000149625 | gnorm 2.133 | loss_scale 16 | train_wall 34 | gb_free 7.6 | wall 1268\n",
            "2022-08-28 21:28:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 017:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:28:50 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2022-08-28 21:28:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 74/75 [00:36<00:00,  2.15it/s, loss=9.178, nll_loss=8.134, ppl=280.88, wps=3661.5, ups=0.54, wpb=6803.7, bsz=531, num_updates=1200, lr=0.00015, gnorm=2.082, loss_scale=16, train_wall=45, gb_free=7.3, wall=1270]2022-08-28 21:29:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.26s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.25s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.23s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.28s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.12s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.07it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:29:34 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 11.441 | nll_loss 10.61 | ppl 1562.67 | bleu 1.8 | wps 1511.4 | wpb 1629.5 | bsz 225.5 | num_updates 1272 | best_loss 11.311\n",
            "2022-08-28 21:29:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1272 updates\n",
            "2022-08-28 21:29:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-28 21:29:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-28 21:30:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint17.pt (epoch 17 @ 1272 updates, score 11.441) (writing took 42.858462631000066 seconds)\n",
            "2022-08-28 21:30:17 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2022-08-28 21:30:18 | INFO | train | epoch 017 | loss 8.744 | nll_loss 7.64 | ppl 199.5 | wps 5890.5 | ups 0.87 | wpb 6782.7 | bsz 535.9 | num_updates 1272 | lr 0.000159 | gnorm 2.081 | loss_scale 16 | train_wall 34 | gb_free 7.3 | wall 1354\n",
            "2022-08-28 21:30:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 018:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:30:18 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2022-08-28 21:30:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 74/75 [00:35<00:00,  2.12it/s, loss=8.662, nll_loss=7.547, ppl=186.99, wps=6987.4, ups=1.03, wpb=6780.4, bsz=536.4, num_updates=1300, lr=0.0001625, gnorm=2.122, loss_scale=16, train_wall=46, gb_free=7.6, wall=1369]2022-08-28 21:30:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  17% 1/6 [00:01<00:07,  1.51s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.38s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 3/6 [00:04<00:03,  1.31s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.32s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.15s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:31:00 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 11.336 | nll_loss 10.483 | ppl 1431.37 | bleu 2.17 | wps 1495.9 | wpb 1629.5 | bsz 225.5 | num_updates 1347 | best_loss 11.311\n",
            "2022-08-28 21:31:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1347 updates\n",
            "2022-08-28 21:31:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-28 21:31:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-28 21:31:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint18.pt (epoch 18 @ 1347 updates, score 11.336) (writing took 40.8387247390001 seconds)\n",
            "2022-08-28 21:31:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2022-08-28 21:31:41 | INFO | train | epoch 018 | loss 8.381 | nll_loss 7.226 | ppl 149.66 | wps 6101.3 | ups 0.9 | wpb 6782.7 | bsz 535.9 | num_updates 1347 | lr 0.000168375 | gnorm 2.115 | loss_scale 16 | train_wall 35 | gb_free 7.3 | wall 1439\n",
            "2022-08-28 21:31:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 019:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:31:43 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2022-08-28 21:31:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 74/75 [00:36<00:00,  2.11it/s, loss=8.192, nll_loss=7.01, ppl=128.93, wps=6989.9, ups=1.04, wpb=6706, bsz=525.2, num_updates=1400, lr=0.000175, gnorm=2.098, loss_scale=16, train_wall=46, gb_free=7.6, wall=1465]2022-08-28 21:32:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.19s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.17s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.14s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.21s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.09s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:32:24 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 10.924 | nll_loss 10.023 | ppl 1040.78 | bleu 3.25 | wps 1575.5 | wpb 1629.5 | bsz 225.5 | num_updates 1422 | best_loss 10.924\n",
            "2022-08-28 21:32:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1422 updates\n",
            "2022-08-28 21:32:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-28 21:32:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-28 21:33:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint19.pt (epoch 19 @ 1422 updates, score 10.924) (writing took 84.50546556600011 seconds)\n",
            "2022-08-28 21:33:49 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2022-08-28 21:33:49 | INFO | train | epoch 019 | loss 8.012 | nll_loss 6.804 | ppl 111.72 | wps 3986.7 | ups 0.59 | wpb 6782.7 | bsz 535.9 | num_updates 1422 | lr 0.00017775 | gnorm 2.068 | loss_scale 16 | train_wall 35 | gb_free 8 | wall 1567\n",
            "2022-08-28 21:33:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 020:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:33:49 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2022-08-28 21:33:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 74/75 [00:33<00:00,  2.17it/s]2022-08-28 21:34:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.20s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.18s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.14s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.16s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.05s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.13it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:34:29 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 10.88 | nll_loss 9.958 | ppl 994.31 | bleu 3.78 | wps 1614.4 | wpb 1629.5 | bsz 225.5 | num_updates 1497 | best_loss 10.88\n",
            "2022-08-28 21:34:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1497 updates\n",
            "2022-08-28 21:34:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-28 21:34:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-28 21:35:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint20.pt (epoch 20 @ 1497 updates, score 10.88) (writing took 84.18672238099998 seconds)\n",
            "2022-08-28 21:35:54 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2022-08-28 21:35:54 | INFO | train | epoch 020 | loss 7.63 | nll_loss 6.367 | ppl 82.52 | wps 4078.3 | ups 0.6 | wpb 6782.7 | bsz 535.9 | num_updates 1497 | lr 0.000187125 | gnorm 2.09 | loss_scale 16 | train_wall 34 | gb_free 7.8 | wall 1691\n",
            "2022-08-28 21:35:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 021:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:35:54 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2022-08-28 21:35:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 74/75 [00:34<00:00,  2.19it/s, loss=7.691, nll_loss=6.436, ppl=86.6, wps=2998.5, ups=0.44, wpb=6839.1, bsz=542.2, num_updates=1500, lr=0.0001875, gnorm=2.077, loss_scale=16, train_wall=46, gb_free=7.6, wall=1693]2022-08-28 21:36:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  17% 1/6 [00:01<00:08,  1.61s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.39s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 3/6 [00:04<00:03,  1.29s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.30s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.14s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:36:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 11.013 | nll_loss 10.091 | ppl 1090.6 | bleu 3.71 | wps 1525.2 | wpb 1629.5 | bsz 225.5 | num_updates 1572 | best_loss 10.88\n",
            "2022-08-28 21:36:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 1572 updates\n",
            "2022-08-28 21:36:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-28 21:36:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-28 21:37:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint21.pt (epoch 21 @ 1572 updates, score 11.013) (writing took 40.23341560600011 seconds)\n",
            "2022-08-28 21:37:16 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2022-08-28 21:37:16 | INFO | train | epoch 021 | loss 7.296 | nll_loss 5.985 | ppl 63.32 | wps 6206.6 | ups 0.92 | wpb 6782.7 | bsz 535.9 | num_updates 1572 | lr 0.0001965 | gnorm 2.168 | loss_scale 16 | train_wall 34 | gb_free 7.9 | wall 1773\n",
            "2022-08-28 21:37:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 022:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:37:20 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2022-08-28 21:37:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 74/75 [00:38<00:00,  2.14it/s, loss=7.182, nll_loss=5.854, ppl=57.86, wps=7001.4, ups=1.03, wpb=6817.6, bsz=542, num_updates=1600, lr=0.0002, gnorm=2.133, loss_scale=16, train_wall=45, gb_free=7.6, wall=1791]2022-08-28 21:37:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.25s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.24s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.20s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.23s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.11s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.07it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:38:01 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 10.878 | nll_loss 9.939 | ppl 981.82 | bleu 4.11 | wps 1529.2 | wpb 1629.5 | bsz 225.5 | num_updates 1647 | best_loss 10.878\n",
            "2022-08-28 21:38:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 1647 updates\n",
            "2022-08-28 21:38:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-28 21:38:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-28 21:39:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint22.pt (epoch 22 @ 1647 updates, score 10.878) (writing took 84.79310543300016 seconds)\n",
            "2022-08-28 21:39:26 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2022-08-28 21:39:26 | INFO | train | epoch 022 | loss 6.911 | nll_loss 5.543 | ppl 46.62 | wps 3913.6 | ups 0.58 | wpb 6782.7 | bsz 535.9 | num_updates 1647 | lr 0.000205875 | gnorm 2.066 | loss_scale 16 | train_wall 34 | gb_free 7.3 | wall 1904\n",
            "2022-08-28 21:39:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 023:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:39:26 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2022-08-28 21:39:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 74/75 [00:33<00:00,  2.21it/s, loss=6.716, nll_loss=5.319, ppl=39.92, wps=4980.6, ups=0.73, wpb=6834.4, bsz=538.9, num_updates=1700, lr=0.0002125, gnorm=2.107, loss_scale=16, train_wall=45, gb_free=7.4, wall=1928]2022-08-28 21:40:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.26s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.20s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.15s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.20s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.07s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:40:07 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 10.791 | nll_loss 9.834 | ppl 912.53 | bleu 5.29 | wps 1591.4 | wpb 1629.5 | bsz 225.5 | num_updates 1722 | best_loss 10.791\n",
            "2022-08-28 21:40:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 1722 updates\n",
            "2022-08-28 21:40:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-28 21:40:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-28 21:41:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint23.pt (epoch 23 @ 1722 updates, score 10.791) (writing took 84.886514713 seconds)\n",
            "2022-08-28 21:41:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2022-08-28 21:41:32 | INFO | train | epoch 023 | loss 6.566 | nll_loss 5.146 | ppl 35.4 | wps 4061.9 | ups 0.6 | wpb 6782.7 | bsz 535.9 | num_updates 1722 | lr 0.00021525 | gnorm 2.153 | loss_scale 16 | train_wall 34 | gb_free 7.5 | wall 2029\n",
            "2022-08-28 21:41:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 024:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:41:32 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2022-08-28 21:41:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 74/75 [00:33<00:00,  2.28it/s]2022-08-28 21:42:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.36s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.43s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.29s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.28s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.13s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.06it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:42:13 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 10.834 | nll_loss 9.868 | ppl 934.21 | bleu 5.82 | wps 1485.6 | wpb 1629.5 | bsz 225.5 | num_updates 1797 | best_loss 10.791\n",
            "2022-08-28 21:42:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 1797 updates\n",
            "2022-08-28 21:42:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-28 21:42:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-28 21:42:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint24.pt (epoch 24 @ 1797 updates, score 10.834) (writing took 43.962546124000255 seconds)\n",
            "2022-08-28 21:42:57 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2022-08-28 21:42:57 | INFO | train | epoch 024 | loss 6.169 | nll_loss 4.689 | ppl 25.8 | wps 5982.2 | ups 0.88 | wpb 6782.7 | bsz 535.9 | num_updates 1797 | lr 0.000224625 | gnorm 1.922 | loss_scale 16 | train_wall 34 | gb_free 7.6 | wall 2114\n",
            "2022-08-28 21:42:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 025:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:42:58 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2022-08-28 21:42:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 74/75 [00:34<00:00,  2.26it/s, loss=6.254, nll_loss=4.787, ppl=27.6, wps=3556.9, ups=0.53, wpb=6708.4, bsz=528.4, num_updates=1800, lr=0.000225, gnorm=1.977, loss_scale=16, train_wall=45, gb_free=7.6, wall=2117]2022-08-28 21:43:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.28s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.24s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.19s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.20s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.07s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:43:38 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 10.687 | nll_loss 9.69 | ppl 826.17 | bleu 6.33 | wps 1589.6 | wpb 1629.5 | bsz 225.5 | num_updates 1872 | best_loss 10.687\n",
            "2022-08-28 21:43:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 1872 updates\n",
            "2022-08-28 21:43:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-28 21:43:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-28 21:45:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint25.pt (epoch 25 @ 1872 updates, score 10.687) (writing took 85.0850006769997 seconds)\n",
            "2022-08-28 21:45:04 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2022-08-28 21:45:04 | INFO | train | epoch 025 | loss 5.823 | nll_loss 4.292 | ppl 19.59 | wps 4026 | ups 0.59 | wpb 6782.7 | bsz 535.9 | num_updates 1872 | lr 0.000234 | gnorm 1.944 | loss_scale 16 | train_wall 34 | gb_free 7.4 | wall 2241\n",
            "2022-08-28 21:45:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 026:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:45:04 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2022-08-28 21:45:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 74/75 [00:33<00:00,  2.19it/s, loss=5.727, nll_loss=4.182, ppl=18.15, wps=4916.5, ups=0.73, wpb=6737.6, bsz=532.3, num_updates=1900, lr=0.0002375, gnorm=1.995, loss_scale=16, train_wall=45, gb_free=7.3, wall=2254]2022-08-28 21:45:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.28s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.21s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.15s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.18s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.06s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:45:44 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 10.666 | nll_loss 9.661 | ppl 809.65 | bleu 7.72 | wps 1612.4 | wpb 1629.5 | bsz 225.5 | num_updates 1947 | best_loss 10.666\n",
            "2022-08-28 21:45:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 1947 updates\n",
            "2022-08-28 21:45:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-28 21:45:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-28 21:47:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint26.pt (epoch 26 @ 1947 updates, score 10.666) (writing took 86.16647874499995 seconds)\n",
            "2022-08-28 21:47:10 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2022-08-28 21:47:10 | INFO | train | epoch 026 | loss 5.469 | nll_loss 3.882 | ppl 14.75 | wps 4023.9 | ups 0.59 | wpb 6782.7 | bsz 535.9 | num_updates 1947 | lr 0.000243375 | gnorm 1.984 | loss_scale 16 | train_wall 33 | gb_free 7.5 | wall 2368\n",
            "2022-08-28 21:47:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 027:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:47:10 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2022-08-28 21:47:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 74/75 [00:33<00:00,  2.18it/s, loss=5.28, nll_loss=3.663, ppl=12.67, wps=4988.2, ups=0.72, wpb=6883.8, bsz=530.7, num_updates=2000, lr=0.00025, gnorm=1.895, loss_scale=16, train_wall=45, gb_free=7.8, wall=2392]2022-08-28 21:47:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.17s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.13s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.20s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.17s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.04s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.13it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:47:50 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 10.477 | nll_loss 9.447 | ppl 698.03 | bleu 8.93 | wps 1607.5 | wpb 1629.5 | bsz 225.5 | num_updates 2022 | best_loss 10.477\n",
            "2022-08-28 21:47:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2022 updates\n",
            "2022-08-28 21:47:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-28 21:47:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-28 21:49:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint27.pt (epoch 27 @ 2022 updates, score 10.477) (writing took 86.32258557199975 seconds)\n",
            "2022-08-28 21:49:16 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2022-08-28 21:49:16 | INFO | train | epoch 027 | loss 5.106 | nll_loss 3.463 | ppl 11.03 | wps 4023.9 | ups 0.59 | wpb 6782.7 | bsz 535.9 | num_updates 2022 | lr 0.00025275 | gnorm 1.856 | loss_scale 16 | train_wall 33 | gb_free 7.8 | wall 2494\n",
            "2022-08-28 21:49:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 028:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:49:16 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2022-08-28 21:49:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 74/75 [00:34<00:00,  2.20it/s]2022-08-28 21:49:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.25s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.20s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.15s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.16s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.03s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:49:57 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 10.475 | nll_loss 9.423 | ppl 686.63 | bleu 9.1 | wps 1648.6 | wpb 1629.5 | bsz 225.5 | num_updates 2097 | best_loss 10.475\n",
            "2022-08-28 21:49:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2097 updates\n",
            "2022-08-28 21:49:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-28 21:50:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-28 21:51:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint28.pt (epoch 28 @ 2097 updates, score 10.475) (writing took 85.50065885899994 seconds)\n",
            "2022-08-28 21:51:23 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2022-08-28 21:51:23 | INFO | train | epoch 028 | loss 4.754 | nll_loss 3.053 | ppl 8.3 | wps 4029.9 | ups 0.59 | wpb 6782.7 | bsz 535.9 | num_updates 2097 | lr 0.000262125 | gnorm 1.714 | loss_scale 16 | train_wall 34 | gb_free 7.6 | wall 2620\n",
            "2022-08-28 21:51:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 029:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:51:23 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2022-08-28 21:51:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 74/75 [00:33<00:00,  2.20it/s, loss=4.808, nll_loss=3.116, ppl=8.67, wps=2918.8, ups=0.43, wpb=6732.1, bsz=551.3, num_updates=2100, lr=0.0002625, gnorm=1.712, loss_scale=16, train_wall=45, gb_free=7.8, wall=2623]2022-08-28 21:51:57 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.36s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.28s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.20s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.17s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.04s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.29it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:52:03 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 10.546 | nll_loss 9.488 | ppl 718.31 | bleu 8.47 | wps 1740.9 | wpb 1629.5 | bsz 225.5 | num_updates 2172 | best_loss 10.475\n",
            "2022-08-28 21:52:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2172 updates\n",
            "2022-08-28 21:52:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-28 21:52:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-28 21:52:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint29.pt (epoch 29 @ 2172 updates, score 10.546) (writing took 41.49377214799961 seconds)\n",
            "2022-08-28 21:52:45 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2022-08-28 21:52:45 | INFO | train | epoch 029 | loss 4.466 | nll_loss 2.717 | ppl 6.57 | wps 6212.8 | ups 0.92 | wpb 6782.7 | bsz 535.9 | num_updates 2172 | lr 0.0002715 | gnorm 1.841 | loss_scale 16 | train_wall 34 | gb_free 7.8 | wall 2702\n",
            "2022-08-28 21:52:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 030:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:52:45 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2022-08-28 21:52:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 74/75 [00:34<00:00,  2.19it/s, loss=4.379, nll_loss=2.615, ppl=6.13, wps=7258.7, ups=1.08, wpb=6742.3, bsz=525.1, num_updates=2200, lr=0.000275, gnorm=1.782, loss_scale=16, train_wall=45, gb_free=7.7, wall=2715]2022-08-28 21:53:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.21s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.16s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.12s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.13s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.03s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.14it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:53:25 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 10.509 | nll_loss 9.445 | ppl 697.09 | bleu 10.13 | wps 1652.3 | wpb 1629.5 | bsz 225.5 | num_updates 2247 | best_loss 10.475\n",
            "2022-08-28 21:53:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2247 updates\n",
            "2022-08-28 21:53:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-28 21:53:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-28 21:54:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint30.pt (epoch 30 @ 2247 updates, score 10.509) (writing took 45.063676857000246 seconds)\n",
            "2022-08-28 21:54:10 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2022-08-28 21:54:10 | INFO | train | epoch 030 | loss 4.122 | nll_loss 2.314 | ppl 4.97 | wps 5938.9 | ups 0.88 | wpb 6782.7 | bsz 535.9 | num_updates 2247 | lr 0.000280875 | gnorm 1.543 | loss_scale 16 | train_wall 34 | gb_free 7.7 | wall 2788\n",
            "2022-08-28 21:54:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 031:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:54:10 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2022-08-28 21:54:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 74/75 [00:34<00:00,  2.16it/s, loss=3.961, nll_loss=2.126, ppl=4.36, wps=6964.7, ups=1.03, wpb=6757.7, bsz=539.6, num_updates=2300, lr=0.0002875, gnorm=1.501, loss_scale=16, train_wall=45, gb_free=7.3, wall=2812]2022-08-28 21:54:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  17% 1/6 [00:01<00:07,  1.53s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.35s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.24s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.23s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.08s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:54:52 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 10.51 | nll_loss 9.428 | ppl 689.01 | bleu 9.69 | wps 1590.3 | wpb 1629.5 | bsz 225.5 | num_updates 2322 | best_loss 10.475\n",
            "2022-08-28 21:54:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 2322 updates\n",
            "2022-08-28 21:54:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-28 21:55:00 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-28 21:55:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint31.pt (epoch 31 @ 2322 updates, score 10.51) (writing took 44.669039557999895 seconds)\n",
            "2022-08-28 21:55:36 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2022-08-28 21:55:36 | INFO | train | epoch 031 | loss 3.827 | nll_loss 1.968 | ppl 3.91 | wps 5904.5 | ups 0.87 | wpb 6782.7 | bsz 535.9 | num_updates 2322 | lr 0.00029025 | gnorm 1.467 | loss_scale 16 | train_wall 34 | gb_free 7.4 | wall 2874\n",
            "2022-08-28 21:55:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 032:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:55:37 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2022-08-28 21:55:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 74/75 [00:34<00:00,  2.17it/s]2022-08-28 21:56:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.21s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.17s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.11s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.14s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.02s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:56:17 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 10.438 | nll_loss 9.34 | ppl 648.13 | bleu 11.06 | wps 1746.2 | wpb 1629.5 | bsz 225.5 | num_updates 2397 | best_loss 10.438\n",
            "2022-08-28 21:56:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 2397 updates\n",
            "2022-08-28 21:56:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-28 21:56:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-28 21:57:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint32.pt (epoch 32 @ 2397 updates, score 10.438) (writing took 84.23613855699978 seconds)\n",
            "2022-08-28 21:57:42 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2022-08-28 21:57:42 | INFO | train | epoch 032 | loss 3.581 | nll_loss 1.674 | ppl 3.19 | wps 4061.5 | ups 0.6 | wpb 6782.7 | bsz 535.9 | num_updates 2397 | lr 0.000299625 | gnorm 1.443 | loss_scale 16 | train_wall 35 | gb_free 7.6 | wall 2999\n",
            "2022-08-28 21:57:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 033:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:57:42 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2022-08-28 21:57:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 74/75 [00:33<00:00,  2.18it/s, loss=3.635, nll_loss=1.737, ppl=3.33, wps=3597.3, ups=0.53, wpb=6781.6, bsz=531, num_updates=2400, lr=0.0003, gnorm=1.435, loss_scale=16, train_wall=46, gb_free=7.5, wall=3001]2022-08-28 21:58:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.32s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.30s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.25s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.26s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.11s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.07it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:58:23 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 10.587 | nll_loss 9.481 | ppl 714.5 | bleu 9.09 | wps 1517.8 | wpb 1629.5 | bsz 225.5 | num_updates 2472 | best_loss 10.438\n",
            "2022-08-28 21:58:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 2472 updates\n",
            "2022-08-28 21:58:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-28 21:58:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-28 21:59:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint33.pt (epoch 33 @ 2472 updates, score 10.587) (writing took 42.52156057399998 seconds)\n",
            "2022-08-28 21:59:05 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2022-08-28 21:59:05 | INFO | train | epoch 033 | loss 3.335 | nll_loss 1.379 | ppl 2.6 | wps 6084 | ups 0.9 | wpb 6782.7 | bsz 535.9 | num_updates 2472 | lr 0.000309 | gnorm 1.339 | loss_scale 16 | train_wall 34 | gb_free 7.5 | wall 3083\n",
            "2022-08-28 21:59:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 034:   0% 0/75 [00:00<?, ?it/s]2022-08-28 21:59:05 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2022-08-28 21:59:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 74/75 [00:34<00:00,  2.20it/s, loss=3.269, nll_loss=1.299, ppl=2.46, wps=7193.8, ups=1.05, wpb=6862.7, bsz=538.3, num_updates=2500, lr=0.0003125, gnorm=1.262, loss_scale=16, train_wall=46, gb_free=7.6, wall=3096]2022-08-28 21:59:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.20s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.18s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.24s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.19s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.05s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:59:47 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 10.583 | nll_loss 9.483 | ppl 715.67 | bleu 10.96 | wps 1571.1 | wpb 1629.5 | bsz 225.5 | num_updates 2547 | best_loss 10.438\n",
            "2022-08-28 21:59:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 2547 updates\n",
            "2022-08-28 21:59:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-28 21:59:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-28 22:00:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint34.pt (epoch 34 @ 2547 updates, score 10.583) (writing took 41.62202046900029 seconds)\n",
            "2022-08-28 22:00:28 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2022-08-28 22:00:28 | INFO | train | epoch 034 | loss 3.119 | nll_loss 1.115 | ppl 2.17 | wps 6131.8 | ups 0.9 | wpb 6782.7 | bsz 535.9 | num_updates 2547 | lr 0.000318375 | gnorm 1.134 | loss_scale 16 | train_wall 34 | gb_free 7.7 | wall 3166\n",
            "2022-08-28 22:00:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 035:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:00:29 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2022-08-28 22:00:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 74/75 [00:37<00:00,  2.17it/s, loss=3.04, nll_loss=1.017, ppl=2.02, wps=6900.8, ups=1.02, wpb=6736.9, bsz=537.2, num_updates=2600, lr=0.000325, gnorm=1.115, loss_scale=16, train_wall=49, gb_free=7.3, wall=3194]2022-08-28 22:01:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.21s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.20s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.15s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.16s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.03s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.30it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:01:12 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 10.667 | nll_loss 9.578 | ppl 764.33 | bleu 11.62 | wps 1740.8 | wpb 1629.5 | bsz 225.5 | num_updates 2622 | best_loss 10.438\n",
            "2022-08-28 22:01:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 2622 updates\n",
            "2022-08-28 22:01:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-28 22:01:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-28 22:01:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint35.pt (epoch 35 @ 2622 updates, score 10.667) (writing took 43.402131046999784 seconds)\n",
            "2022-08-28 22:01:56 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2022-08-28 22:01:56 | INFO | train | epoch 035 | loss 2.97 | nll_loss 0.931 | ppl 1.91 | wps 5822.1 | ups 0.86 | wpb 6782.7 | bsz 535.9 | num_updates 2622 | lr 0.00032775 | gnorm 1.077 | loss_scale 16 | train_wall 37 | gb_free 7.5 | wall 3254\n",
            "2022-08-28 22:01:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 036:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:01:56 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2022-08-28 22:01:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 74/75 [00:34<00:00,  2.12it/s]2022-08-28 22:02:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.21s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.17s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.12s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.14s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.02s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:02:37 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 10.562 | nll_loss 9.455 | ppl 702.08 | bleu 11.83 | wps 1750 | wpb 1629.5 | bsz 225.5 | num_updates 2697 | best_loss 10.438\n",
            "2022-08-28 22:02:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 2697 updates\n",
            "2022-08-28 22:02:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-28 22:02:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-28 22:03:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint36.pt (epoch 36 @ 2697 updates, score 10.562) (writing took 43.530708716999925 seconds)\n",
            "2022-08-28 22:03:20 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2022-08-28 22:03:21 | INFO | train | epoch 036 | loss 2.837 | nll_loss 0.762 | ppl 1.7 | wps 6004.1 | ups 0.89 | wpb 6782.7 | bsz 535.9 | num_updates 2697 | lr 0.000337125 | gnorm 0.953 | loss_scale 16 | train_wall 35 | gb_free 7.5 | wall 3338\n",
            "2022-08-28 22:03:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 037:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:03:21 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2022-08-28 22:03:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  99% 74/75 [00:34<00:00,  2.08it/s, loss=2.871, nll_loss=0.804, ppl=1.75, wps=4666.1, ups=0.68, wpb=6817.9, bsz=535.4, num_updates=2700, lr=0.0003375, gnorm=0.986, loss_scale=16, train_wall=46, gb_free=7.6, wall=3340]2022-08-28 22:03:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.17s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.15s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.11s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.11s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.09it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.39it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:04:01 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 10.452 | nll_loss 9.329 | ppl 643.18 | bleu 11.7 | wps 1864.5 | wpb 1629.5 | bsz 225.5 | num_updates 2772 | best_loss 10.438\n",
            "2022-08-28 22:04:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 2772 updates\n",
            "2022-08-28 22:04:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-28 22:04:10 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-28 22:04:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint37.pt (epoch 37 @ 2772 updates, score 10.452) (writing took 39.63124288400013 seconds)\n",
            "2022-08-28 22:04:41 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2022-08-28 22:04:41 | INFO | train | epoch 037 | loss 2.762 | nll_loss 0.664 | ppl 1.58 | wps 6316.7 | ups 0.93 | wpb 6782.7 | bsz 535.9 | num_updates 2772 | lr 0.0003465 | gnorm 0.933 | loss_scale 16 | train_wall 35 | gb_free 7.4 | wall 3419\n",
            "2022-08-28 22:04:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 038:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:04:45 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2022-08-28 22:04:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  99% 74/75 [00:38<00:00,  2.12it/s, loss=2.737, nll_loss=0.633, ppl=1.55, wps=7105.9, ups=1.05, wpb=6792.5, bsz=538.2, num_updates=2800, lr=0.00035, gnorm=0.902, loss_scale=16, train_wall=46, gb_free=7.4, wall=3436]2022-08-28 22:05:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.29s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.38s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.26s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.19s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.02s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.31it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:05:26 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 10.714 | nll_loss 9.635 | ppl 794.87 | bleu 11.49 | wps 1700.8 | wpb 1629.5 | bsz 225.5 | num_updates 2847 | best_loss 10.438\n",
            "2022-08-28 22:05:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 2847 updates\n",
            "2022-08-28 22:05:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-28 22:05:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-28 22:06:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint38.pt (epoch 38 @ 2847 updates, score 10.714) (writing took 38.927362392000305 seconds)\n",
            "2022-08-28 22:06:05 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2022-08-28 22:06:05 | INFO | train | epoch 038 | loss 2.692 | nll_loss 0.576 | ppl 1.49 | wps 6082 | ups 0.9 | wpb 6782.7 | bsz 535.9 | num_updates 2847 | lr 0.000355875 | gnorm 0.829 | loss_scale 16 | train_wall 35 | gb_free 7.7 | wall 3503\n",
            "2022-08-28 22:06:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 039:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:06:09 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2022-08-28 22:06:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  99% 74/75 [00:38<00:00,  2.10it/s, loss=2.671, nll_loss=0.551, ppl=1.46, wps=7055.7, ups=1.04, wpb=6762.9, bsz=537.3, num_updates=2900, lr=0.0003625, gnorm=0.804, loss_scale=16, train_wall=46, gb_free=7.3, wall=3532]2022-08-28 22:06:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.29s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.23s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.16s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.17s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.02it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:06:50 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 10.579 | nll_loss 9.453 | ppl 700.68 | bleu 11.35 | wps 1820.7 | wpb 1629.5 | bsz 225.5 | num_updates 2922 | best_loss 10.438\n",
            "2022-08-28 22:06:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 2922 updates\n",
            "2022-08-28 22:06:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-28 22:06:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-28 22:07:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint39.pt (epoch 39 @ 2922 updates, score 10.579) (writing took 39.886615565000284 seconds)\n",
            "2022-08-28 22:07:30 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2022-08-28 22:07:30 | INFO | train | epoch 039 | loss 2.649 | nll_loss 0.524 | ppl 1.44 | wps 6018.5 | ups 0.89 | wpb 6782.7 | bsz 535.9 | num_updates 2922 | lr 0.00036525 | gnorm 0.773 | loss_scale 16 | train_wall 34 | gb_free 7.6 | wall 3588\n",
            "2022-08-28 22:07:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 040:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:07:30 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2022-08-28 22:07:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  99% 74/75 [00:34<00:00,  2.15it/s]2022-08-28 22:08:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  17% 1/6 [00:01<00:07,  1.41s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.32s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.23s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.19s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.04s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.26it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:08:11 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 10.434 | nll_loss 9.292 | ppl 626.86 | bleu 10.75 | wps 1716.7 | wpb 1629.5 | bsz 225.5 | num_updates 2997 | best_loss 10.434\n",
            "2022-08-28 22:08:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 2997 updates\n",
            "2022-08-28 22:08:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-28 22:08:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-28 22:09:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint40.pt (epoch 40 @ 2997 updates, score 10.434) (writing took 87.3676329939999 seconds)\n",
            "2022-08-28 22:09:38 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2022-08-28 22:09:38 | INFO | train | epoch 040 | loss 2.613 | nll_loss 0.482 | ppl 1.4 | wps 3954 | ups 0.58 | wpb 6782.7 | bsz 535.9 | num_updates 2997 | lr 0.000374625 | gnorm 0.726 | loss_scale 16 | train_wall 35 | gb_free 7.3 | wall 3716\n",
            "2022-08-28 22:09:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 041:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:09:39 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2022-08-28 22:09:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  99% 74/75 [00:34<00:00,  2.32it/s, loss=2.623, nll_loss=0.494, ppl=1.41, wps=3647.1, ups=0.54, wpb=6793.8, bsz=535.5, num_updates=3000, lr=0.000375, gnorm=0.732, loss_scale=16, train_wall=46, gb_free=7.7, wall=3718]2022-08-28 22:10:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.21s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.14s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.10s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.08s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  83% 5/6 [00:04<00:00,  1.12it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.43it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:10:19 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 10.407 | nll_loss 9.295 | ppl 628.36 | bleu 12.43 | wps 1933.4 | wpb 1629.5 | bsz 225.5 | num_updates 3072 | best_loss 10.407\n",
            "2022-08-28 22:10:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 3072 updates\n",
            "2022-08-28 22:10:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-28 22:10:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-28 22:11:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint41.pt (epoch 41 @ 3072 updates, score 10.407) (writing took 85.75113556100041 seconds)\n",
            "2022-08-28 22:11:44 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2022-08-28 22:11:44 | INFO | train | epoch 041 | loss 2.574 | nll_loss 0.441 | ppl 1.36 | wps 4040.1 | ups 0.6 | wpb 6782.7 | bsz 535.9 | num_updates 3072 | lr 0.000384 | gnorm 0.635 | loss_scale 16 | train_wall 34 | gb_free 7.6 | wall 3842\n",
            "2022-08-28 22:11:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 042:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:11:47 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2022-08-28 22:11:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  99% 74/75 [00:35<00:00,  2.21it/s, loss=2.564, nll_loss=0.429, ppl=1.35, wps=4847.9, ups=0.72, wpb=6753.9, bsz=539.9, num_updates=3100, lr=0.0003875, gnorm=0.625, loss_scale=16, train_wall=45, gb_free=7.5, wall=3857]2022-08-28 22:12:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.22s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.17s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.22s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.16s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.05it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.33it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:12:26 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 10.33 | nll_loss 9.208 | ppl 591.46 | bleu 12.29 | wps 1782.5 | wpb 1629.5 | bsz 225.5 | num_updates 3147 | best_loss 10.33\n",
            "2022-08-28 22:12:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 3147 updates\n",
            "2022-08-28 22:12:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-28 22:12:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-28 22:13:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint42.pt (epoch 42 @ 3147 updates, score 10.33) (writing took 86.51730386300005 seconds)\n",
            "2022-08-28 22:13:53 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2022-08-28 22:13:53 | INFO | train | epoch 042 | loss 2.553 | nll_loss 0.423 | ppl 1.34 | wps 3954.2 | ups 0.58 | wpb 6782.7 | bsz 535.9 | num_updates 3147 | lr 0.000393375 | gnorm 0.609 | loss_scale 16 | train_wall 34 | gb_free 7.7 | wall 3971\n",
            "2022-08-28 22:13:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 043:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:13:53 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2022-08-28 22:13:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  99% 74/75 [00:33<00:00,  2.17it/s, loss=2.542, nll_loss=0.415, ppl=1.33, wps=4946.6, ups=0.73, wpb=6810.5, bsz=535.2, num_updates=3200, lr=0.0004, gnorm=0.595, loss_scale=16, train_wall=45, gb_free=7.6, wall=3995]2022-08-28 22:14:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  17% 1/6 [00:01<00:05,  1.19s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.15s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.09s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.09s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  83% 5/6 [00:04<00:00,  1.11it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:14:33 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 10.304 | nll_loss 9.198 | ppl 587.31 | bleu 11.86 | wps 1944.8 | wpb 1629.5 | bsz 225.5 | num_updates 3222 | best_loss 10.304\n",
            "2022-08-28 22:14:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 3222 updates\n",
            "2022-08-28 22:14:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-28 22:14:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-28 22:15:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint43.pt (epoch 43 @ 3222 updates, score 10.304) (writing took 86.16464366400032 seconds)\n",
            "2022-08-28 22:15:59 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2022-08-28 22:15:59 | INFO | train | epoch 043 | loss 2.531 | nll_loss 0.403 | ppl 1.32 | wps 4043 | ups 0.6 | wpb 6782.7 | bsz 535.9 | num_updates 3222 | lr 0.00040275 | gnorm 0.582 | loss_scale 16 | train_wall 34 | gb_free 7.9 | wall 4097\n",
            "2022-08-28 22:15:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 044:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:15:59 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2022-08-28 22:15:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  99% 74/75 [00:33<00:00,  2.19it/s]2022-08-28 22:16:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.28s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.24s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.16s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.12s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.06it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:16:39 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 10.338 | nll_loss 9.249 | ppl 608.48 | bleu 11.71 | wps 1827.2 | wpb 1629.5 | bsz 225.5 | num_updates 3297 | best_loss 10.304\n",
            "2022-08-28 22:16:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 3297 updates\n",
            "2022-08-28 22:16:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-28 22:16:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-28 22:17:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint44.pt (epoch 44 @ 3297 updates, score 10.338) (writing took 44.71641175200057 seconds)\n",
            "2022-08-28 22:17:24 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2022-08-28 22:17:24 | INFO | train | epoch 044 | loss 2.517 | nll_loss 0.391 | ppl 1.31 | wps 6003.4 | ups 0.89 | wpb 6782.7 | bsz 535.9 | num_updates 3297 | lr 0.000412125 | gnorm 0.58 | loss_scale 16 | train_wall 34 | gb_free 7.5 | wall 4181\n",
            "2022-08-28 22:17:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 045:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:17:24 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2022-08-28 22:17:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  99% 74/75 [00:33<00:00,  2.19it/s, loss=2.522, nll_loss=0.398, ppl=1.32, wps=3589, ups=0.53, wpb=6753.6, bsz=531.8, num_updates=3300, lr=0.0004125, gnorm=0.582, loss_scale=16, train_wall=45, gb_free=7.5, wall=4183]2022-08-28 22:17:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.27s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.40s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  50% 3/6 [00:04<00:04,  1.38s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.33s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.09s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:18:04 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 10.463 | nll_loss 9.39 | ppl 671.1 | bleu 11.86 | wps 1572.7 | wpb 1629.5 | bsz 225.5 | num_updates 3372 | best_loss 10.304\n",
            "2022-08-28 22:18:04 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 3372 updates\n",
            "2022-08-28 22:18:04 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-28 22:18:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-28 22:18:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint45.pt (epoch 45 @ 3372 updates, score 10.463) (writing took 41.241695390999666 seconds)\n",
            "2022-08-28 22:18:46 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2022-08-28 22:18:46 | INFO | train | epoch 045 | loss 2.51 | nll_loss 0.392 | ppl 1.31 | wps 6203.7 | ups 0.91 | wpb 6782.7 | bsz 535.9 | num_updates 3372 | lr 0.0004215 | gnorm 0.608 | loss_scale 16 | train_wall 34 | gb_free 7.6 | wall 4263\n",
            "2022-08-28 22:18:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 046:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:18:49 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2022-08-28 22:18:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  99% 74/75 [00:37<00:00,  2.13it/s, loss=2.502, nll_loss=0.382, ppl=1.3, wps=7088, ups=1.04, wpb=6813, bsz=543, num_updates=3400, lr=0.000425, gnorm=0.579, loss_scale=16, train_wall=45, gb_free=7.5, wall=4279]2022-08-28 22:19:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.36s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  33% 2/6 [00:03<00:06,  1.61s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  50% 3/6 [00:04<00:04,  1.39s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  67% 4/6 [00:05<00:02,  1.30s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  83% 5/6 [00:06<00:01,  1.05s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset: 100% 6/6 [00:06<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:19:30 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 10.541 | nll_loss 9.471 | ppl 709.6 | bleu 10.91 | wps 1493.6 | wpb 1629.5 | bsz 225.5 | num_updates 3447 | best_loss 10.304\n",
            "2022-08-28 22:19:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 3447 updates\n",
            "2022-08-28 22:19:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-28 22:19:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-28 22:20:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint46.pt (epoch 46 @ 3447 updates, score 10.541) (writing took 38.308917461999954 seconds)\n",
            "2022-08-28 22:20:08 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2022-08-28 22:20:09 | INFO | train | epoch 046 | loss 2.479 | nll_loss 0.358 | ppl 1.28 | wps 6150.4 | ups 0.91 | wpb 6782.7 | bsz 535.9 | num_updates 3447 | lr 0.000430875 | gnorm 0.486 | loss_scale 16 | train_wall 34 | gb_free 7.3 | wall 4346\n",
            "2022-08-28 22:20:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 047:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:20:10 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2022-08-28 22:20:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  99% 74/75 [00:35<00:00,  2.04it/s, loss=2.467, nll_loss=0.348, ppl=1.27, wps=7338.6, ups=1.08, wpb=6786.9, bsz=521.6, num_updates=3500, lr=0.0004375, gnorm=0.474, loss_scale=16, train_wall=46, gb_free=7.5, wall=4372]2022-08-28 22:20:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.30s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.27s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.19s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.14s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.07it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:20:50 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 10.428 | nll_loss 9.359 | ppl 656.73 | bleu 11.78 | wps 1817.9 | wpb 1629.5 | bsz 225.5 | num_updates 3522 | best_loss 10.304\n",
            "2022-08-28 22:20:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 3522 updates\n",
            "2022-08-28 22:20:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-28 22:20:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-28 22:21:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint47.pt (epoch 47 @ 3522 updates, score 10.428) (writing took 44.56292302699967 seconds)\n",
            "2022-08-28 22:21:35 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2022-08-28 22:21:36 | INFO | train | epoch 047 | loss 2.463 | nll_loss 0.347 | ppl 1.27 | wps 5908.5 | ups 0.87 | wpb 6782.7 | bsz 535.9 | num_updates 3522 | lr 0.00044025 | gnorm 0.483 | loss_scale 16 | train_wall 34 | gb_free 7.5 | wall 4433\n",
            "2022-08-28 22:21:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 048:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:21:37 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2022-08-28 22:21:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  99% 74/75 [00:35<00:00,  2.18it/s]2022-08-28 22:22:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.32s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.25s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.19s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.14s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.01it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.36it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:22:18 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 10.681 | nll_loss 9.658 | ppl 807.87 | bleu 11.49 | wps 1807.6 | wpb 1629.5 | bsz 225.5 | num_updates 3597 | best_loss 10.304\n",
            "2022-08-28 22:22:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 3597 updates\n",
            "2022-08-28 22:22:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-28 22:22:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-28 22:22:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint48.pt (epoch 48 @ 3597 updates, score 10.681) (writing took 39.84487464999984 seconds)\n",
            "2022-08-28 22:22:58 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2022-08-28 22:22:58 | INFO | train | epoch 048 | loss 2.461 | nll_loss 0.349 | ppl 1.27 | wps 6194.4 | ups 0.91 | wpb 6782.7 | bsz 535.9 | num_updates 3597 | lr 0.000449625 | gnorm 0.496 | loss_scale 16 | train_wall 35 | gb_free 7.6 | wall 4516\n",
            "2022-08-28 22:22:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 049:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:22:59 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2022-08-28 22:22:59 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  99% 74/75 [00:37<00:00,  2.08it/s, loss=2.464, nll_loss=0.354, ppl=1.28, wps=4544.5, ups=0.67, wpb=6768.9, bsz=542.6, num_updates=3600, lr=0.00045, gnorm=0.503, loss_scale=16, train_wall=49, gb_free=7.3, wall=4521]2022-08-28 22:23:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  17% 1/6 [00:01<00:06,  1.28s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  33% 2/6 [00:02<00:04,  1.22s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.14s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.12s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  83% 5/6 [00:05<00:00,  1.10it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.47it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:23:42 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 10.35 | nll_loss 9.29 | ppl 626.19 | bleu 11.95 | wps 1926.2 | wpb 1629.5 | bsz 225.5 | num_updates 3672 | best_loss 10.304\n",
            "2022-08-28 22:23:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 3672 updates\n",
            "2022-08-28 22:23:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-28 22:23:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-28 22:24:25 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint49.pt (epoch 49 @ 3672 updates, score 10.35) (writing took 43.17989318899981 seconds)\n",
            "2022-08-28 22:24:25 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2022-08-28 22:24:25 | INFO | train | epoch 049 | loss 2.446 | nll_loss 0.335 | ppl 1.26 | wps 5862.9 | ups 0.86 | wpb 6782.7 | bsz 535.9 | num_updates 3672 | lr 0.000459 | gnorm 0.461 | loss_scale 16 | train_wall 37 | gb_free 7.7 | wall 4603\n",
            "2022-08-28 22:24:25 | INFO | fairseq.data.iterators | grouped total_num_itrs = 75\n",
            "epoch 050:   0% 0/75 [00:00<?, ?it/s]2022-08-28 22:24:25 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2022-08-28 22:24:25 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  99% 74/75 [00:34<00:00,  2.12it/s, loss=2.44, nll_loss=0.328, ppl=1.26, wps=7160.6, ups=1.05, wpb=6812.4, bsz=527.8, num_updates=3700, lr=0.0004625, gnorm=0.447, loss_scale=16, train_wall=46, gb_free=7.5, wall=4616]2022-08-28 22:25:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  17% 1/6 [00:01<00:07,  1.51s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  33% 2/6 [00:02<00:05,  1.31s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  50% 3/6 [00:03<00:03,  1.21s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  67% 4/6 [00:04<00:02,  1.16s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  83% 5/6 [00:05<00:01,  1.02s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 6/6 [00:05<00:00,  1.32it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:25:06 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 10.425 | nll_loss 9.372 | ppl 662.41 | bleu 12.19 | wps 1806.6 | wpb 1629.5 | bsz 225.5 | num_updates 3747 | best_loss 10.304\n",
            "2022-08-28 22:25:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 3747 updates\n",
            "2022-08-28 22:25:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-28 22:25:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-28 22:25:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint50.pt (epoch 50 @ 3747 updates, score 10.425) (writing took 40.59109367699966 seconds)\n",
            "2022-08-28 22:25:47 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2022-08-28 22:25:47 | INFO | train | epoch 050 | loss 2.44 | nll_loss 0.334 | ppl 1.26 | wps 6223.4 | ups 0.92 | wpb 6782.7 | bsz 535.9 | num_updates 3747 | lr 0.000468375 | gnorm 0.467 | loss_scale 16 | train_wall 35 | gb_free 7.7 | wall 4684\n",
            "2022-08-28 22:25:47 | INFO | fairseq_cli.train | done training in 4684.5 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:16:00.904977Z",
          "iopub.execute_input": "2022-07-24T19:16:00.905444Z",
          "iopub.status.idle": "2022-07-24T19:16:01.196986Z",
          "shell.execute_reply.started": "2022-07-24T19:16:00.905403Z",
          "shell.execute_reply": "2022-07-24T19:16:01.195842Z"
        },
        "trusted": true,
        "id": "yL7nIungdwcw",
        "outputId": "135b3d89-5c0b-4bbe-cd83-bee7b7e1e8dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdata-bin\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/drive/dsb-de/valid2/valid2.de --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang de --target-lang dsb \\\n",
        "      --beam 5 data-bin/wmt_de_dsb | grep -P \"D-[0-9]+\" | cut -f3 > target.txt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:05:22.918437Z",
          "iopub.execute_input": "2022-07-24T18:05:22.919517Z",
          "iopub.status.idle": "2022-07-24T18:06:13.344631Z",
          "shell.execute_reply.started": "2022-07-24T18:05:22.919454Z",
          "shell.execute_reply": "2022-07-24T18:06:13.343313Z"
        },
        "trusted": true,
        "id": "PdGwekMXdwcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d706dbb-33d4-4f0b-9d07-efde2418203a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 22:35:27 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/drive/dsb-de/valid2/valid2.de'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_de_dsb', 'source_lang': 'de', 'target_lang': 'dsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 22:35:27 | INFO | fairseq.tasks.translation | [de] dictionary: 62320 types\n",
            "2022-08-28 22:35:27 | INFO | fairseq.tasks.translation | [dsb] dictionary: 80848 types\n",
            "2022-08-28 22:35:27 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-28 22:35:42 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-28 22:35:42 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-28 22:35:42 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-28 22:35:49 | INFO | fairseq_cli.interactive | Total time: 22.817 seconds; translation time: 5.645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat target.txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:13.348554Z",
          "iopub.execute_input": "2022-07-24T18:06:13.348954Z",
          "iopub.status.idle": "2022-07-24T18:06:14.121539Z",
          "shell.execute_reply.started": "2022-07-24T18:06:13.348910Z",
          "shell.execute_reply": "2022-07-24T18:06:14.108183Z"
        },
        "trusted": true,
        "id": "BtsGN-Fydwcy",
        "outputId": "37d8bffe-3a16-404d-ccaf-1464f3a85033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pśed kuždym rozsudom wó póstajenju jadnogo popajźeństwa ma se pótrjefjonemu móžnosć daś, pó swójej wóli pšawniskego zastupnika k tomu braś.\n",
            "Wognjowa wobora jo wuwiła nowu metodu gasenja.\n",
            "To jo wuknjeńska statistika.\n",
            "Šule we sedleńskem rumje směju se pomjeniś “serbska šula”.\n",
            "Rozpušćenje gmejnow pśeśiwo jich wóli pomina wótpowědnu kazń.\n",
            "We wšych pśedmjatach deje se zrownju słowa ze wšednego žywjenja we pśiigodnej šyrokosći teke w serbskej rěcy posrědnjaś.\n",
            "(6) Kraj, gmejny a gmejnske zwězki pódpěruju źiśownje a centrumy za lichy cas młodostnych, njewótwisnje jich nosarstwa.\n",
            "Wobśěžkajuca abo wobśěžkajucy ma se wó rozsuźenju ned informěrowaś; sada 2 płaśi wótpowědujucy.\n",
            "Wuzwólone su te kandidaty, kenž su w pótajmnem wótgłosowanju głose wětšyny wót dweju tśeśinowu cłonkow krajnego sejma dostali.\n",
            "Kraj se procujo wó wuběźowanje a šansowu spšawnosć.\n",
            "Kraj statkujo na to, až se źiwa na nastupnosći Serbow w pórucenjach.\n",
            "Jolic njejsćo link až do toś togo casa aktiwěrowali, dejśo kšace wóspjetowaś.\n",
            "Pinguiny pśejdu drogu.\n",
            "Jo-lic teke pó wóspjetnem licenju žedna rownosć njewujźo, tak ma se to w protokolu zapisaś a wujasniś.\n",
            "Kraj pśipóznajo serbskej rěcy ako wuraz duchnego a kulturnego bogatstwa kraja.\n",
            "28. awgusta 1998 jo se statne dogrono wó załoženju pšawniski samostatneje Załožby za serbski lud pódpisało.\n",
            "Dosega, gaž to pśitrjefijo za jaden źěl gmejny.\n",
            "(6) Wjednica abo wjednik wólbow póstajijo pismawjednicu abo pismawjednika.\n",
            "Pšawidła wódobala su se změnili.\n",
            "Pó wótstawku 2 wuzwólone cłonki rady, powołaju se do amta pśez prezidentku abo prezidenta krajnego sejma.\n",
            "Zapózdźone wólbne listy se wót jadnańskego běrowa pśiwzeju, te pominane zapiski se napišu a se njewótcynjone zapakuju.\n",
            "Wón pśistajonych krajnego sejma powołujo a wótwołujo.\n",
            "Pśedsedarstwo Maćice Serbskeje pósćijo myto a informěrujo lawreata/lawreatku.\n",
            "Kraj spěchujo slěźenje a wucbu na pólu dolnoserbskeje rěcy ako teke na pólu stawiznow a kultury Serbow.\n",
            "W zapisu wólarjow ma se do za to pśedwiźonego słupka zapis “wp” zapisaś.\n",
            "Wóna pokazujo na póstajenja wó wopśimjeśu a formje wólbnych naraźenjow jadnotliwego.\n",
            "Załožba se we głownem pśez Bramborsku, Lichotny stat Sakska a zwězk financěrujo.\n",
            "Nadrobne informacije dostanjośo w našom wužywanju cookijow.\n",
            "Wužywa wóna abo wón to pšawo, statkujo to rowno tak, ako by wóna abo wón wužywał nimsku rěc.\n",
            "We wobgranicujucej kazni ma se na zakładne pšawo z pomjenjenim artikla pokazaś.\n",
            "Póžedanje na zapisanje do zapisa wólarjow ma se pisnje w zastojnstwowem běrowje zapódaś.\n",
            "Daś wuzwólowaś mógu se wšykne Serby, kótarež su se za wólby Rady za serbske nastupnosći zapisali.\n",
            "Zjawne twarjenja a institucije, drogi, puśe, naměsta, mósty a měsćańske tofle maju se w nimskej a serbskej rěcy wóznamjeniś.\n",
            "Změna jich wótpóranje lažy jich wótpóranje lažy w zagronitosći pśedsedarstwa Maćice Serbskeje.\n",
            "Šule we starodawnem sedleńskem rumje.\n",
            "dnja pśed slědnym dnjom listowych wólbow pla wjednice abo wjednika wólbow pisnje zapódaś.\n",
            "Zapódajśo pšosym swójo wužywaŕske mě abo mejlkowu adresu.\n",
            "Krajny kaznidawaŕ jo za wobcerk zastojnskich jadnanjow rědował trjebanje rěcy w § 23 wotr. 5 kazni k zastojnskim jadnanjam kraja Bramborska.\n",
            "(5) Zwopšawźenje pšawow Serbow rědujo kazń.\n",
            "Zarownaju se wudawki za dwójorěcne napisma na zjawnych twarjenjach a městnych toflach.\n",
            "Zwěsćiś ma se licby wótedanych głosow za jadnotliwe kandidatki a jadnotliwych kandidatow (wólbne naraźenja jadnotliwego).\n",
            "(1) Politiski pśeslědowane maju pšawo na azyl.\n",
            "Z našogo glědanja póstajijo se historiska šansa, pśidatnu identitu twórjecu perspektiwu za ceły region wutwóriś.\n",
            "Wósebnje w casu nacionalsocializma jo był w swójej eksistency wobgrozony.\n",
            "(4) Wobwinowany ma pšawo, w kuždej situaciji sudniskego jadnanja pomoc zašćitowarja wužywaś.\n",
            "To płaśi wósebnje za wólarjow a zapis wó za njepłaśiwje wurjaknjonych wólbnych łopjenow.\n",
            "(3) Nichten njesmějo se nuzkaś k wobźělenju pśi religioznem abo swětonaglědnem jadnanju abo k nałožowanju religiozneje formy pśisegi.\n",
            "Wólbnemu naraźenjoju jadnotliwego maju se pśipołožyś wuzjawjenje kandidata, až wón pśigłosujo wólbnemu naraźenjoju jadnotliwego.\n",
            "Kuždy bergaŕ ma pšawo, se wobźěliś na ludowych póžedanjach a ludowych rozsudach.\n",
            "Wóni spěchuju serbske wuměłstwo, wašnje a nałogi.\n",
            "Chtož dla wólbneje funkcije njejo w połoženje pla Jěwy wiźeł, móžo se pomoc wósoby pomoc wósoby dowěry.\n",
            "Ako myto wupišo Maćica Serbska 500 euro.\n",
            "Nadawki zastojnstwa społnjuju zastojniki, kenž źěłaju strońskonjewótwisnje a su wustawje a kaznjam zawězane.\n",
            "(3) Kužda kazń a kuždy pšawniski wukaz dej źeń nabyśa płaśiwosći póstajiś.\n",
            "(1) Njepósrědnje pó zakóńcenju wólbnego casa zwěsćijo pśedsedarstwo listowych wólbow w zjawnem pósejźenju wuslědk listowych wólbow.\n",
            "Rozpšawa wopśimjejo wobstojnosći, analyzěrujo statkownosć spěchowańskich napšawow a wugronijo se k pśedewześam krajnego kněžarstwa.\n",
            "Kuždy ma pšawo na jadnaki pśistup do zjawnych kubłanišćow, njewótwisnje wót swójogo politiskego pśeznanjenja.\n",
            "Wustne stajenje póžedanja z telefonom njejo dowólone.\n",
            "Rowno tak wažne jo serbskemu kšywowemu zwězkoju, w ramiku strukturneje změny źiwanje na nastupnosći Serbow wustajiś.\n",
            "Dalej pśistoj jim centralna rola pśi organizaciji wólbow Rady za nastupnosći Serbow pśi krajnem sejmje.\n",
            "Pśedpołoženje aktow a howacnych amtojskich pódłožkow smějotej se jano pón wótpokazaś, gaž pśewažece zjawne zajmy za pótajmstwo to raznje pominaju.\n",
            "Wótpokazane wólbne listy maju se ze swójim wopśimjeśim wusortěrowaś a z pśispomnjeśim wóznamjeniś.\n",
            "(7) Z dalejkubłańskimi pórucenjami za dorosćonych spěchujo se zachowanje a woplěwanje serbskeje rěcy a kultury.\n",
            "Wózjawjenje musy se w jadnom w sedleńskem rumje Serbow rozšyrjonem dnjownem casniku wózjawiś.\n",
            "Jadnaki zawězk se wugronijo serbsku rěc nastupajucy, wósebnje glědajucy na dolnoserbsku rěc.\n",
            "Za nastupnosći Serbow pśisłušne ministarstwo móžo na póstajiś změny starodawnego sedleńskego ruma.\n",
            "Krajny sejm jo 14. apryla 1992 pśedłogu krajneje wustawy wobzamknuł.\n",
            "(1) Bramborska šćita a spěchujo serbsku kulturu.\n",
            "Pśeśiwo rozsudam wuběrka a pśedsedarstwa myta dla njejo pšawniska droga móžna.\n",
            "Jo-lic se dosć góźecych wósobow njenaraźijo, pówołujo wjednica wólbow dalšne cłonki pó swójom rozsuźenju.\n",
            "(2) Pśi zasajźenju kuždego nowego pśepytowańskego wuběrka wuměnijo se pśedsedanje mjazy frakcijami pó pórěźe jich wjelikosći.\n",
            "Kazń zarucyjo lichotu wuznaśa k serbskemu ludoju.\n",
            "Rozwěž wuspěšnje nadawk, pla kótaregož jo cas wobgranicowany, aby licbu dypkow pówušył.\n",
            "Wjednica abo wjednik wólbow ma se wó to staraś, až jadnański běrow rozsuźenje dostanjo.\n",
            "Towarišnostnym kupkam w toś tej wustawje zawěsćone zakładne pšawa zawězuju, dalokož toś ta wustawa to póstajijo, teke tśeśe.\n",
            "Změna w zwěsćenju wólbnych wuslědkow musy se w běgu jadnogo tyźenja pó prědnem wobzamknjenju pśewjasć.\n",
            "Kuždy ma pšawo na liche rozwijanje swójeje wósobiny, dalokož wón njezranijo wustawu a jej wótpowědujuce kazni.\n",
            "Wutwórjenje frakcije pó konstituěrowanju krajnego sejma trjeba jogo pśigłosowanje.\n",
            "Po njej w zastojnskich jadnanjach po VwVfGBbg serbske wobźělniki na kosty za dolmetšarje a pśestajarje.\n",
            "(4) Jo we wobalce głosowańskego lisćika wěcej głosowańskich lisćikow, tak maju se te głosowańske lisćiki ako njepłaśiwy głosowański lisćik licyś.\n",
            "Pśi tom ma se zawěsćiś statkowne politiske sobupóstajanje serbskego luda zawěsćiś.\n",
            "Jeje wustne a pisne nałožowanje w zjawnem žywjenju se šćita a spěchujo.\n",
            "Spóznawajucy, až jo pšawo Serbow na swóju narodnu identitu źěl uniwerselnych cłowjeskich pšawow, wobzamknjo krajny sejm.\n",
            "Zwěsćiś ma se licba wólarkow a wólarjow.\n",
            "Rozwěž wuspěšnje wšykne nadawki lekcije.\n",
            "Wužywaŕske mě smějo z maksimalnje sedymnasćo znamuškow wobstaś.\n",
            "W namrětem sedlenskem rumje se pśizwoluju ludoju: šćitanje a spěchowanje serbskeje kultury.\n",
            "Njamóžo-lic se pśi pśesedlenju górnistwa dla žedna pśigódna płonina za zasejzasedlenje pórucyś, tak se sedleński rum rozšyrijo.\n",
            "(3) Pomaganje pó wótstawku 1 numer 2 ma se na dopołnjenje žycenjow wólarkow abo wólarjow wobgranicowaś.\n",
            "(5) Ceptarkam a ceptarjam dej se možnosć daś, až pśiiswoje se znajobnosći w serbskej rěcy a až mogo ju podłymiś.\n",
            "Licba njepłaśiwych głosowańskich lisćikow a licby wótedanych głosow za jadnotliwych kandidatow se pśepišu do protokola.\n",
            "Rozpušćenje jadnotliwych wokrejsow pomina wótpowědnu kazń.\n",
            "Wójnska propaganda a zjawne, cłowjecnu dostojnosć ranjece, diskriminěrowanja su zakazane.\n",
            "Pla tšachy muse dwě tśeśinje tych, kenž su swój głos wótedali, jo za tych, kenž maju za tych, kenž muse měś njekśě.\n",
            "Njepśigłosyjoli krajny sejm pšosbje na rozpušćenje krajnego sejma w casu styrich mjasecow, pśewjeźo se ludowe póžedanje.\n",
            "(3) Žony a muže su rownopšawne.\n",
            "Jež žerjo slinika.\n",
            "Kužda wósoba dostanjo wót jadnańskego běrowa bźez komuźenja, pisnu informaciju wó zapisanju do zapisa wólarjow.\n",
            "Wón zapišo na kuždem na slědnem dnju listowych wólbow pó zakóńcenju wólbnego casa dojźonem wólbnem lisće źeń a dokradny cas dochada.\n",
            "Pó wótběgnjenju naraźeńskego casa pówołujo wjednik wólbow bźez komuźenja zastojarja listowych wólbow a zastupnika ako teke pśisedarjow.\n",
            "Wugbajuca móc lažy w rukach krajnego kněžarstwa, zastojnskich instancow a w organach samozastojnstwa.\n",
            "(3) Pśepytowańske wuběrki maju pšawo, dopokaze zwěsćiś.\n",
            "(7) Kužde góle ma pó póstajenju kazni pšawo na wótkubłanje, kubłanje, wótwardowanje a zastaranje w źiśowni.\n",
            "Wólbny list ma se wót pśedsedarstwa listowych wólbow wótpokazaś, gaž pśedlažy staw pó § 29 wótstawk 1 numer 2 do 8.\n",
            "(2) Cłonki krajnego kněžarstwa a jich zagronite maju pśistup k pósejźenjam krajnego sejma a k pósejźenjam jogo wuběrkow.\n",
            "Wótpósłańce bźeze frakcije maju pšawo, sobuźěłaś w jadnom wuběrku z pšawom na głosowanje.\n",
            "Póžedajuca wósoba móžo w běgu dweju dnjowu pó informaciji wótpokazajucego rozsuźenja zapódaś wobśěžkanje pla wjednika wólbow.\n",
            "Wjednica abo wjednik wólbow móžo wótchylecy wót sady 1 pśizwóliś, až se wólbne pódłožki jěsnjej znice.\n",
            "(2) W póžedanju muse stojaś familijowe mě, pśedmjenja a narodny datum ako teke adresa do wuzwólowanja wopšawnjoneje wósoby.\n",
            "Dwě starej lětadle hyšći leśitej.\n",
            "Dopołnjenje tych wuměnjenjow se pó saźe 2 z póžedanim za zapisanim do zapisa wuzwólowarjow zwuraznijo.\n",
            "Wólba se pśewjeźo, gaž dwě tśeśinje tych, kenž su swój głos wótedali, jo pśigłosyła.\n",
            "(2) Powołańske wobradowanje mimo zapłaśenja a pósrědnjenje źěła se zawěsćijotej.\n",
            "(1) Wužywanje gruntow a wódow jo wósebnje zajmam powšyknosći a pśiducym generacijam winowate.\n",
            "Pśi wuzamknjenju zjawnosći ma se zjawne wobtwarźenje pódaś.\n",
            "Z teju rozpšawu rozpšawijo krajne kněžarstwo wó połoženju.\n",
            "Z tym kazń wuraznje pokazujo, kótary wuznam pśistoj serbskej rěcy za wuchowanje a dalšne wuwiśe swójskeje identity.\n",
            "Młody kněz se amizěrujo na karnewalowych swěźenjach.\n",
            "Łužyski rewěr jo ze swójeju měrneju koeksistencu Serbow a Nimcow jadnorazowe socialne derbstwo cełeje towarišnosći.\n",
            "Krajny sejm móžo ministarskemu prezidentoju jano z tym njedowěru wugroniś, až wón z głosami wětšyny naslědnika wuzwólijo.\n",
            "Kuždy wobydlaŕ ma pšawo, nałožowaś serbsku rěc w krajnych zastojnstwach, w jogo doglědoju pódstojecych zjadnośeństwach.\n",
            "Ryba plěwa w mórju.\n",
            "Kazń a wujasnijo z tym, až płaśe jogo póstawjenja za wšych w Bramborskej žywych Serbow.\n",
            "Lud kraja Bramborska wuznawa se k zakładnym pšawam, ako su zapisane w Zakładnej kazni Zwězkoweje republiki Nimskeje.\n",
            "Wótstupijo kandidatka abo kandidat wót kandidatury abo wón, tak se woglědujo wólbne naraźenje jadnotliwego ako njezapódane.\n",
            "To jo jězykłamate gronko.\n",
            "(3) Nichten njesmějo se dla samskego njestatka na zakłaźe powšyknych kaznjow wěcej razow pochłostaś.\n",
            "Wjednica abo wjednik wólbow rozsuźijo nanejpózdźej na sedymem dnju pśed slědnym dnjom listowych wólbow wó wobśěžkanju.\n",
            "(2) Kšywowy zwězk w zmysle togo póstajenja jo kšywowy zwězk serbskich towaristwow a zjadnośeństwow pó § 4a Serbskeje kazni.\n",
            "Artikel 13 wótrězk 3 se nałožujo.\n",
            "Nejlěpjej, až zatwarijoš licby a wjelike pismiki.\n",
            "Jadnański běrow rozsuźijo ned wó póžedanju.\n",
            "Woni su wězane na kazniske směrnice.\n",
            "Wólbny list ma se wótpokazaś, gaž wólaŕka pódpisała njejo.\n",
            "Wón zmóžnijo jim wuwijanje swójeje identity teke pód wuměnjenjami globalizěrowanego swěta.\n",
            "Demonstracije mogu se pód striktnem zachowanim zasady poměrnosći wobgranicyś, rozpušćiś abo zakazaś.\n",
            "akty a howacne amtojske pódłožki pśedpołožyś.\n",
            "Pśi licenju njepłaśiwych a płaśiwych głosow maju se wjasć liceńske lisćiny.\n",
            "Wustawa zgubijo swóju płaśiwosć, gaž jo w ludowem rozsuźe wětšyna wótgłosujucych nowej wustawje pśigłosyła.\n",
            "Kšywowy zwězk móžo pśipóznaś za serbske nastupnosći zagronite ministarstwo.\n",
            "(2) Wuškowne wólbne pódłožki mógu se 60 dnjow pśed nowowólbami znicyś.\n",
            "Za swójo statkowanje dostanu zarownanje za wudawki.\n",
            "(2) Bergarje maju pšawo, pominaś wólbu wustawu dawajucu zgromaźinu, kótaraž nowu krajnu wustawu wuźěłajo.\n",
            "Wón jo italsku spiwarku a primadonu Margheritu Salicola wuwlakł, kótaraž njewobogaśijo jano italsku operu w Drježdźanach.\n",
            "Póžedanje stajuca wósoba móžo město adrese pódaś adresu, pód kótarejuž jo k dostawanju.\n",
            "Zwěsćenje dopokazow njejo dowólone, gaž widobnje njelažy we wobłuku pśepytowańskego nadawka.\n",
            "(3) Kraj pśipóznawa zjawnostny nadawk cerkwjow a nabóžninskich zgromaźeństwow.\n",
            "Šćit a spěchowanje autochtonych – to groni wótdawna zasedlonych – mjeńšynow maju swój zakład w regulach Europejskeje rady.\n",
            "(2) Pomocnice a pomocniki pó wótstawku 1 mógu pśi wulicenju a zwěsćenju wuslědkow listowych wólbow ako teke pśi napisanju protokolow sobu statkowaś.\n",
            "Wóna jo wuknjeński wuspěch dojśpiła.\n",
            "Pšawodawstwo jo dowěrjone njewótwisnym sudnikam.\n",
            "Zwucuj góźinu wob źeń!\n",
            "Tak dej se procowaś wó nowelěrowanje serbskego šulskego póstajenja.\n",
            "(4) Pśeśiwo póstajenjam wjednice abo wjednika wólbow w póstupowanju wótpóranja brachow jo móžno, se na wósobu dowěry wobrośiś.\n",
            "Wótstawk 3 sada 2 płaśi wótpowědujucy.\n",
            "Do šule jo serbska stawizny a serbska kultura starstwu wótpowědujucy do wugótowanja graśa a kubłańskego źěła zapśěgnuś.\n",
            "Z tym se wótpowědujo dopórucenjam wuběrka za kubłanje, młoźinu a sport.\n",
            "(1) Listowe ako teke postowe a telefonowe pótajmstwo su njezranjobne.\n",
            "Druge wobydlarje maju pšawo, se wobźěliś a kupkowe pšawo na pomocy wobźěliś.\n",
            "(2) Sudnistwa za wósebne ressorty mogu se jano pśez kazń załožyś.\n",
            "(3) Zakładne pšawa płaśe teke za tukrajne juristiske wósoby, dalokož daju se wone pšawa pó swójom charakterje za nich nałožowaś.\n",
            "Z teju kaznju orientěrujo se Bramborska pó aktualnych diskusijach wó mjeńšynowych pšawach.\n",
            "Płaśece wólbne naraźenje jadnotliwego njepśedlažy, gaž wólbne naraźenje jadnotliwego njejo se zapódało wót zjadnośeństwa.\n",
            "(2) Starjejše maju pšawo na winowatosć a winowatosć k wótkubłanju swójich źiśi.\n",
            "Krajne kněžarstwo ma winowatosć, krajny sejm docasnje informěrowaś wó pśigótowanju wukazow a pśewjeźenju wjelikich pśedewześow.\n",
            "Trěbnych jo nanejmjenjej wósym znamuškow.\n",
            "Wón zwěsćijo licbu njepłaśiwych głosowańskich lisćikow a licbu płaśiwych głosow.\n",
            "Na zakłaźe póstaja ministaŕ nutśikownego pó napšašowanju Rady za nastupnosći Serbow.\n",
            "Rozpšawa analyzěrujo statkownosć spěchowanja serbskeje rěcy a kultury.\n",
            "Wólby do rady muse se nejpózdźej w tom lěśe wótměś, kótarež slědujo lětoju wólbow do krajnego sejma.\n",
            "Zarědnik wutwóriś.\n",
            "Na wósebny charakter starodawnego sedleńskego ruma Serbow ma se pśi wugótowanju krajneje a komunalneje politiki źiwaś.\n",
            "(2) Krajny sejm wuraźujo zjawnje.\n",
            "Teritorij gmejnow móžo se pśez dojadnanje gmejnow z pśizwólenim amta za pšawniski doglěd, pśez kazń změniś.\n",
            "Kazń póstajijo wobšyrny šćit kulturneje a nacionalneje identity Serbow.\n",
            "To jo wětšy źěl tak, gaž se wotměwaju serbske namše.\n",
            "(2) Wótpósłaŕka abo wótpósłaŕ wótpokazanego wólbnego lista se njelicy ako wólaŕka abo wólaŕ.\n",
            "Kandidaty ako teke wósoby dowěry a zastupujuce wósoby dowěry njesměju se póstajiś ako cłonki wólbnego organa.\n",
            "Zapódajśo samske šćitne gronidło drugi raz.\n",
            "Pśi awtomatiskem wjeźenju zapisa wólarjow ma se dataja wuśišćaś.\n",
            "Wón móžo se kuždy cas wobrośiś na krajny sejm.\n",
            "Njepłaśiwe su głose, gaž głosowański lisćik njama žedno abo wěcej ako pěś wóznamjenjenjow.\n",
            "(1) Organizacija statnego krajnego zastojnstwa a rědowanje zagronitosćow se pśez kazń abo na zakłaźe kazni póstajijotej.\n",
            "Lažecu zepěru lěpjej njecyńśo w žołtem sněgu.\n",
            "Wideo wótpóraś.\n",
            "Na źěle samem njesmějo mě awtora/awtorki napisane byś, aby se gódnośiło bźeze znaśa wósoby.\n",
            "Bramborska ludnosć jo ju 14. junija 1992 pśez ludowy rozsud pśiwzeła.\n",
            "Jadnański běrow dajo mjenja wjednice abo wjednika wólbow a jeje zastupnice a jogo zastupnika k wěsći.\n",
            "Gaž na „wugódnośenje“ tusnjoš, wiźiš, kótare z twójich wótegronow jo pšawe a kótare wopacne.\n",
            "Hympanje njejo hyšći olympiske graśe.\n",
            "Dwa něžnej psycka grajkatej.\n",
            "W tej chyli jo Domowina z.t. jadnučki pśipóznaty kšywowy zwězk serbskich towaristwow a zjadnośeństwow.\n",
            "Gaž jo pśedsedarstwo listowych wólbow swój nadawk dopołniło, zapakujo jaden cłonk pśecej źělone głosowańske lisćiki a wólbne łopjena.\n",
            "Sańkowanje jo zymski sport.\n",
            "Prezident krajnego sejma ma te wót krajnego sejma wobzamknjone abo pśez ludowy rozsud pśiwzete kazni bźez komuźenja wustajiś.\n",
            "Pśi tom ma se zarucyś pósrědnjenje dolnoserbskich znaśow na pólu literaturnych a stawizniskich wědomnosćow.\n",
            "Zwěsćiś ma se licba njepłaśiwych głosow a licba płaśiwych głosow.\n",
            "Kraj zapśimjejo serbske towaristwa a institucije do swójogo zgromadnego źěła z drugimi zwězkowymi krajami a drugimi statami.\n",
            "To móžoš hyšći lěpjej!\n",
            "(4) Wjednica abo wjednik wólbow se stara wó to, až se cłonki listowego pśedsedarstwa wó swójich słušnosćach informěruju.\n",
            "(1) Wědomnosć, slěźenje a wucba su lichotne.\n",
            "Wšykne wobydlarje maju pšawo, krajnemu sejmoju we wobłuku jogo kompetence wěste temy wutwórjenja politiskeje wóle naraźiś.\n",
            "Z kuždym pšawym wótegronom móžoš dypki zběraś a za wuspěch se mytujoš.\n",
            "Jo-lic se wósoba ze zapisa wólarjow wušmarnjo, tak ma se južo wuźělone wuzwólowańske łopjeno za njepłaśece deklarěrowaś.\n",
            "Pśedwuměnjenje za pśisłušnosć gmejny k namrětemu serbskemu sedleńskemu rumoju jo dane, gaž lažy gmejna w teritoriumje.\n",
            "Gmejny a źěle gmejnow w starodawnem sedleńskem rumje Serbow su Góry, Móst, Lěšće- Zakrjejc, Janšojce\n",
            "To jo wjelikosć.\n",
            "We wugbaśu swójogo amta jo wón njewótwisny, ale ma se kazni pódejśpiś.\n",
            "Kužde wólbne naraźenje jadnotliwego dej měś mě, adresu a telekomunikaciske pśizamknjenje wósoby dowěry.\n",
            "(2) Jadnański běrow rozsuźijo w běgu tśich dnjow wó spśeśiwjenju.\n",
            "Wóni su korporacije zjawnego pšawa, dalokož běchu to doněnta.\n",
            "Wopśimjeśe a wobgranicowanja su pśez kazni póstajone.\n",
            "Na głosowańskem lisćiku stoje pśizwólone wólbne naraźenja jadnotliwego z pódaśim familijowego mjenja a pśedmjenja.\n",
            "Krajne kněžarstwo jo statnego sekretarja ako prědnego zagronitego za nastupnosći Serbow pówołało.\n",
            "§ 19 wótstawk 2 a 3 komunalneje wustawy Bramborskeje płaśi wótpowědnje.\n",
            "Pśed rozsuźenim ma se wólone zastupnistwo gmejnskego zwězka słyšaś.\n",
            "Śěgowy wjednik jo se zajěł.\n",
            "Nabraśe kreditow, kenž mogu w pśiducych góspodarskich lětach wudawki zawinowaś, trjebaju kaznisku połnomóc.\n",
            "Wjednica abo wjednik wólbow dajo pśizwólone wólbne naraźenja jadnotliwego w tom rěźe, ako su póstajne.\n",
            "(3) Rada za serbske nastupnosći póraźujo krajny sejm.\n",
            "Možnosć dwojorěcnych popisanjow dejała se wobšyrnje wužywaś.\n",
            "Wótpósłańce maju pšawo, za fakty, kenž su w toś tej funkciji dowěrliwje zgónili, znankstwo wótpokazaś.\n",
            "Kuždy ma pó kazni pšawo na glědanje do aktow, dalokož tomu napśeśiwo njestoje wažne zjawne abo priwatne zajmy.\n",
            "Pśi chłostanju ma se na cłowjecnu dostojnosć źiwaś.\n",
            "Kraj zarucyjo pósrědnjenje znaśow w ramiku wukubłanja, do- a dalejkubłanja wótkubłarkow a wótkubłarjow a wucecych.\n",
            "Som z wuzjawjenim wó šćiśe datow wobjadny.\n",
            "Rozsud ma se wótpósłańcoju zdźěliś a wobtwarźiś.\n",
            "Za to wužywamy cookije a analyzowy rěd Matomo.\n",
            "Dalej zaběgujo postajony cas teke pśez pšosby, zdźěleńki abo wuzjawjenja wole, spisane w serbskej rěcy.\n",
            "Wóno dej ako zaměr měś, chłostańca wuzamóžniś, w pśiducem casu wjasć swójo žywjenje we socialnej zagronitosći bźeze chłostańskich njestatkow.\n",
            "Kuždy ma pšawo se z powšyknje pśistupnych abo drugich pó pšawje zwužywajobnych žrědłow informěrowaś.\n",
            "To jo tema slědnego zwucowanja.\n",
            "Pśi tom ma se wósebnje glědaś na zjawny zajm nješkódnego wužywanja zemje.\n",
            "Jadnański běrow stajijo pśedsedarstwam listowych wólbow te za jich źěłabnosć trěbne pomocniki a pomocne srědki k dispoziciji.\n",
            "Krajnej barwje stej cerwjena a běła.\n",
            "Pśepytowanje abo konfiscěrowanje w rumnosćach krajnego sejma smějo se jano staś z pśizwólenim prezidenta krajnego sejma.\n",
            "(4) Wuswójenje jo jano za derjeměśe wšych luźi dowólone.\n",
            "Wjednica abo wjednik wólbow dajo toś ten nachylny wuslědk wólbow wustnje abo w se góźecej formje k wěsći.\n",
            "Zarědowanje institucijow za wobradowanje pśi chórosći ako teke za druge socialne a karitatiwne zaměry matej se wót stata spěchowaś.\n",
            "(3) Teritorij gmejnskich zwězkow móžo se pśez kazń abo na zakłaźe kazni změniś.\n",
            "(2) Krajny sejm móžo dalšych zagronitych wuzwóliś. Wótrězk 1 sada 3 wótpowědnje płaśi, dalokož njejo nic drugego kazniski póstajone.\n",
            "Pytańske zapśimjeśe zapódaś.\n",
            "Dwojorěcnje wugotowaś maju se směrniki nr. 432 po § 42 StVO k nutśikoměstnym cilam a k městnam z wjelikim wuznamom za wobchad.\n",
            "Pódla togo se licba zgromaźonych wólbnych łopjenow licy.\n",
            "(3) Wólby a ludowe wótgłosowanja su powšykne, njepósrědne, jadnake, lichotne a pótajmne.\n",
            "Šćitne gronidło jo se změniło.\n",
            "(4) W sedleńskem rumje Serbow ma se serbska rěc do zjawnych napismow zapśěgnuś.\n",
            "Zwězk móžo, bźez togo až jo w swójich pšawach zranjony, zapódaś pšawniske srědki pśeśiwo napšawam komunalnego regionalnego zjadnośeństwa.\n",
            "Wó takem naraźenju ma se w njezjawnem pósejźenju rozsuźiś.\n",
            "Zapódajśo pšosym swójo wužywaŕske mě adresu.\n",
            "Tak daloko ako njejo w tom póstajenju hynacej wustajone, muse pśedpisane wuzjawjenja pla pśisłušnego městna w originale pśedlažaś.\n",
            "Zagronity cłonk krajnego kněžarstwa za nastupnosći Serbow dostanjo połnomóc.\n",
            "Dwojorěcnje wugotowaś maju se směrniki nr. 437 po § 42 StVO, nadrozne tofle.\n",
            "Po SK deje te tam pomjenjone amty a komunalne teritorialne korporacije pomjeniś zagronite za nastupnosći Serbow.\n",
            "Wón teke njesmějo policiju w zmysle amtskeje pomocy wó napšawy pšosyś, za kótarež sam njejo wopšawnjony.\n",
            "Šule we serbskem sedleńskem rumje maju słušnosć, starjejše informowaś.\n",
            "To wótpadnjo, gaž se pódłožki listowych wólbow na zwenka Zwězkoweje republiki Nimska lažece městno pósćelu.\n",
            "Kuždy kandidat musy póžedanje na zapisanje do zapisa wólarjow zapódaś nejpózdźej až do zakóńcenja casa zapódaśa wólbnych naraźenjow jadnotliwego.\n",
            "Wólbny list ma se wótpokazaś, gaž njejo se wužywał wót jadnańskego běrowa wudany głosowański lisćik.\n",
            "Kněžarstwo wobzamknjo swóje rozsuźenja z wětšynu głosow.\n",
            "K pšawam, kotarež ma kuždy Serb, bydlecy w kraju Bramborska, słuša pšawo, swoju identitu licho zwuraznjowaś.\n",
            "Familijowe mě dej nanejmjenjej dwanasćo znamuškow měś.\n",
            "Kraj móžo gmejny zawězaś a se pśi tom pšawo pśikaza pó pšawniskich pśedpisach wobchowaś.\n",
            "Kraj se wó to stara, až se w krajnem wobcerku žedne atomowe, biologiske abo chemiske broni njewuwijaju, njegótuju a njeskładuju.\n",
            "(1) Wuměłstwo jo lichotne. Wóno trjeba zjawne spěchowanje, wósebnje za pódpěru wuměłcow.\n",
            "Conk zgubijo swójo cłonkojstwo w raźe pśez nowe zwěsćenje wólbnego wuslědka abo wótpadnjenje wuměnjenjow wopšawnjenja do wólbow.\n",
            "Pušćenje ze źěła abo discipliněrowanje dla sobustatkowanja w bergarskich gibanjach, zwězkach, religioznych zgromaźeństwach abo partajach njejo dowólone.\n",
            "Zwucuj pśed źewjeś źewjeśich zajtša, aby licbu dypkow pówušył.\n",
            "(2) Kraj se procujo wó zawěsćenje kulturneje awtonomije Serbow pśez krajne granice.\n",
            "Zwěsćijo-lic brachy, tak wóna abo wón informěrujo wósobu dowěry a napominajo ju, wurownajobne brachy scasom wótpóraś.\n",
            "Zapśimjeśa a wobgranicowanja směju se pśewjasć k šćitu wobgrozonych źiśi a młodostnych.\n",
            "Nejlěpjej wuzwól sebje how gódło.\n",
            "Wobśěžkanje musy se w jadnańskem běrowje zapódaś.\n",
            "(4) Cłonki rady wugbaju swójo zastojnstwo cesnoamtski.\n",
            "(1) Wusoke šule maju we wobłuku kaznjow pšawo na samozastojnstwo, pśi kótaremž su wuwucujuce, druge pśistajone a studenty wobźělone.\n",
            "Poruca se, až se dopołnjenje toś togo nadawka za zjawnosć zrozymliwe ( na pś. pśez wotpowědne woblicenja w etatu abo projekty).\n",
            "(1) Wólbne naraźenja jadnotliwego maju se pisnje pla wjednice abo wjednika wólbow zapódaś.\n",
            "Słušnosć spěchowaś kulturu dopołnijo kraj wosebnje pśez swojo wobźělenje na Załožbje za serbski lud.\n",
            "Jo-lic se jano jadna wósoba dowěry pomjenijo, tak płaśi ta wósoba ako zastupujuca wósoba dowěry.\n",
            "Wólbna listowa wobalka na se zacyniś.\n",
            "Šćitne gronidło nic změniś.\n",
            "Wopśimjeśe pśepytaś.\n",
            "Do toś togo póla zapiš wužywaŕske mě.\n",
            "Źiwajucy na mjazynarodne zawězki k šćitoju a spěchowanju narodnych mjeńšynow wobzamknjo krajny sejm.\n",
            "Móžo se teke z religioznym zlubjenim pśisegaś.\n",
            "Zwucowanje jo se pakśikoju južo raz pśidało.\n",
            "(2) Rěd narownańskich wósobow ma se pó licbje za nju wótedanych głosow.\n",
            "Dalej zaběgujo postajony cas teke pśez pšosby abo wuzjawjenja wole, spisane w serbskej rěcy.\n",
            "Kraj zawěsćijo spěchowanje struktury regionow ze zaměrom, stwóriś we wšych krajnych źělach rownogódne źěłowe wuměnjenja.\n",
            "Wótpokazane wólbne listy maju se zasej zacyniś a běžnje numerěrowaś.\n",
            "Nadawk pśepytowanja njesmějo se pśeśiwo wóli togo, kenž jo pšosbu stajił, pśeměniś.\n",
            "(1) Dalejkubłanje dorosćonych ma se wót kraja, gmejnow a gmejnskich zwězkow spěchowaś.\n",
            "Nuriś se móžo kuždy.\n",
            "We internatach dejtej se serbska rěc a kultura na pśigodnu wizu woplěwaś.\n",
            "Pšawo serbskego luda na šćit swójeje narodneje identity a swójogo starodawnego sedleńskego ruma se zarucyjo.\n",
            "Kuždy ma pšawo, se w zgromadnosći abo w zgromadnosći z drugimi wobrośiś z póstarkom, na krajny sejm.\n",
            "Wón jo wuknjeński wuspěch dojśpił.\n",
            "Nadrobnosći rědujo jadnański pórěd krajnego sejma.\n",
            "Wó Wuwześa rozsuźijo krajny sejm.\n",
            "Wólbnemu naraźenjoju jadnotliwego maju se pśipołožyś wuzjawjenje kandidatki, až wóna pśigłosujo wólbnemu naraźenjoju jadnotliwego.\n",
            "Ned gaž jo pśedsedarstwo listowych wólbow wuslědk listowych wólbow zwěsćił, dajo jen jaden cłonk wjednikoju wólbow k wěsći.\n",
            "Kazń zmóžnijo pśipóznaśe kšywowych zwězkow serbskich zjadnośeństwow a towaristwow.\n",
            "Wózjawjenje wopśimjejo za kužde wólbne naraźenje jadnotliwego te pomjenjone daty.\n",
            "Wideo jo se pakśikoju južo raz pśidało.\n",
            "Sudniki wóle se na amtski cas wót pěśich lět.\n",
            "Wjednik wólbow jo k pśiwześu takego wobwěsćenja město pśisegi pśisłušny.\n",
            "Gaž se derjeměśe wobgrozyjo, wósebnje pśez njedocynjenje zagronitych za wótkubłanje, ma komuna kazniski zrědowane napšawy pśewjasć.\n",
            "Chtož źiśi wótkubłujo, ma pšawo na pśiměrjonu statnu pomoc a towarišnostnu rozglědniwosć.\n",
            "Pśepšosenja na pósejźenja dejali cłonki nanejpózdźej 24 góźinow pśed tym gromaźe z dnjownym pórědom dostaś.\n",
            "Wjednik wólbow zdźělijo prezidentoju krajnego sejma familijowe mjenja, pśedmjenja a adrese.\n",
            "Serbska kazń (SK) dawa abo serbskemu ludoju tak teke kuždemu jednotliwemu Serboju pšawa.\n",
            "Pśed jich wólbu pśewjeźo se pśipósłuchanje w jadnom wót krajnego sejma póstajonem wuběrku.\n",
            "Lichota jich sobustatkownosći pśi politiskem twórjenju wóle ma se zawěsćiś.\n",
            "To samske płaśi, dalokož se jadna wó temy zasadnego wóznama.\n",
            "(1) Wšykne luźe su pśed kaznju jadnake.\n",
            "Wětšy źěl se kazniskim pominanjam wotpowědujo, gaž wugbawa jaden sobuźěłaśer amta abo teritorialneje korporacije toś ten nadawk pśidatnje.\n",
            "Wobstoj pšawo na wótegrono w pśiměrjonem casu.\n",
            "Wuslědki pśespytowanja se krajnemu sejmoju a krajnemu kněžarstwoju w lětnej rozpšawje pśepódaju.\n",
            "Póstoj na głowje lěpjej njecyńśo w žołtem sněgu.\n",
            "Wóna abo wón płaśe ako zastojnstwo w zmysle Knigłow pokuśeńskich kaznjow.\n",
            "Wobdarjone, socialnje slědk stajane abo zbrašne luźe maju se wósebnje spěchowaś.\n",
            "Jo dowólone, kandidatku abo kandidata ako wósobu dowěry abo zastupujucu wósoby dowěry pomjeniś.\n",
            "W starodawnem sedleńskem rumje statkujo kraj na to, až se źiwa na pśiswójenje dolnoserbskich rěcnych znajobnosćow w pórucenjach.\n",
            "Stakim se źiwa na cil, koordinaciju ministarstwow w Serbow nastupajucych pšašanjach pólěpšyś.\n",
            "Wón smějo je jano w paźe njedocakowaneje a njewótpokazajobneje pótrěbnosći dowóliś.\n",
            "Wólbnemu naraźenjoju jadnotliwego maju se pśipołožyś wobwěsćenje město pśisegi kandidatki abo kandidata pó wótstawku 5 sada 1.\n",
            "Wótpósłańce se pó takej wašni póstupowanja wóle, kótaraž wólbu wósobiny ze zasadami poměroweje wólby zwěžo.\n",
            "Zapódaj šćitne gronidło za pśistup k zwucowańskemu pakśikoju.\n",
            "Njepłaśiwe su głose, gaž głosowański lisćik njejo njecwibelnje póznaś.\n",
            "We tom paźe postupujo wuknik we pśedmjaśe serbšćina ako druga rěc po kuždem šulskem lěśe do pśiducego wušego lětnika.\n",
            "Šćitne gronidło změniś.\n",
            "Pšosym składowaś.\n",
            "Bramborska jo lichotny, pšawostatny a socialny kraj.\n",
            "Bramborska jo kraj, kótaryž se procujo wó zgromadne źěło z drugimi ludami, wósebnje z pólskim susedom.\n",
            "Zastojaŕka abo zastojaŕ nawjedujo źěłabnosć pśedsedarstwa listowych wólbow.\n",
            "Numer 6 toś tych postajenjow zgubijo płaćiiwosć z 31. julijom lěta 2001.\n",
            "To njejo źedno dokońcne, ale jano pśikładowe nalicenje kriteriumow.\n",
            "Na šulach deje se zasajźiś ceptarki a ceptarje, kotarež serbsku rěc we pominanej wobšyrnosći wobkněže.\n",
            "Kraj, gmejny a gmejnske zwězki spěchuju zwopšawźenje togo pšawa.\n",
            "Z tym se kóńcy wólbny cas pjerwjejšnego krajnego sejma.\n",
            "Gmejny spěchuju wuměnjenja, kótarež zmóžniju bergarkam a bergarjam, swóju rěc zachowaś a dalej wuwijaś.\n",
            "Njenamakajo-li pšosba ministarskego prezidenta na krajny sejm pśigłosowanje wětšyny, móžo se krajny sejm za cas dwaźasća dnjow rozpušćiś.\n",
            "Dojadnanja, kenž toś to pšawo wobgranicuju abo jomu zadoraś wopytuju, su knicomne a na to wusměrjone napšawy su njekazniske.\n",
            "Toś ta kazń se w nimskej a serbskej rěcy wózjawijo.\n",
            "My, bergarki a bergarje kraja Bramborska, smy sebje wólny rozsud dali zaměrnje wugótowaś zwězkowy kraj Bramborska.\n",
            "We wobłuku swójich nadawkow mogu se teke ze swójskeje iniciatiwy z nastupnosću zaběraś a krajnemu sejmoju pórucenja pśedpołožyś.\n",
            "Pśisłušniki staty statow a bźezstatne z městnom pśebywanja w Bramborskej su Nimcam w zmysle Zakładneje kazni rownopšawne.\n",
            "Jo-lic se dosć góźecych wósobow njenaraźijo, pówołujo wjednik wólbow dalšne cłonki pó swójom rozsuźenju.\n",
            "Wuzjawjenje z wuměnjenim se woglědujo ako wótpokazanje.\n",
            "Nabranki z kreditow njesměju te w góspodarskem planje woblicone wudawki za inwesticije pśestupiś.\n",
            "Zarědnik wótpóraś.\n",
            "Do wucby woglědowaś ma se postupujucy w rědomjach a po šulskich stawach a schojźeńkach, z woglědowanim wucby wopśestaś možo se jano ku końcoju šulskego lěta.\n",
            "Rozsuźijo se pó napšašowanju gmejny a gmejnske zwězki zwězkowego pśedsedaŕstwa.\n",
            "Pśizjawjeńske daty njejsu pšawe.\n",
            "Wótlět se wokomuźijo wó někotare góźiny.\n",
            "W paźe pśestupjenja z wótpoglědom móžo se pušćenje wugroniś.\n",
            "Wóni spěchuju wót tradicije a tolerance pregowane zgromadne žywjenje jich wobydlarkow a wobydlarjow.\n",
            "Kraj, gmejny a gmejnske zwězki spěchuju wósebnje kulturnu samostatnosć a statkowne politiske sobupóstajenje serbskego luda.\n",
            "Pó § 5 Serbskeje kazni ma Serbska rada pśi krajnem sejmje Bramborska nadawk, zajmy Serbow zachowaś.\n",
            "Kužde zjadnośeństwo ma swójim wólbnym naraźenjam jadnotliwego pśipołožyś kopiju protokola wó póstajenju kandidatow.\n",
            "Zapowěźejo jaden cłonk pśedsedarstwa listowych wólbow swój pódpis, tak ma se pśicyna togo w protokolu zapisaś.\n",
            "(3) Nadrobnosći rědujo sejm.\n",
            "Mimo šćitoju ma se winowatosć k spěchowanju kultury ako kubłański koncept serbskego luda.\n",
            "(2) Ministarski prezident a na jogo pšosbu teke ministarje maju winowatosć, swóje nadawki až do amtskego nastupa jich naslědnikow dalej wugbaś.\n",
            "(1) Rada za serbske nastupnosći se wuzwólijo pśecej za jadnu legislaturnu periodu krajnego sejma.\n",
            "Maćica Serbska pósći kužde druge lěto Myto Arnošta Muki.\n",
            "Nowe šćitne gronidło zapódaś.\n",
            "Póžedajo jaden cłonk pśedsedarstwa listowych wólbow pśed pódpisanim protokola wóspjetne licenje głosow, tak ma se to wóspjetowaś.\n",
            "Pózdźej dopomnjeś.\n",
            "Pśedsedanje wugbajo wótpowědny ministaŕ bźez pšawa na głosowanje.\n",
            "(2) Wólbny wuběrk jadna a rozsuźijo w zjawnem pósejźenju.\n",
            "Pó wótběgnjenju naraźeńskego casa pówołujo wjednica wólbow bźez komuźenja zastojaŕku listowych wólbow a zastupnicu ako teke pśisedaŕki.\n",
            "Nadawki krajnego zagronitego za Serbow njewobgranicuju se na koordinaciju ministarstwow w nastupnosćach Serbow.\n",
            "Mysli pak na to, až móžoš jano z płaśiweju mejlkoweju adresu swójo šćitne gronidło wobnowiś, jolic jo zabydnjoš.\n",
            "Tam jo material k pakśikoju.\n",
            "Liche wubraśe wucbnych a wuknjeńskich srědkow ma se pśez kazń rědowaś.\n",
            "Namrěty sedleński rum (§ 3 SK) Gmejnam pśistoj pśepytowanje a zwěsćenje, lěc woni pśisłušuju k namrětemu sedleńskemu rumoju.\n",
            "Gmejny w starodawnem sedleńskem zapśimuju serbsku kulturu wótpowědujucy do swójeje kulturneje źěłabnosći.\n",
            "Źěłowe łopjeno wótpóraś.\n",
            "Wuknjeńske wuspěchy gromaźiś.\n",
            "Na zajmy Serbow ma se pśi wugótowanju krajneje a komunalneje politiki źiwaś.\n",
            "Pšosym wótzjawiś se.\n",
            "Z dalšnym wužywanim internetoweje strony pśigłosyjośo wužywanju toś tych.\n",
            "Zwiganje wažydłow jo jano pśi grawitaciji zmysłapołne.\n",
            "Toś to póstajenje płaśi za njepósrědne wólby Rady za nastupnosći Serbow.\n",
            "Dwě sprětej łopjeni wisatej na gałuzce.\n",
            "To jo zawod.\n",
            "Pśistupne daty zabył.\n",
            "Dalokož móžo zakładne pšawo pśez kazń abo na zakłaźe kazni wobgranicowaś, ma se zasada poměrnosći doźaržaś.\n",
            "Prezident krajnego sejma póstajijo w wobjadnosći z prezidiumom krajnego sejma źeń wólby.\n",
            "(2) Iniciatiwy ku krajnemu etatu, k słužbnym a zastarańskim nabrankam, wótedankam a personalnym rozsudam njejsu dowólone.\n",
            "To jo wětšy źěl tak, gaž se woplěwaju serbske nałogi.\n",
            "We serbskich šulach z wosebnym charakterom ma se serbska rěc tek zwonka wucby spěhowaś a z rosćecym rěcnym zamoženim wuknicow.\n",
            "Pśed wólbu pśewjeźo se pśipósłuchanje w jadnom wót krajnego sejma póstajonem wuběrku.\n",
            "Wón zwěsćijo licby wótedanych płaśiwych głosow za jadnotliwu kandidatku a jadnotliwego kandidata (wólbne naraźenja jadnotliwego).\n",
            "(3) Nadrobnosći rědujo kazń.\n",
            "Wólbne naraźenja mógu se zapódaś pla zjadnośeństwow, kótarež se w swójich wustawkach ku serbskim cilam wuznawaju.\n",
            "Znjewužywanje góspodarskeje mócy njejo dowólone a tomu zadoraś.\n",
            "Ministarski prezident a ministarje krajnego kněžarstwa do pśiwześa nadawkow pśed krajnym sejmom slědujuce pśisegaju.\n",
            "Nimce maju w Bramborskej samske pšawa a winowatosći, dalokož njewobstoj za bergarjow Bramborskeje kazniske wobmyslenje.\n",
            "Na w dolnoserbskej rěcy stajone pšosby, móžo se w dolnoserbskej rěcy wótegroniś a wó nich rozsuźiś.\n",
            "Kuždy ma pšawo na šćit swójeje njewobškóźonosći pśed zranjenjami a njeznjasliwymi wobgrozenjami.\n",
            "Pśedsedarstwo Maćice Serbskeje póstajijo wuběrk za pósuźowanje zapódanych źěłow.\n",
            "Zagronity cłonk krajnego kněžarstwa za wobchad dostanjo połnomóc, dokradnosći k § 11 rědowaś.\n",
            "Kandidatka ma napśeśiwo wjednicy město pśisegi wobwěsćiś, až jo wóna do wuzwólowanja krajnego sejma wopšawnjona.\n",
            "Kužde wólbne naraźenje jadnotliwego musy wopśimjeś familijowe mě, pśedmě a pówołanje abo źěłabnosć.\n",
            "Pśipóznawajucy wólu Serbow, swóju identitu teke w pśichoźe skšuśiś, wobzamknjo krajny sejm slědujucu kazń.\n",
            "Pśi wokrejsach w starodawnem sedleńskem rumje ma se zagronity za serbske nastupnosći.\n",
            "(2) Pótrěbnosć na šćit drugich žywjeńskich zgromaźeństwow, kenž maju na pśecej traś, se pśipóznajo.\n",
            "Nadrobnosći rědujo kazń, kótaraž móžo pjerwjejše wupócerje pšawniskego póstupowanja a wósebne jadnanje pśed pśiwześim wobśěžkowanja pominaś.\n",
            "We njom muse wšykne frakcije zastupjone byś.\n",
            "Wustawowe sudnistwo rozsuźijo we wšych drugich nastupnosćach, ako su se jomu z toś teju wustawu abo pśez kazń pśipokazali.\n",
            "Woni muse se po nutśikownem diferencěrowanju rozwucowaá a godnośiś po postajenjach potrjefjonego kubłanskego pśeběga.\n",
            "Pśedłoga pśiwzeta, gaž nanejmjenjej běrtyl bergarjow z pšawom na głosowanje jo pśigłosył.\n",
            "Wědobny sebje, až ma kraj wósebnu zagronitosć za šćit a spěchowanje narodneje identity Serbow, wobzamknjo krajny sejm.\n",
            "Pśed kuždym sudnikojskim rozsudom wó dalejtraśu jadnogo popajźeństwa ma se pótrjefjonemu móžnosć daś, pšawniskego zastupnika k tomu braś.\n",
            "Spěchowanje kubłanišćow a wótwardowanje w zmysle granice a nadawkow su nadawki załožby.\n",
            "Wobsejźarje a wugbarje załožkow maju wótpowědnu słušnosć wózjawjenja.\n",
            "Wuměłske twórby a pomniki kultury stoje pód šćitom kraja, gmejnow a gmejnskich zwězkow.\n",
            "Pšosba wó rozpušćenje krajnego sejma trjeba pśigłosowanje nanejmjenjej wót dwěsće tysac bergarjow z pšawom na głosowanje.\n",
            "Wjednica abo wjednik wólbow pśedpołožyjo wólbnemu wuběrkoju wše zapódane wólbne naraźenja jadnotliwego.\n",
            "Kuždy ma pšawo na toś te informacije, dalokož napśeśiwo njestoje pśewažece zjawne abo priwatne zajmy.\n",
            "Dalokož njedajo se pśiměrjona móžnosć za źěło dopokazaś, wobstoj pšawo na pśekubłanje, powołańske dalejkubłanje a na zežywjeńsku pódpěru.\n",
            "Wjednik wólbow jo k pśiwześeju pśisłušny.\n",
            "Wótpósłańc, kótaryž swóju wědu na taku wašnju znjewužyjo, až se naglědnosć krajnego sejma gropnje wobgrozyjo, móžo se pśeskjaržyś.\n",
            "Wobradowanja wuběrka a lisćina zapódanych źěłow su dowěrliwe.\n",
            "(2) Wobsejźeństwo napołožyjo winowatosći.\n",
            "Pšawo serbskego luda na zachowanje swójeje narodneje identity a swójogo starodawnego sedleńskego ruma se zarucyjo.\n",
            "(2) Za wóspjetne wólby płaśe pón pśedpise Serbskeje kazni a togo póstajenja.\n",
            "Pśepytowanja směju se w tšachu dla komudy teke wót kaznjach pśedwiźonych organow póstajiś.\n",
            "Pówołujucy se na artikel 3 Zakładneje kazni a na protokolnu noticu cysło 14 k artikloju 35 Zjadnośeńskego dogrona wobzamknjo krajny sejm.\n",
            "Wjednica wólbow zdźělijo prezidentce krajnego sejma ned, kótare kandidatki a kandidaty su wólby wótpokazali.\n",
            "Za pśespytowanje wótgłosowanja ludowego rozsuda płaśe te z krajom Barliń dojadnane wótchylne rědowanja w statnem dogronje.\n",
            "Kócki pórěźaju kompjuter.\n",
            "Kraj, gmejny a zwězki gmejnow w starodawnem sedleńskem rumje Serbow zarucuju toś to pšawo.\n",
            "Pokazku na pytane słowo pokazaś.\n",
            "Pśi toś tom licenju se wusortěruju a njeźiwa na głosowańske lisćiki, kótarychž płaśiwosć njejo wěsta.\n",
            "Na njej dej stojaś powěsć, až jo do wuzwólowanja wopšawnjona wósoba do zapisa wólarjow zapisana.\n",
            "Centralnego pśigranjańskego partnarja za Serbow z boka krajnego kněžarstwa zasajźiś.\n",
            "W namrětem sedlenskem rumje se pśizwoluju ludoju: dwojorěcne popisanja drogow, puśow, naměstow a mostow.\n",
            "(4) Wólbny wuběrk móžo wobzamknuś, gaž jo pśedsedaŕka abo pśedsedaŕ pśipódla.\n",
            "Kraj se z pomocu financielnego wurownanja wó to stara, až gmejny a gmejnske zwězki swóje nadawki społniś mogu.\n",
            "(2) Kuždy dla chłostajobnego njestatka wobwinowany abo pśeskjaržony ma se tak dłujko ako njewinowaty wobglědowaś, dłujkož njejo pšawomócnje zasuźony.\n",
            "Kubłanišćo dej nanejmjenjej źaseś znamuškow měś.\n",
            "K notnym wěcnym kostam lice kosty za pśestajenje wózjawjenjow a pśedśišćanych formularow w serbskej rěcy.\n",
            "Pśipóznawajucy wólu Serbow, kótarež su wót stolěśow wósebnje we Łužycy doma, wobzamknjo krajny sejm.\n",
            "(2) Z njeźelami a swěźenjami zwězane tradicije maju se wažyś.\n",
            "Rozsudy pódlaže sudniskej kontroli.\n",
            "(3) Jo-lic nastanu wobmyslenja pśeśiwo wólbnemu listoju, tak rozsuźijo pśedsedarstwo listowych wólbow wó pśizwólennju abo wótpokazanju.\n",
            "(3) Pśistup k studiumoju na wusokej šuli ma kuždy, kenž jo złožył wusokošulsku zdrjałosć.\n",
            "Pśizjawjeńske daty njejsu korektne.\n",
            "Redaktory strony Serbski kšac pó kšacu deje Was nejpjerwjej ako nowego wužywarja pśizwoliś.\n",
            "(8) Źěło wót źiśi jo zakazane.\n",
            "Źěłoju ma se pśipołožyś list.\n",
            "Wón póstajijo rownocasnje cas, na kótaremž wólbny cas listowych wólbow se na tom slědnem dnju zakóńcyjo (kóńc wólbnego casa).\n",
            "Pcołki zběraju nektar.\n",
            "Dataju zlodowaś.\n",
            "To wótpowědne płaśi za religiozne zgromaźeństwa.\n",
            "Dalšne śěžyšcó serbskego zasedlenja wobstoj w Górnej Łužycy w Lichotnem staśe Sakska.\n",
            "Wjednica abo wjednik wólbow ma se wó to staraś, až jadnański běrow rozsuźenje ned dostanjo.\n",
            "Nadrobnosći rědujo jogo jadnański pórěd.\n",
            "Za ten part pśirěduju se woni tej kupce, za kotaruž jich rěcne zamožnosći dosegaju.\n",
            "Wótwólenje ma płaśiwosć, gaž stej dwě tśeśinje cłonkow krajnego sejma pśigłosyłej.\n",
            "To jo licba pśizjawjenjow.\n",
            "To su góźiny.\n",
            "Z tym su byli zwězane tśuśa politiskeje, kulturneje a konfesionalneje autonomije a pluratity.\n",
            "Elektroniska forma jo wuzamknjona.\n",
            "Protokoloju maju se dodaś głosowańske lisćiki a wobalki głosowańskich lisćikow, wó kótarež jo pśedsedarstwo listowych wólbow wósebnje wobzamknuło.\n",
            "Rozsuźenje ma se na pósejźenju wólbnego wuběrka k wěsći daś.\n",
            "Rozwěž wuspěšnje wšykne nadawki 20 lekcijow!\n",
            "(3) Zwěsćenja pó wótstawkach 1 a 2 trjefijo prezidentka abo prezident krajnego sejma.\n",
            "Na njej dej stojaś pódaśe slědnego dnja listowych wólbow a kóńc wólbnego casa.\n",
            "Zapśimjeśa a wobgranicowanja směju se jano pśewjasć k woboranju powšykneje tšašnosći abo w tšachu wó žywjenje za jadnotliwe wósoby.\n",
            "Do wuzwólowanja wopšawnjone su wšy Serby, kenž su na slědnem dnju listowych wólbow za wólby ko Krajnego sejma Bramborskeje do wuzwólowanja wopšawnjone.\n",
            "Pakśik pśeměnjowaś.\n",
            "Wó wótpokazanju póžedanja musy se póžedajuca wósoba nejmalsnjej informěrowaś.\n",
            "Za źěle góspodarskego plana móžo se pśedwiźeś, až wóni płaśe za rozdźělne casowe wobłuki, źělone góspodarskich lětach.\n",
            "(5) Žurnalistiskej źěłabnosći pó pšawje njesmějo se pśez winowatosć dopokaza zamóžnosći, skonfiskowanje a pśepytowanje zadoraś.\n",
            "Dalokož njejo žednogo rěcnych znajobnosćow.\n",
            "(1) Krajny liceński dwór jo samostatne, jano kazni powdane nejwuše krajne zastojnstwo.\n",
            "Njepłaśiwe su głose, gaž głosowański lisćik ma někaki dodank, ma někake wuměnjenje abo pśerězany.\n",
            "Wužywanje zemje wót rolnikarstwa a gólnikarstwa ma se wusměriś na ekologisku znjasliwosć.\n",
            "(4) Rozpšawy pśepytowańskich wuběrkow njepódlaže sudniskej kontroli.\n",
            "Kraj a gmejny su zawězane, se staraś wó rownogódnosć žywjeńskich wuměnjenjow luźi ze zbrašnosćami abo bźeze zbrašnosćow.\n",
            "Kužda frakcija ma pšawo, nanejmjenjej z jadnym cłonkom w kuždem wuběrku zastupjona byś.\n",
            "Nowe šćitne gronidło wóspjetowaś.\n",
            "Wobglědujo se pśedmjat, nic pak na realnej šuli ako nimšćina, matematika, prědna cuza rěc abo ten wuzwolony wolnosłušny pśedmjat.\n",
            "Źěłowe łopjeno jo se wuspěšnje zwucowańskemu pakśikoju pśidało.\n",
            "Wustawowe sudnistwo rozsuźijo wó pśezjadnosći jadneje krajneje kazni z toś teju wustawu.\n",
            "To pótrjefijo pśede wšym krajne granice pśesegajuce statkowanje institucijow za woplěwanje a wuslěźenje rěcy a stawiznow.\n",
            "Kraj ma ze swójim etatowym góspodarjenim pśirodnych žywjeńskich zakładow něntejšnych a pśiducych generacijow wótpowědowaś.\n",
            "To su wuknjeńske wuspěchy.\n",
            "Material zwězaś.\n",
            "Šćitnej gronidle se njekšyjotej.\n",
            "(3) Zjadnośeństwa w zmysle togo póstajenja su towaristwa a zwězki, kenž se w swójich wustawkach k serbskim cilam wuznaju.\n",
            "Ako starodawny sedleński rum płaśe gmejny, w kótarychž dajo se dopokazaś kulturna tradicija až do pśibytnosći.\n",
            "Lice se jano płaśiwe „jo“- a „ně“- głose.\n",
            "To sy derje wugbał!\n",
            "Take dyrdakojske tšojenje se wulicuju ako rozbuźecy zgromadny muzikowy spektakel.\n",
            "Protokoloju maju se dodaś wólbne łopjena, wó kótarychž jo pśedsedarstwo listowych wólbow wobzamknuło, bźez togo, až su se wólbne listy wótpokazali.\n",
            "(2) Kuždy ma pšawo na wulichowanje wót źeła za powołańske, kulturne abo politiske dalejkubłanje. Nadrobnosći rědujo kazń.\n",
            "Pśed změnu teritorija gmejny ma se ludnosć direktnje pótrjefjonych stronow słyšaś.\n",
            "Nosarje maju winowatosć, wó móžnosćach nawuknjenja a woplěwanja dolnoserbskeje rěcy informěrowaś.\n",
            "W jadnotliwem paźe móžo zagronite ministarstwo pśizwóliś pó casu wobgranicowane wuwześa na region se póśěgujucych napšawow.\n",
            "Z teju rozpšawu su zwězane pśespytowanje statkownosći spěchowańskich instrumentow a wugronjenja k pśedewześam krajnego kněžarstwa.\n",
            "Kazń móžo wósebnje pśedwiźeś, až te we wótrězkach 1 do 4 pomjenjone pšawa jano ten ma, ako jo wobydlaŕ we wólbnem abo wótgłosowańskem wobwoźe.\n",
            "(3) Wótpósłańcam ma se pśistup k zastojnstwam a k słužbnym instancam kraja zmóžniś.\n",
            "(5) Zjadnośeństwa za zgromadne woplěwanje jadnogo swětonaglěda su nabóžninskim zgromaźeństwam rownogódne.\n",
            "Trjebne wěcnym za pśigótowanje a pśewjeźenje wólbow ma kraj Bramborska.\n",
            "Wuzwólona strona njejo se namakała.\n",
            "(1) Njestatk móžo se jano chłostaś, gaž běšo chłostajobnosć pśed jogo wugbaśim kazniski póstajona.\n",
            "Zastojaŕka abo zastojaŕ listowych wólbow abo wót njeje pśedcytajo z kždego głosowańskego lisćika, za kótaru kandidatku su se głose wótedali.\n",
            "Połnomóc za wudaśe pšawniskego wukaza móžo se jano pśez kazń wugroniś.\n",
            "Na pšosbu pśedsedarstwa pśedstajijo lawreat/lawreatka swójo źěło na głownej zgromaźinje.\n",
            "Město narodnego datuma a bydleńskeje adrese pak se pódaju pśecej jano lěto naroźenja a bydlišćo.\n",
            "Aby zarucało jadnotne wuloženje a pśewjeźenje SK, se podaju w slědujucem pokazki.\n",
            "Zarownaju se wudawki za dwójorěcne napisma na drogach, puśach, naměstach a móstach.\n",
            "Pśestupijo-li zjawna móc winowatosć zjawnego pšawa, tak zarucyjo jeje nosaŕ drugemu narownanje napóraneje škódy.\n",
            "Stakim se źiwa na cil, koordinaciju ministarstwow pólěpšyś.\n",
            "Gmejny maju słušnosć, wšym luźam pśistup do pśirody, pód nałožowanim teje zasady za šćit wobswěta pśirodny wobswět.\n",
            "Wjednica abo wjednik wólbow pśepšosyjo wósoby dowěry wólbnych naraźenjow jadnotliwego na pósejźenje.\n",
            "Wón móžo toś to pšawo na drugego pśenjasć.\n",
            "Z tym se wótpowědujo wuběrka za kubłanje, młoźinu a sport z dnja 28. maja 2013.\n",
            "Z klikom abo tusnjenim na pšašak dostanjoš pokiw za pšawe wótegrono.\n",
            "(1) Pśedmjat serbšćina porucyjo se ako druga rěc abo ako cuza rěc.\n",
            "Dojadnanje pó wótrězku 1 móžo pśedwiźeś, až se wót jogo nabyśa płaśiwosći sem až k wutwórjenju zgromadnego kraja pšawa pśepódaju.\n",
            "Wóno dej źiwaś na wósebne pótrěbnosći wuknikow, studentow, seniorow a zbrašnych.\n",
            "Kaznidawarstwo jo wězane na Zwězkowe pšawo a na krajnu wustawu, wugbajuca móc a pšawodawstwo stej wězanej na kazń a pšawo.\n",
            "Te w tom póstajenju pśedpisane case a terminy se njepódlejše abo se njezměnje z tym.\n",
            "Wustupujo zmólka.\n",
            "Rěd wólbnych naraźenjow jadnotliwego na głosowańskem lisćiku ma se pó licbje głosow.\n",
            "Wóni njamaju napśeśiwo drugim narodnym mjeńšynam (n.p. Danam w Schleswig-Holsteinskej) žeden „maśeriny kraj” zwenka nimskich granicow.\n",
            "Zagronity cłonk krajnego kněžarstwa dostanjo połnomóc, pśez pšawniske póstajenja dokradnosći rědowaś.\n",
            "pśistajonych a jich Źěłarnistwa maju pó póstajenju kaznjow pšawo na zakłaźe kazni we nastupnosćach a medijowe pśedewześa w nastupnosćach Serbow.\n",
            "Móžośo na pśikład zwucowanja indiwidualnje zestajaś a swójski material składowaś.\n",
            "wobstarosć dostawaju maśerje, sam wótkubłajuce a familije z wjele źiśimi ako teke familije ze zbrašnymi swójźbnymi.\n",
            "Chtož jo se do zapisa wuzwólowarjow zapisał, móžo swójo głosowańske pšawo pśez listowe wuzwólowanje wugbaś.\n",
            "Trěbnych jo nanejmjenjej źewjeś znamuškow.\n",
            "Conk zgubijo swójo cłonkojstwo w raźe pśez wótpadnjenje pśicynow za pówołanje narownańskeje wósoby.\n",
            "(2) Pó zakóńcenju casa zapódaśa (§ 19 wótstawk 2) mógu se jano hyšći brachy zapšawym płaśecych wólbnych naraźenjow wótpóraś.\n",
            "Aby se zarucało jadnotne wuloženje a pśewjeźenje, se podaju w slědujucem pokazki.\n",
            "Kazniska pśedłoga jo pśez ludowy rozsud pśiwzeta, gaž wětšyna tych, ako su swój głos wótedali, jo pśigłosył.\n",
            "Jolic pśisłušne ministartswo zwěšćijo, až wuměnjenja za pśisłušnosć njepśedlaže, ma se krajny sejm wó tom informěrowaś.\n",
            "Wólbny list ma se wótpokazaś, gaž wólaŕ pódpisał njejo.\n",
            "Pśi registrěrowanju se zrazom twój aktualny wuknjeński staw składujo a móžoš dokradnje tam pókšacowaś, źož sy pśestała.\n",
            "To pak płaśi za kubłański serwer).\n",
            "Pismawjednica abo pismawjednik wjeźo wó kuždem pósejźenju protokol.\n",
            "Zastojniki złože słužbnu pśisegu.\n",
            "Pomoc za njeznate słowa a serbski pšawopis namakajoš tam.\n",
            "Na twóju mejlkowu adresu, kótaraž jo na twójom wužywaŕskem konśe składowana, sy dostał mejlku z linkom, kak maš swójo šćitne gronidło slědk sajźiś.\n",
            "(2) Lichota casnikarstwa, rozgłosa, filma a drugich masowych medijow jo zawěsćona.\n",
            "Njejoli góspodarski plan za pśiduce lěto zawěsćony, tak ma krajne kněžarstwo až do jogo nabyśa płaśiwosći połnomóc.\n",
            "Protokoloju maju se dodaś wólbne listy, kótarež jo pśedsedarstwo listowych wólbow wótpokazało.\n",
            "Nakliknjony link jo njepłaśiwy.\n",
            "(1) Wó powołanju do sudnikojskego amta rozsuźijo wótpowědny ministaŕ gromaźe z wuběrkom za wólbu sudnikow.\n",
            "Wólby wóstanu płaśiwe, gaž zwěsćone pśejźenje wuslědk wólbow njewobwliwujo abo se jano snadnje wustatkujo.\n",
            "(2) Do zapisa wólarjow směju se jano do wuzwólowanja wopšawnjone wósoby zapisaś, kenž su swójo zapisanje pó § 12 póžedali.\n",
            "Wón wabi za pomjenjone pórucenja wukubłanja, do- a dalejkubłanja.\n",
            "Kšywowe zwězki pomjeniju wuzwólowarski wuběrk, kótaryž wuzwólijo ze swójich cłonkow wjednicu abo wjednika wólbow.\n",
            "Za nastupnosći zagronity cłonk krajnego sejma dostanjo połnomóc.\n",
            "Rozpšawa wopśimjejo wobstojnosći a wugronijo se k pśedewześam krajnego kněžarstwa.\n",
            "(1) Za zawěsćenje zakładnego pšawa na šćit datow pó artiklu 11 wuzwólijo krajny sejm bźez debaty zagronitego kraja za šćit datow.\n",
            "Wukniki maju pšawo na to, až se po wopśimjeśu togo postajenja w pśedmjaśe serbšćina rozwucuju.\n",
            "Kšywowe zwězki pó § 4a organizěruju zgromadnje wólne, jadnake, pótajmne a direktne wuzwólowanje cłonkow rady.\n",
            "lěc jaden wótpósłańc jo zgubił swój mandat w krajnem sejmje.\n",
            "To jo licba dypkow.\n",
            "Zacynjona wobalka głosowańskego lisćika a pódpisane wólbne łopjeno se scynijotej do wólbneje listoweje wobalki.\n",
            "Wjednik wólbow pokazujo na kuždem pósejźenju wše cłonki na swóju winowatosć k njestrońskemu wugbanju swójogo amta.\n",
            "Njepłaśiwe su głose, gaž głosowański lisćik njejo se wót jadnańskego běrowa wudał.\n",
            "(4) Prezidentka abo prezident krajnego sejma informěrujo narownańsku wósobu a pokazujo ju na pśedpis § 40 wótstawk 1.\n",
            "Za to jo deja źaseś procentow bergarjow z pšawom na głosowanje wótpowědnu iniciatiwu pódpisaś.\n",
            "Dejśo dwójcy samske šćitne gronidło zapódaś.\n",
            "Joli až to pśi pśistajenju garantowane njejo, muse woni sebje wob cas tśich lět po nastupjenju swojeje słužby rěcne znajobnosći pśiswojś a je dopokazaś.\n",
            "To jo mě wužywarja.\n",
            "Wózjawjenje wopśimjejo za kužde wólbne naraźenje jadnotliwego w § 20 wótstawk 2 wólbne naraźenje jadnotliwego te pomjenjone daty.\n",
            "Pjas se zuby rěšy.\n",
            "Dalej tak.\n",
            "Nadrobnje wopśimjejo starodawny sedleński rum te gmejny a źěle gmejnow, kótarež su póstajone w dodanku ku tej kazni.\n",
            "Wóna ma zawěsćiś, až w nastupnosćach Serbow, wósebnje pśi kaznidawarstwje, serbske zastupniki sobu źěłaju.\n",
            "Pjerobalo njamóžoš pśi mócnem wětšu graś.\n",
            "Trěbnych jo nanejmjenjej sedym znamuškow.\n",
            "Za wólby, ako maju se pśewjasć wót krajnego sejma, mogu se pśez kazń abo pśez jadnański pórěd krajnego sejma wuwześa dopušćiś.\n",
            "Kužde zwěsćenje na wósobu wězanych datow ma se ned wopšawnjonemu znate cyniś, gaž zaměr zwěsćenja to dowólijo.\n",
            "W napominanju dej se pokazaś na rědowanje § 5 wótstawk 3 .\n",
            "Gaž wólba za cas tśich mjasecow pó konstituěrowanju krajnego sejma swój zaměr njedojśpijo, płaśi krajny sejm ako rozpušćony.\n",
            "(1) Wólby do rady deje se pó móžnosći pó casu blisko z wólbami do krajnego sejma pśewjasć.\n",
            "To jo mě zarědnika.\n",
            "Ty njejsy zwucowanje zakóńcył.\n",
            "Postajenja za wobcerki kubłanje, šula a źiśownje se pśewostajiju wosebnemu rědowanju.\n",
            "Samske pšawo maju teke zjadnośeństwa pótrjefjonych.\n",
            "Ministarski prezident póstajijo směrnice politiki kněžarstwa a jo za to krajnemu sejmoju zagronity.\n",
            "Wjednica wólbow jo k pśiwześu takego wobwěsćenja město pśisegi pśisłušna.\n",
            "Toś ta wustawa nabydnjo płaśiwosć na dnju pó jeje pśipowěźenju.\n",
            "To jo wuknikojski wobłuk.\n",
            "(2) Nuzkanje k frakcijam njejo dowólone.\n",
            "Zwucowanje dołoj suwnuś.\n",
            "Gaž mógu na fakty pokazaś, z kótarychž wujźo njepšawosć abo njedopołnosć zapisa wólarjow.\n",
            "Jadnański běrow ma te we wótstawku 1 pomjenjone pódłožki na pominanje wjednicy abo wjednikoju wólbow pśedpołožyś.\n",
            "Wjednica wólbow pokazujo na kuždem pósejźenju wše cłonki na swóju winowatosć k njestrońskemu wugbanju swójogo amta.\n",
            "Cerkwjam pó kazni, dogronje abo howacnych pšawach pśistojece winowatosći kraja mogu se jano pśez dojadnanje wótpóraś.\n",
            "Wjednica wólbow pokazujo kuždu pomocnicu a kuždego pomocnika na swój zawězk k zamjelcanju wšych zgónjonych nastupnosćow.\n",
            "Wobźěłaj wšykne zwucowanja 15 lekcijow!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:14.128164Z",
          "iopub.execute_input": "2022-07-24T18:06:14.128710Z",
          "iopub.status.idle": "2022-07-24T18:06:15.443914Z",
          "shell.execute_reply.started": "2022-07-24T18:06:14.128640Z",
          "shell.execute_reply": "2022-07-24T18:06:15.442820Z"
        },
        "trusted": true,
        "id": "bxoLU0pqdwcz",
        "outputId": "d5b6d04e-5476-45c3-9733-b6d3b5b301c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdrive\u001b[0m/    \u001b[01;34mgdrive\u001b[0m/       target.txt\n",
            "\u001b[01;34mdata-bin\u001b[0m/     \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:08:11.485595Z",
          "iopub.execute_input": "2022-07-24T18:08:11.486787Z",
          "iopub.status.idle": "2022-07-24T18:08:12.475192Z",
          "shell.execute_reply.started": "2022-07-24T18:08:11.486730Z",
          "shell.execute_reply": "2022-07-24T18:08:12.474153Z"
        },
        "trusted": true,
        "id": "NnEBR0Xhdwc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu target.txt -i /content/drive/dsb-de/valid2/valid2.dsb -l de-dsb -m bleu chrf ter)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:16.590743Z",
          "iopub.execute_input": "2022-07-24T18:06:16.591679Z",
          "iopub.status.idle": "2022-07-24T18:06:19.209461Z",
          "shell.execute_reply.started": "2022-07-24T18:06:16.591625Z",
          "shell.execute_reply": "2022-07-24T18:06:19.208405Z"
        },
        "trusted": true,
        "id": "bqOJKYxXdwc2",
        "outputId": "091e1bf6-0a29-4948-ae48-c3a10b819126",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 95.9,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\",\n",
            " \"verbose_score\": \"97.7/96.3/95.3/94.4 (BP = 1.000 ratio = 1.010 hyp_len = 8369 ref_len = 8286)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"chrF2\",\n",
            " \"score\": 98.2,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"yes\",\n",
            " \"nc\": \"6\",\n",
            " \"nw\": \"0\",\n",
            " \"space\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"TER\",\n",
            " \"score\": 3.2,\n",
            " \"signature\": \"nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"lc\",\n",
            " \"tok\": \"tercom\",\n",
            " \"norm\": \"no\",\n",
            " \"punct\": \"yes\",\n",
            " \"asian\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "}\n",
            "]\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading blind test data\n",
        "!gdown --id 1wGTpS2dj_oCA4ml62Il7K2hjT6WCIUGS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD0Zhyg5Mmtd",
        "outputId": "65786cc7-6613-43d1-ad5f-f33d2d103d43"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wGTpS2dj_oCA4ml62Il7K2hjT6WCIUGS\n",
            "To: /content/test_DE-DSB.de_src.txt\n",
            "100% 93.0k/93.0k [00:00<00:00, 66.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/test_DE-DSB.de_src.txt --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang de --target-lang dsb \\\n",
        "      --beam 5 data-bin/wmt_de_dsb | grep -P \"D-[0-9]+\" | cut -f3 > /content/gdrive/MyDrive/target_de-dsb_sup_submission.txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr0yH7NZUF1B",
        "outputId": "25410271-f95e-45d0-e5e6-b665e71e36f9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 22:36:30 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/test_DE-DSB.de_src.txt'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_de_dsb', 'source_lang': 'de', 'target_lang': 'dsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 22:36:31 | INFO | fairseq.tasks.translation | [de] dictionary: 62320 types\n",
            "2022-08-28 22:36:31 | INFO | fairseq.tasks.translation | [dsb] dictionary: 80848 types\n",
            "2022-08-28 22:36:31 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-28 22:36:36 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-28 22:36:36 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-28 22:36:36 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-28 22:36:45 | INFO | fairseq_cli.interactive | Total time: 14.990 seconds; translation time: 6.796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/test_DE-DSB.de_src.txt --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang de --target-lang dsb \\\n",
        "      --beam 5 data-bin/wmt_de_dsb | grep -P \"D-[0-9]+\" | cut -f3 > /content/target_de-dsb_sup_submission.txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kzADuVicnYCM",
        "outputId": "ce90ae45-6a43-408d-8783-ea3bbe4a59f1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 22:41:12 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/test_DE-DSB.de_src.txt'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_de_dsb', 'source_lang': 'de', 'target_lang': 'dsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 22:41:13 | INFO | fairseq.tasks.translation | [de] dictionary: 62320 types\n",
            "2022-08-28 22:41:13 | INFO | fairseq.tasks.translation | [dsb] dictionary: 80848 types\n",
            "2022-08-28 22:41:13 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-28 22:41:19 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-28 22:41:19 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-28 22:41:19 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-28 22:41:28 | INFO | fairseq_cli.interactive | Total time: 15.349 seconds; translation time: 6.901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "id": "9Zw2XTYgbzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu target.txt -i /content/drive/dsb-de/valid2/valid2.dsb -l de-dsb -m bleu chrf ter)"
      ],
      "metadata": {
        "id": "7LNPRlDQb4ER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}