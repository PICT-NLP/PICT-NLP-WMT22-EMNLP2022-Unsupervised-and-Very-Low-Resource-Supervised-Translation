{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "dsb-hsb_transformers_supervised.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# create a seperate folder to store everything\n",
        "# !mkdir wmt\n",
        "# %cd wmt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:45.876120Z",
          "iopub.execute_input": "2022-07-24T19:01:45.876582Z",
          "iopub.status.idle": "2022-07-24T19:01:46.194317Z",
          "shell.execute_reply.started": "2022-07-24T19:01:45.876499Z",
          "shell.execute_reply": "2022-07-24T19:01:46.192944Z"
        },
        "trusted": true,
        "id": "J9o_gWJIdwcc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tree\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:47.842831Z",
          "iopub.execute_input": "2022-07-24T19:01:47.843475Z",
          "iopub.status.idle": "2022-07-24T19:03:13.957220Z",
          "shell.execute_reply.started": "2022-07-24T19:01:47.843443Z",
          "shell.execute_reply": "2022-07-24T19:03:13.955881Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sULkb42Jdwci",
        "outputId": "cb14924d-3854-46dc-f640-fa0a7bc66388"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 1s (39.2 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 31.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 68.7 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 71.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 52.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=ef4cc14e1d9ea79a1dbe0504759677f60931452b70068ffb3d60d1a7c4605162\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, portalocker, morfessor, colorama, tensorboardX, sacremoses, sacrebleu, mock, indic-nlp-library\n",
            "Successfully installed colorama-0.4.5 indic-nlp-library-0.81 mock-4.0.3 morfessor-2.0.6 portalocker-2.5.1 sacrebleu-2.2.0 sacremoses-0.0.53 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0 tensorboardX-2.5.1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 32239, done.\u001b[K\n",
            "remote: Counting objects: 100% (110/110), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 32239 (delta 39), reused 55 (delta 19), pack-reused 32129\u001b[K\n",
            "Receiving objects: 100% (32239/32239), 22.64 MiB | 17.72 MiB/s, done.\n",
            "Resolving deltas: 100% (23585/23585), done.\n",
            "/content/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2.2.0)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 56.7 MB/s \n",
            "\u001b[?25hCollecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 64.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.1+cu113)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.32)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.9.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 74.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.5)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.8.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=3effe33d060ebab9af68d0bb83b228a317e6d69b1cfcfe0da79e6464d4474204\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 fairseq hydra-core-1.0.7 omegaconf-2.0.6\n",
            "/content\n",
            "/env/python\n",
            "/env/python:/content/fairseq/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:13.960095Z",
          "iopub.execute_input": "2022-07-24T19:03:13.960531Z",
          "iopub.status.idle": "2022-07-24T19:03:14.252882Z",
          "shell.execute_reply.started": "2022-07-24T19:03:13.960500Z",
          "shell.execute_reply": "2022-07-24T19:03:14.251592Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XleJnPzidwck",
        "outputId": "2d577a97-7fb8-4b34-cf4a-180a3ec73b51"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mfairseq\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U --no-cache-dir gdown --pre"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:14.254120Z",
          "iopub.execute_input": "2022-07-24T19:03:14.254498Z",
          "iopub.status.idle": "2022-07-24T19:03:36.192512Z",
          "shell.execute_reply.started": "2022-07-24T19:03:14.254462Z",
          "shell.execute_reply": "2022-07-24T19:03:36.191399Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIuDbtOgdwcm",
        "outputId": "7cf1b8e9-8ed2-41c9-e0dd-335bdf084063"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=3be6d48ddb3e36c1a22568bb65276305c3bd5bd3c3117aac67f6cdb1459134a3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-1j1t235z/wheels/3d/ec/b0/a96d1d126183f98570a785e6bf8789fca559853a9260e928e1\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNjo_Q8_IlXz",
        "outputId": "bd0779ac-d089-4620-8e0c-8eebe0c4856b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir drive\n",
        "%cd drive"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.194917Z",
          "iopub.execute_input": "2022-07-24T19:03:36.195189Z",
          "iopub.status.idle": "2022-07-24T19:03:36.494103Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.195162Z",
          "shell.execute_reply": "2022-07-24T19:03:36.492910Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AopzC4DBdwcn",
        "outputId": "c9d1b903-86b7-4d18-d85d-8dedd28d237a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1P3l5qdyrNA7KTGi0rEzJidd7ab9DnrE7 --folder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.495274Z",
          "iopub.execute_input": "2022-07-24T19:03:36.495519Z",
          "iopub.status.idle": "2022-07-24T19:03:47.662284Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.495496Z",
          "shell.execute_reply": "2022-07-24T19:03:47.661258Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa1K1LT9dwcn",
        "outputId": "1b124b47-e8b5-46ac-cdb3-b683d29509a8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Retrieving folder list\n",
            "Retrieving folder 1skvX_-rLx9Stl-XCBi-yfE3b6bwp0SB1 valid_dsb-hsb\n",
            "Processing file 1CCavbsyYo971QuR_TdxtfAEmHkC-80Zi dev_dsb-hsb.dsb\n",
            "Processing file 1-BafAQg_d_HN315YusYGzaJ9h_Grgk0P dev_dsb-hsb.hsb\n",
            "Processing file 1j6wZ9H9Tn0jChyDJjs127Th8r1cgGHJQ valid_dsb-hsb.dsb\n",
            "Processing file 1sAS6023wb9CUv4hv7qPktttBDTSLbvgr valid_dsb-hsb.hsb\n",
            "Processing file 1JHJJeoChv-kco5ICc9v2eTeiRk7YaUMu train_dsb-hsb.dsb\n",
            "Processing file 1z1t2yHGYZ1eF0LubUQyaFudeOBpBgUiD train_dsb-hsb.hsb\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CCavbsyYo971QuR_TdxtfAEmHkC-80Zi\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/dev_dsb-hsb.dsb\n",
            "100% 56.6k/56.6k [00:00<00:00, 93.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-BafAQg_d_HN315YusYGzaJ9h_Grgk0P\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/dev_dsb-hsb.hsb\n",
            "100% 55.2k/55.2k [00:00<00:00, 98.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j6wZ9H9Tn0jChyDJjs127Th8r1cgGHJQ\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb\n",
            "100% 31.6k/31.6k [00:00<00:00, 66.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sAS6023wb9CUv4hv7qPktttBDTSLbvgr\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb\n",
            "100% 30.9k/30.9k [00:00<00:00, 66.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JHJJeoChv-kco5ICc9v2eTeiRk7YaUMu\n",
            "To: /content/drive/dsb-hsb/train_dsb-hsb.dsb\n",
            "100% 4.97M/4.97M [00:00<00:00, 152MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z1t2yHGYZ1eF0LubUQyaFudeOBpBgUiD\n",
            "To: /content/drive/dsb-hsb/train_dsb-hsb.hsb\n",
            "100% 4.85M/4.85M [00:00<00:00, 209MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUdiQyJdJBdt",
        "outputId": "5a63cbdd-afa7-4c75-af9f-912592e70123"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:47.663751Z",
          "iopub.execute_input": "2022-07-24T19:03:47.664065Z",
          "iopub.status.idle": "2022-07-24T19:03:47.671365Z",
          "shell.execute_reply.started": "2022-07-24T19:03:47.664036Z",
          "shell.execute_reply": "2022-07-24T19:03:47.670044Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXt8nnHHdwco",
        "outputId": "ee9b5814-4a59-47c6-f872-c1efc845d7cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:04:11.972219Z",
          "iopub.execute_input": "2022-07-24T19:04:11.972913Z",
          "iopub.status.idle": "2022-07-24T19:04:12.256665Z",
          "shell.execute_reply.started": "2022-07-24T19:04:11.972888Z",
          "shell.execute_reply": "2022-07-24T19:04:12.255254Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ihwjB6Gdwcp",
        "outputId": "6069208c-990e-4052-bfc2-790a1763da03"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-preprocess \\\n",
        "    --source-lang dsb --target-lang hsb \\\n",
        "    --trainpref drive/dsb-hsb/train_dsb-hsb --validpref drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb \\\n",
        "    --destdir data-bin/wmt_dsb_hsb --thresholdtgt 0 --thresholdsrc 0 \\\n",
        "    --workers 20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:14:48.909980Z",
          "iopub.execute_input": "2022-07-24T19:14:48.910354Z",
          "iopub.status.idle": "2022-07-24T19:15:18.024931Z",
          "shell.execute_reply.started": "2022-07-24T19:14:48.910330Z",
          "shell.execute_reply": "2022-07-24T19:15:18.024044Z"
        },
        "trusted": true,
        "id": "XxPJ-2-zdwcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a95793a7-d79b-4566-c6b9-94f464acffcb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 06:30:00 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wmt_dsb_hsb', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='dsb', srcdict=None, suppress_crashes=False, target_lang='hsb', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='drive/dsb-hsb/train_dsb-hsb', use_plasma_view=False, user_dir=None, validpref='drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb', wandb_project=None, workers=20)\n",
            "2022-08-28 06:30:11 | INFO | fairseq_cli.preprocess | [dsb] Dictionary: 104024 types\n",
            "2022-08-28 06:30:20 | INFO | fairseq_cli.preprocess | [dsb] drive/dsb-hsb/train_dsb-hsb.dsb: 62565 sents, 736316 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-28 06:30:20 | INFO | fairseq_cli.preprocess | [dsb] Dictionary: 104024 types\n",
            "2022-08-28 06:30:21 | INFO | fairseq_cli.preprocess | [dsb] drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb: 709 sents, 5301 tokens, 18.7% replaced (by <unk>)\n",
            "2022-08-28 06:30:21 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 103512 types\n",
            "2022-08-28 06:30:30 | INFO | fairseq_cli.preprocess | [hsb] drive/dsb-hsb/train_dsb-hsb.hsb: 62565 sents, 716965 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-28 06:30:30 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 103512 types\n",
            "2022-08-28 06:30:33 | INFO | fairseq_cli.preprocess | [hsb] drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb: 709 sents, 5248 tokens, 18.1% replaced (by <unk>)\n",
            "2022-08-28 06:30:33 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt_dsb_hsb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uol2ncymJWFr",
        "outputId": "ae490474-8e66-4386-f879-9b156a9a7c06"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 31.3 MB/s \n",
            "\u001b[?25hCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 52.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 76.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 80.0 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=2339186462591eef4cb46c6fc9453af51282eacf4fff139daa8b03fb25be5c82\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"wmt2022_dsb-hsb_transformers_supervised\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:18.026892Z",
          "iopub.execute_input": "2022-07-24T19:15:18.027225Z",
          "iopub.status.idle": "2022-07-24T19:15:26.733355Z",
          "shell.execute_reply.started": "2022-07-24T19:15:18.027200Z",
          "shell.execute_reply": "2022-07-24T19:15:26.732531Z"
        },
        "trusted": true,
        "id": "hoUXe8Kndwcr",
        "outputId": "3b5e9be7-1871-46cb-fa54-9cec7b9c5c14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220828_063330-2r1fhica</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/rahul2023_usa/wmt2022_dsb-hsb_transformers_supervised/runs/2r1fhica\" target=\"_blank\">divine-pond-1</a></strong> to <a href=\"https://wandb.ai/rahul2023_usa/wmt2022_dsb-hsb_transformers_supervised\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rahul2023_usa/wmt2022_dsb-hsb_transformers_supervised/runs/2r1fhica?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f87c3ba2090>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Finetuning the model\n",
        "\n",
        "# # pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
        "\n",
        "\n",
        "# # some notable args:\n",
        "# # --max-update=1000     -> for this example, to demonstrate how to finetune we are only training for 1000 steps. You should increase this when finetuning\n",
        "# # --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# # --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# # --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# # --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\n",
        "# # --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\n",
        "# # --max_tokns           -> this is max tokens per batch\n",
        "\n",
        "\n",
        "# !( fairseq-train data-bin/wmt_dsb_de \\\n",
        "# --max-source-positions=210 \\\n",
        "# --max-target-positions=210 \\\n",
        "# --max-update=1000 \\\n",
        "# --save-interval=1 \\\n",
        "# --arch=transformer \\\n",
        "# --criterion=label_smoothed_cross_entropy \\\n",
        "# --source-lang=dsb \\\n",
        "# --lr-scheduler=inverse_sqrt \\\n",
        "# --target-lang=de \\\n",
        "# --label-smoothing=0.1 \\\n",
        "# --optimizer adam \\\n",
        "# --adam-betas \"(0.9, 0.98)\" \\\n",
        "# --clip-norm 1.0 \\\n",
        "# --warmup-init-lr 1e-07 \\\n",
        "# --warmup-updates 4000 \\\n",
        "# --dropout 0.2 \\\n",
        "# --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "# --save-dir checkpoints/model \\\n",
        "# --keep-last-epochs 5 \\\n",
        "# --patience 5 \\\n",
        "# --skip-invalid-size-inputs-valid-test \\\n",
        "# --fp16 \\\n",
        "# --update-freq=2 \\\n",
        "# --distributed-world-size 1 \\\n",
        "# --max-tokens 1024 \\\n",
        "# --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "# --lr 5e-4 \\\n",
        "# --reset-lr-scheduler \\\n",
        "# --reset-meters \\\n",
        "# --reset-dataloader \\\n",
        "# --reset-optimizer \\\n",
        "# --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w4cGb93Bdwct"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!( fairseq-train data-bin/wmt_dsb_hsb \\\n",
        "    --arch transformer \\\n",
        "    --max-epoch=50 \\\n",
        "    --criterion=label_smoothed_cross_entropy \\\n",
        "    --source-lang=dsb \\\n",
        "    --lr-scheduler=inverse_sqrt \\\n",
        "    --target-lang=hsb \\\n",
        "    --label-smoothing=0.1 \\\n",
        "    --optimizer adam \\\n",
        "    --adam-betas \"(0.9, 0.98)\" \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --dropout 0.2 \\\n",
        "    --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "    --wandb-project 'wmt2022_dsb-hsb_transformers_supervised' \\\n",
        "    --save-dir checkpoints/model \\\n",
        "    --keep-last-epochs 5 \\\n",
        "    --fp16 \\\n",
        "    --update-freq=2 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --lr 5e-4 \\\n",
        "    --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "    --reset-lr-scheduler \\\n",
        "    --reset-meters \\\n",
        "    --reset-dataloader \\\n",
        "    --reset-optimizer \\\n",
        "    --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:42.672724Z",
          "iopub.execute_input": "2022-07-24T19:15:42.673143Z",
          "iopub.status.idle": "2022-07-24T19:16:00.902554Z",
          "shell.execute_reply.started": "2022-07-24T19:15:42.673112Z",
          "shell.execute_reply": "2022-07-24T19:16:00.901385Z"
        },
        "trusted": true,
        "id": "U2IouMGudwcv",
        "outputId": "63901b56-43c0-4139-addb-8f1e2646d55a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 06:36:43 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../../../tmp/tensorboard-wandb', 'wandb_project': 'wmt2022_dsb-hsb_transformers_supervised', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/model', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wmt_dsb_hsb', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=True, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=50, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='checkpoints/model', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='dsb', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='hsb', task='translation', tensorboard_logdir='../../../tmp/tensorboard-wandb', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='wmt2022_dsb-hsb_transformers_supervised', warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/wmt_dsb_hsb', 'source_lang': 'dsb', 'target_lang': 'hsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 06:36:44 | INFO | fairseq.tasks.translation | [dsb] dictionary: 104024 types\n",
            "2022-08-28 06:36:44 | INFO | fairseq.tasks.translation | [hsb] dictionary: 103512 types\n",
            "2022-08-28 06:36:47 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(104024, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(103512, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=103512, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-08-28 06:36:47 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-08-28 06:36:47 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-08-28 06:36:47 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-08-28 06:36:47 | INFO | fairseq_cli.train | num. shared model params: 203,395,072 (num. trained: 203,395,072)\n",
            "2022-08-28 06:36:47 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-08-28 06:36:47 | INFO | fairseq.data.data_utils | loaded 709 examples from: data-bin/wmt_dsb_hsb/valid.dsb-hsb.dsb\n",
            "2022-08-28 06:36:47 | INFO | fairseq.data.data_utils | loaded 709 examples from: data-bin/wmt_dsb_hsb/valid.dsb-hsb.hsb\n",
            "2022-08-28 06:36:47 | INFO | fairseq.tasks.translation | data-bin/wmt_dsb_hsb valid dsb-hsb 709 examples\n",
            "2022-08-28 06:36:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-28 06:36:51 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-08-28 06:36:51 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-28 06:36:51 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-08-28 06:36:51 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-08-28 06:36:51 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/model/checkpoint_last.pt\n",
            "2022-08-28 06:36:51 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/model/checkpoint_last.pt\n",
            "2022-08-28 06:36:51 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-08-28 06:36:51 | INFO | fairseq.data.data_utils | loaded 62,565 examples from: data-bin/wmt_dsb_hsb/train.dsb-hsb.dsb\n",
            "2022-08-28 06:36:51 | INFO | fairseq.data.data_utils | loaded 62,565 examples from: data-bin/wmt_dsb_hsb/train.dsb-hsb.hsb\n",
            "2022-08-28 06:36:51 | INFO | fairseq.tasks.translation | data-bin/wmt_dsb_hsb train dsb-hsb 62565 examples\n",
            "2022-08-28 06:36:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 001:   0% 0/98 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul2023_usa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220828_063652-2u530njt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_dsb-hsb_transformers_supervised\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_dsb-hsb_transformers_supervised/runs/2u530njt\u001b[0m\n",
            "2022-08-28 06:36:52 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-08-28 06:36:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2022-08-28 06:36:56 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   3% 3/98 [00:05<02:18,  1.45s/it]2022-08-28 06:36:57 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   6% 6/98 [00:07<01:10,  1.31it/s]2022-08-28 06:36:59 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  99% 97/98 [00:53<00:00,  1.98it/s]2022-08-28 06:37:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 1/4 [00:00<00:02,  1.01it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 2/4 [00:01<00:01,  1.52it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 3/4 [00:01<00:00,  2.41it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:37:48 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-08-28 06:37:48 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.239 | nll_loss 13.863 | ppl 14900 | bleu 0 | wps 4307.5 | wpb 1312 | bsz 177.2 | num_updates 95\n",
            "2022-08-28 06:37:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 95 updates\n",
            "2022-08-28 06:37:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-28 06:38:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-28 06:38:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint1.pt (epoch 1 @ 95 updates, score 14.239) (writing took 31.220109202000003 seconds)\n",
            "2022-08-28 06:38:22 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-08-28 06:38:22 | INFO | train | epoch 001 | loss 15.892 | nll_loss 15.719 | ppl 53950.9 | wps 8003.3 | ups 1.09 | wpb 7317.9 | bsz 628.1 | num_updates 95 | lr 1.1875e-05 | gnorm 3.439 | loss_scale 16 | train_wall 52 | gb_free 6.1 | wall 91\n",
            "2022-08-28 06:38:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 002:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:38:22 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-08-28 06:38:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 97/98 [00:51<00:00,  1.86it/s, loss=15.836, nll_loss=15.656, ppl=51640.9, wps=8111.3, ups=1.11, wpb=7309.4, bsz=641.5, num_updates=100, lr=1.25e-05, gnorm=3.375, loss_scale=16, train_wall=54, gb_free=5.4, wall=94]2022-08-28 06:39:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 1/4 [00:01<00:03,  1.23s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.05it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.20it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:39:18 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.457 | nll_loss 12.987 | ppl 8121.19 | bleu 0.13 | wps 1091.2 | wpb 1312 | bsz 177.2 | num_updates 193 | best_loss 13.457\n",
            "2022-08-28 06:39:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 193 updates\n",
            "2022-08-28 06:39:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-28 06:39:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-28 06:39:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint2.pt (epoch 2 @ 193 updates, score 13.457) (writing took 38.59136878200002 seconds)\n",
            "2022-08-28 06:39:57 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-08-28 06:39:57 | INFO | train | epoch 002 | loss 14.558 | nll_loss 14.237 | ppl 19306.7 | wps 7556 | ups 1.03 | wpb 7316 | bsz 638.4 | num_updates 193 | lr 2.4125e-05 | gnorm 1.839 | loss_scale 16 | train_wall 52 | gb_free 5.9 | wall 186\n",
            "2022-08-28 06:39:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 003:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:39:58 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-08-28 06:39:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 97/98 [00:53<00:00,  1.87it/s, loss=14.509, nll_loss=14.183, ppl=18601.9, wps=7544.6, ups=1.03, wpb=7294.1, bsz=624.3, num_updates=200, lr=2.5e-05, gnorm=1.861, loss_scale=16, train_wall=53, gb_free=5.5, wall=191]2022-08-28 06:40:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.42s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.42s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.09s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.08it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:40:56 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.037 | nll_loss 12.526 | ppl 5898.26 | bleu 0.14 | wps 961 | wpb 1312 | bsz 177.2 | num_updates 291 | best_loss 13.037\n",
            "2022-08-28 06:40:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 291 updates\n",
            "2022-08-28 06:40:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-28 06:41:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-28 06:41:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint3.pt (epoch 3 @ 291 updates, score 13.037) (writing took 42.58417342099983 seconds)\n",
            "2022-08-28 06:41:39 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-08-28 06:41:39 | INFO | train | epoch 003 | loss 13.566 | nll_loss 13.134 | ppl 8989.24 | wps 7063.6 | ups 0.97 | wpb 7316 | bsz 638.4 | num_updates 291 | lr 3.6375e-05 | gnorm 2.574 | loss_scale 16 | train_wall 53 | gb_free 5.9 | wall 288\n",
            "2022-08-28 06:41:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 004:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:41:39 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-08-28 06:41:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:   5% 5/98 [00:03<00:53,  1.74it/s]2022-08-28 06:41:42 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 004:  99% 97/98 [00:53<00:00,  1.85it/s, loss=13.491, nll_loss=13.05, ppl=8480.81, wps=7205.1, ups=0.98, wpb=7384.2, bsz=645.7, num_updates=300, lr=3.75e-05, gnorm=2.656, loss_scale=8, train_wall=55, gb_free=5.5, wall=294]2022-08-28 06:42:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.64s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.36s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.05s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:42:36 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.674 | nll_loss 12.08 | ppl 4330.69 | bleu 0.21 | wps 1065 | wpb 1312 | bsz 177.2 | num_updates 388 | best_loss 12.674\n",
            "2022-08-28 06:42:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 388 updates\n",
            "2022-08-28 06:42:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-28 06:42:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-28 06:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint4.pt (epoch 4 @ 388 updates, score 12.674) (writing took 39.416877604999854 seconds)\n",
            "2022-08-28 06:43:16 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-08-28 06:43:16 | INFO | train | epoch 004 | loss 12.783 | nll_loss 12.241 | ppl 4841.72 | wps 7286.5 | ups 1 | wpb 7307.6 | bsz 629.8 | num_updates 388 | lr 4.85e-05 | gnorm 2.612 | loss_scale 8 | train_wall 53 | gb_free 6.2 | wall 385\n",
            "2022-08-28 06:43:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 005:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:43:16 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-08-28 06:43:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 97/98 [00:53<00:00,  1.91it/s, loss=12.702, nll_loss=12.147, ppl=4533.95, wps=7458.3, ups=1.02, wpb=7330, bsz=631.5, num_updates=400, lr=5e-05, gnorm=2.523, loss_scale=8, train_wall=54, gb_free=5.4, wall=392]2022-08-28 06:44:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.34s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.26s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.00it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.14it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:44:13 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.295 | nll_loss 11.6 | ppl 3105.12 | bleu 0.26 | wps 1044 | wpb 1312 | bsz 177.2 | num_updates 486 | best_loss 12.295\n",
            "2022-08-28 06:44:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 486 updates\n",
            "2022-08-28 06:44:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-28 06:44:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-28 06:44:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint5.pt (epoch 5 @ 486 updates, score 12.295) (writing took 42.78399986999989 seconds)\n",
            "2022-08-28 06:44:56 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-08-28 06:44:56 | INFO | train | epoch 005 | loss 12.234 | nll_loss 11.592 | ppl 3087.63 | wps 7135.8 | ups 0.98 | wpb 7316 | bsz 638.4 | num_updates 486 | lr 6.075e-05 | gnorm 2.425 | loss_scale 8 | train_wall 53 | gb_free 5.8 | wall 485\n",
            "2022-08-28 06:44:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 006:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:44:56 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-08-28 06:44:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 97/98 [00:52<00:00,  1.85it/s, loss=12.197, nll_loss=11.548, ppl=2993.51, wps=7155.4, ups=0.99, wpb=7249.7, bsz=618.2, num_updates=500, lr=6.25e-05, gnorm=2.408, loss_scale=8, train_wall=54, gb_free=5.4, wall=493]2022-08-28 06:45:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 1/4 [00:01<00:03,  1.21s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.17s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.04it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:45:53 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.997 | nll_loss 11.246 | ppl 2428.96 | bleu 0.4 | wps 1059.4 | wpb 1312 | bsz 177.2 | num_updates 584 | best_loss 11.997\n",
            "2022-08-28 06:45:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 584 updates\n",
            "2022-08-28 06:45:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-28 06:46:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-28 06:46:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint6.pt (epoch 6 @ 584 updates, score 11.997) (writing took 42.29373757999997 seconds)\n",
            "2022-08-28 06:46:35 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-08-28 06:46:35 | INFO | train | epoch 006 | loss 11.893 | nll_loss 11.189 | ppl 2334.87 | wps 7261.8 | ups 0.99 | wpb 7316 | bsz 638.4 | num_updates 584 | lr 7.3e-05 | gnorm 2.602 | loss_scale 8 | train_wall 52 | gb_free 6.1 | wall 584\n",
            "2022-08-28 06:46:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 007:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:46:35 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-08-28 06:46:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 97/98 [00:51<00:00,  1.90it/s, loss=11.825, nll_loss=11.111, ppl=2211.23, wps=7315.5, ups=1, wpb=7315.3, bsz=652.6, num_updates=600, lr=7.5e-05, gnorm=2.517, loss_scale=8, train_wall=53, gb_free=5.5, wall=593]2022-08-28 06:47:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 1/4 [00:01<00:03,  1.30s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.23s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.02it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:47:31 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.668 | nll_loss 10.861 | ppl 1859.58 | bleu 0.44 | wps 1060.3 | wpb 1312 | bsz 177.2 | num_updates 682 | best_loss 11.668\n",
            "2022-08-28 06:47:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 682 updates\n",
            "2022-08-28 06:47:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-28 06:47:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-28 06:48:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint7.pt (epoch 7 @ 682 updates, score 11.668) (writing took 42.878742729999885 seconds)\n",
            "2022-08-28 06:48:14 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-08-28 06:48:14 | INFO | train | epoch 007 | loss 11.484 | nll_loss 10.719 | ppl 1685.35 | wps 7231.1 | ups 0.99 | wpb 7316 | bsz 638.4 | num_updates 682 | lr 8.525e-05 | gnorm 2.28 | loss_scale 8 | train_wall 51 | gb_free 6.2 | wall 683\n",
            "2022-08-28 06:48:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 008:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:48:14 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-08-28 06:48:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 97/98 [00:52<00:00,  2.07it/s, loss=11.384, nll_loss=10.605, ppl=1556.95, wps=7327.6, ups=0.99, wpb=7366, bsz=646.5, num_updates=700, lr=8.75e-05, gnorm=2.348, loss_scale=8, train_wall=52, gb_free=6, wall=694]2022-08-28 06:49:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.37s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.27s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.10s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.07it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:49:11 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.385 | nll_loss 10.525 | ppl 1473.22 | bleu 0.92 | wps 972.5 | wpb 1312 | bsz 177.2 | num_updates 780 | best_loss 11.385\n",
            "2022-08-28 06:49:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 780 updates\n",
            "2022-08-28 06:49:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-28 06:49:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-28 06:49:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint8.pt (epoch 8 @ 780 updates, score 11.385) (writing took 39.543786311000076 seconds)\n",
            "2022-08-28 06:49:51 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-08-28 06:49:51 | INFO | train | epoch 008 | loss 11.047 | nll_loss 10.218 | ppl 1190.84 | wps 7442.4 | ups 1.02 | wpb 7316 | bsz 638.4 | num_updates 780 | lr 9.75e-05 | gnorm 2.503 | loss_scale 8 | train_wall 51 | gb_free 5.8 | wall 780\n",
            "2022-08-28 06:49:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 009:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:49:51 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-08-28 06:49:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 97/98 [00:52<00:00,  1.92it/s, loss=10.972, nll_loss=10.13, ppl=1120.9, wps=7465.7, ups=1.03, wpb=7271.5, bsz=634.7, num_updates=800, lr=0.0001, gnorm=2.581, loss_scale=8, train_wall=52, gb_free=5.4, wall=791]2022-08-28 06:50:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.74s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.56s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.17s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.03it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:50:48 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.715 | nll_loss 10.894 | ppl 1903.13 | bleu 0.66 | wps 957.1 | wpb 1312 | bsz 177.2 | num_updates 878 | best_loss 11.385\n",
            "2022-08-28 06:50:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 878 updates\n",
            "2022-08-28 06:50:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-28 06:50:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-28 06:51:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint9.pt (epoch 9 @ 878 updates, score 11.715) (writing took 31.220259064999937 seconds)\n",
            "2022-08-28 06:51:19 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-08-28 06:51:19 | INFO | train | epoch 009 | loss 10.519 | nll_loss 9.616 | ppl 784.6 | wps 8123.9 | ups 1.11 | wpb 7316 | bsz 638.4 | num_updates 878 | lr 0.00010975 | gnorm 2.235 | loss_scale 8 | train_wall 51 | gb_free 6 | wall 868\n",
            "2022-08-28 06:51:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 010:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:51:19 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-08-28 06:51:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 97/98 [00:52<00:00,  1.89it/s, loss=10.412, nll_loss=9.495, ppl=721.52, wps=8223.8, ups=1.12, wpb=7320.2, bsz=650.9, num_updates=900, lr=0.0001125, gnorm=2.273, loss_scale=8, train_wall=52, gb_free=5.4, wall=880]2022-08-28 06:52:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.56s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.36s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.05s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.11it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:52:15 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.902 | nll_loss 9.94 | ppl 981.99 | bleu 1.74 | wps 1042.9 | wpb 1312 | bsz 177.2 | num_updates 976 | best_loss 10.902\n",
            "2022-08-28 06:52:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 976 updates\n",
            "2022-08-28 06:52:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-28 06:52:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-28 06:52:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint10.pt (epoch 10 @ 976 updates, score 10.902) (writing took 42.572682862999955 seconds)\n",
            "2022-08-28 06:52:58 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-08-28 06:52:58 | INFO | train | epoch 010 | loss 9.993 | nll_loss 9.019 | ppl 518.76 | wps 7228.9 | ups 0.99 | wpb 7316 | bsz 638.4 | num_updates 976 | lr 0.000122 | gnorm 2.22 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 967\n",
            "2022-08-28 06:52:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 011:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:52:58 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2022-08-28 06:52:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 97/98 [00:52<00:00,  1.86it/s, loss=9.8, nll_loss=8.801, ppl=445.93, wps=7318.3, ups=1, wpb=7343.2, bsz=638.4, num_updates=1000, lr=0.000125, gnorm=2.098, loss_scale=8, train_wall=52, gb_free=5.5, wall=980]2022-08-28 06:53:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.34s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.24s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.01it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:53:54 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.552 | nll_loss 9.53 | ppl 739.08 | bleu 3.88 | wps 1052.7 | wpb 1312 | bsz 177.2 | num_updates 1074 | best_loss 10.552\n",
            "2022-08-28 06:53:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1074 updates\n",
            "2022-08-28 06:53:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-28 06:54:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-28 06:54:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint11.pt (epoch 11 @ 1074 updates, score 10.552) (writing took 48.81033721299991 seconds)\n",
            "2022-08-28 06:54:43 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-08-28 06:54:44 | INFO | train | epoch 011 | loss 9.437 | nll_loss 8.388 | ppl 335.01 | wps 6792.7 | ups 0.93 | wpb 7316 | bsz 638.4 | num_updates 1074 | lr 0.00013425 | gnorm 2.624 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 1073\n",
            "2022-08-28 06:54:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 012:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:54:44 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2022-08-28 06:54:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 97/98 [00:55<00:00,  1.91it/s, loss=9.31, nll_loss=8.244, ppl=303.26, wps=6690.6, ups=0.91, wpb=7383.8, bsz=633.8, num_updates=1100, lr=0.0001375, gnorm=2.68, loss_scale=8, train_wall=53, gb_free=5.5, wall=1091]2022-08-28 06:55:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.36s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.26s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.00s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:55:43 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.32 | nll_loss 9.271 | ppl 617.94 | bleu 4.62 | wps 1054.8 | wpb 1312 | bsz 177.2 | num_updates 1172 | best_loss 10.32\n",
            "2022-08-28 06:55:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1172 updates\n",
            "2022-08-28 06:55:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-28 06:55:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-28 06:57:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint12.pt (epoch 12 @ 1172 updates, score 10.32) (writing took 104.15394242000002 seconds)\n",
            "2022-08-28 06:57:28 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-08-28 06:57:30 | INFO | train | epoch 012 | loss 8.928 | nll_loss 7.809 | ppl 224.28 | wps 4320.2 | ups 0.59 | wpb 7316 | bsz 638.4 | num_updates 1172 | lr 0.0001465 | gnorm 2.791 | loss_scale 8 | train_wall 51 | gb_free 6.1 | wall 1239\n",
            "2022-08-28 06:57:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 013:   0% 0/98 [00:00<?, ?it/s]2022-08-28 06:57:30 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2022-08-28 06:57:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 97/98 [00:50<00:00,  1.90it/s, loss=8.799, nll_loss=7.662, ppl=202.5, wps=4414.2, ups=0.62, wpb=7174.3, bsz=635.5, num_updates=1200, lr=0.00015, gnorm=2.771, loss_scale=8, train_wall=51, gb_free=5.8, wall=1253]2022-08-28 06:58:21 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.42s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.34s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.06s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 06:58:25 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.987 | nll_loss 8.908 | ppl 480.43 | bleu 6.81 | wps 996.3 | wpb 1312 | bsz 177.2 | num_updates 1270 | best_loss 9.987\n",
            "2022-08-28 06:58:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1270 updates\n",
            "2022-08-28 06:58:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-28 06:58:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-28 07:00:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint13.pt (epoch 13 @ 1270 updates, score 9.987) (writing took 105.18764208599987 seconds)\n",
            "2022-08-28 07:00:10 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2022-08-28 07:00:10 | INFO | train | epoch 013 | loss 8.323 | nll_loss 7.122 | ppl 139.3 | wps 4461.2 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 1270 | lr 0.00015875 | gnorm 2.448 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 1399\n",
            "2022-08-28 07:00:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 014:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:00:10 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2022-08-28 07:00:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 97/98 [00:51<00:00,  1.88it/s, loss=8.162, nll_loss=6.938, ppl=122.6, wps=4513.4, ups=0.61, wpb=7341.4, bsz=615.6, num_updates=1300, lr=0.0001625, gnorm=2.484, loss_scale=8, train_wall=51, gb_free=5.8, wall=1416]2022-08-28 07:01:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  25% 1/4 [00:01<00:03,  1.30s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.17s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.05it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:01:06 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 9.813 | nll_loss 8.687 | ppl 412.22 | bleu 9.15 | wps 1093.5 | wpb 1312 | bsz 177.2 | num_updates 1368 | best_loss 9.813\n",
            "2022-08-28 07:01:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1368 updates\n",
            "2022-08-28 07:01:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-28 07:01:18 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-28 07:02:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint14.pt (epoch 14 @ 1368 updates, score 9.813) (writing took 104.85522778899985 seconds)\n",
            "2022-08-28 07:02:51 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2022-08-28 07:02:51 | INFO | train | epoch 014 | loss 7.781 | nll_loss 6.504 | ppl 90.76 | wps 4455.5 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 1368 | lr 0.000171 | gnorm 2.603 | loss_scale 8 | train_wall 50 | gb_free 5.8 | wall 1560\n",
            "2022-08-28 07:02:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 015:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:02:51 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2022-08-28 07:02:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 97/98 [00:50<00:00,  1.94it/s, loss=7.621, nll_loss=6.321, ppl=79.93, wps=4527.7, ups=0.62, wpb=7290.1, bsz=644.8, num_updates=1400, lr=0.000175, gnorm=2.582, loss_scale=8, train_wall=51, gb_free=5.7, wall=1577]2022-08-28 07:03:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.54s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.32s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.05s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:03:47 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.478 | nll_loss 8.302 | ppl 315.51 | bleu 9.55 | wps 1042.9 | wpb 1312 | bsz 177.2 | num_updates 1466 | best_loss 9.478\n",
            "2022-08-28 07:03:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1466 updates\n",
            "2022-08-28 07:03:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-28 07:03:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-28 07:05:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint15.pt (epoch 15 @ 1466 updates, score 9.478) (writing took 107.69142512100007 seconds)\n",
            "2022-08-28 07:05:34 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2022-08-28 07:05:35 | INFO | train | epoch 015 | loss 7.219 | nll_loss 5.862 | ppl 58.18 | wps 4390.7 | ups 0.6 | wpb 7316 | bsz 638.4 | num_updates 1466 | lr 0.00018325 | gnorm 2.374 | loss_scale 8 | train_wall 50 | gb_free 5.9 | wall 1724\n",
            "2022-08-28 07:05:35 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 016:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:05:35 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2022-08-28 07:05:35 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 97/98 [00:50<00:00,  1.90it/s, loss=7.04, nll_loss=5.658, ppl=50.51, wps=4490.5, ups=0.61, wpb=7391, bsz=625.3, num_updates=1500, lr=0.0001875, gnorm=2.44, loss_scale=8, train_wall=52, gb_free=5.4, wall=1742]2022-08-28 07:06:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.52s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.22s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.03it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.17it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:06:30 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.289 | nll_loss 8.086 | ppl 271.82 | bleu 13 | wps 1137.9 | wpb 1312 | bsz 177.2 | num_updates 1564 | best_loss 9.289\n",
            "2022-08-28 07:06:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1564 updates\n",
            "2022-08-28 07:06:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-28 07:06:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-28 07:08:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint16.pt (epoch 16 @ 1564 updates, score 9.289) (writing took 106.33732584400013 seconds)\n",
            "2022-08-28 07:08:16 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2022-08-28 07:08:17 | INFO | train | epoch 016 | loss 6.767 | nll_loss 5.344 | ppl 40.63 | wps 4409.5 | ups 0.6 | wpb 7316 | bsz 638.4 | num_updates 1564 | lr 0.0001955 | gnorm 2.501 | loss_scale 8 | train_wall 50 | gb_free 6.2 | wall 1886\n",
            "2022-08-28 07:08:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 017:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:08:17 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2022-08-28 07:08:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 97/98 [00:50<00:00,  1.90it/s, loss=6.603, nll_loss=5.156, ppl=35.64, wps=4419.3, ups=0.61, wpb=7217.8, bsz=659.9, num_updates=1600, lr=0.0002, gnorm=2.428, loss_scale=8, train_wall=51, gb_free=5.6, wall=1905]2022-08-28 07:09:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.44s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.22s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.01it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.15it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:09:12 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.049 | nll_loss 7.788 | ppl 221.07 | bleu 14.11 | wps 1092.5 | wpb 1312 | bsz 177.2 | num_updates 1662 | best_loss 9.049\n",
            "2022-08-28 07:09:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1662 updates\n",
            "2022-08-28 07:09:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-28 07:09:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-28 07:10:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint17.pt (epoch 17 @ 1662 updates, score 9.049) (writing took 102.62730745699992 seconds)\n",
            "2022-08-28 07:10:55 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2022-08-28 07:10:55 | INFO | train | epoch 017 | loss 6.296 | nll_loss 4.804 | ppl 27.94 | wps 4536.7 | ups 0.62 | wpb 7316 | bsz 638.4 | num_updates 1662 | lr 0.00020775 | gnorm 2.493 | loss_scale 8 | train_wall 50 | gb_free 5.9 | wall 2044\n",
            "2022-08-28 07:10:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 018:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:10:55 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2022-08-28 07:10:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 97/98 [00:54<00:00,  1.95it/s, loss=6.136, nll_loss=4.619, ppl=24.57, wps=4477.3, ups=0.61, wpb=7287.8, bsz=631.8, num_updates=1700, lr=0.0002125, gnorm=2.411, loss_scale=8, train_wall=51, gb_free=5.6, wall=2068]2022-08-28 07:11:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.58s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 2/4 [00:03<00:02,  1.49s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  75% 3/4 [00:04<00:01,  1.31s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.06s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:11:55 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 8.997 | nll_loss 7.767 | ppl 217.79 | bleu 14.16 | wps 843 | wpb 1312 | bsz 177.2 | num_updates 1760 | best_loss 8.997\n",
            "2022-08-28 07:11:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1760 updates\n",
            "2022-08-28 07:11:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-28 07:12:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-28 07:13:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint18.pt (epoch 18 @ 1760 updates, score 8.997) (writing took 103.88327438199985 seconds)\n",
            "2022-08-28 07:13:39 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2022-08-28 07:13:39 | INFO | train | epoch 018 | loss 5.848 | nll_loss 4.287 | ppl 19.53 | wps 4386.7 | ups 0.6 | wpb 7316 | bsz 638.4 | num_updates 1760 | lr 0.00022 | gnorm 2.282 | loss_scale 8 | train_wall 50 | gb_free 5.8 | wall 2208\n",
            "2022-08-28 07:13:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 019:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:13:39 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2022-08-28 07:13:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 97/98 [00:50<00:00,  1.94it/s, loss=5.635, nll_loss=4.042, ppl=16.48, wps=4643.9, ups=0.62, wpb=7490.3, bsz=637.5, num_updates=1800, lr=0.000225, gnorm=2.205, loss_scale=8, train_wall=52, gb_free=5.4, wall=2229]2022-08-28 07:14:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.58s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.26s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.02s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:14:34 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 8.739 | nll_loss 7.415 | ppl 170.66 | bleu 15.9 | wps 1092.7 | wpb 1312 | bsz 177.2 | num_updates 1858 | best_loss 8.739\n",
            "2022-08-28 07:14:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1858 updates\n",
            "2022-08-28 07:14:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-28 07:14:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-28 07:16:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint19.pt (epoch 19 @ 1858 updates, score 8.739) (writing took 105.47281628500014 seconds)\n",
            "2022-08-28 07:16:19 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2022-08-28 07:16:20 | INFO | train | epoch 019 | loss 5.438 | nll_loss 3.814 | ppl 14.06 | wps 4456.2 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 1858 | lr 0.00023225 | gnorm 2.075 | loss_scale 8 | train_wall 50 | gb_free 6.2 | wall 2369\n",
            "2022-08-28 07:16:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 020:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:16:20 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2022-08-28 07:16:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 97/98 [00:51<00:00,  2.01it/s, loss=5.309, nll_loss=3.664, ppl=12.68, wps=4454.8, ups=0.62, wpb=7233.5, bsz=636.2, num_updates=1900, lr=0.0002375, gnorm=2.16, loss_scale=8, train_wall=51, gb_free=5.6, wall=2391]2022-08-28 07:17:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.58s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.28s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.01s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.13it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:17:15 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 8.619 | nll_loss 7.25 | ppl 152.25 | bleu 18.3 | wps 1097.3 | wpb 1312 | bsz 177.2 | num_updates 1956 | best_loss 8.619\n",
            "2022-08-28 07:17:15 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1956 updates\n",
            "2022-08-28 07:17:15 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-28 07:17:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-28 07:19:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint20.pt (epoch 20 @ 1956 updates, score 8.619) (writing took 105.64468011000008 seconds)\n",
            "2022-08-28 07:19:01 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2022-08-28 07:19:05 | INFO | train | epoch 020 | loss 5.045 | nll_loss 3.359 | ppl 10.26 | wps 4335.8 | ups 0.59 | wpb 7316 | bsz 638.4 | num_updates 1956 | lr 0.0002445 | gnorm 1.944 | loss_scale 8 | train_wall 50 | gb_free 5.9 | wall 2534\n",
            "2022-08-28 07:19:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 021:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:19:05 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2022-08-28 07:19:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 97/98 [00:50<00:00,  1.90it/s, loss=4.945, nll_loss=3.243, ppl=9.47, wps=4419.3, ups=0.6, wpb=7313.1, bsz=655.5, num_updates=2000, lr=0.00025, gnorm=1.967, loss_scale=8, train_wall=51, gb_free=5.7, wall=2557]2022-08-28 07:19:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.58s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.33s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.03s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:20:00 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.519 | nll_loss 7.143 | ppl 141.35 | bleu 20.39 | wps 1070.4 | wpb 1312 | bsz 177.2 | num_updates 2054 | best_loss 8.519\n",
            "2022-08-28 07:20:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2054 updates\n",
            "2022-08-28 07:20:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-28 07:20:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-28 07:21:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint21.pt (epoch 21 @ 2054 updates, score 8.519) (writing took 102.62465824499986 seconds)\n",
            "2022-08-28 07:21:47 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2022-08-28 07:21:47 | INFO | train | epoch 021 | loss 4.752 | nll_loss 3.016 | ppl 8.09 | wps 4427.1 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 2054 | lr 0.00025675 | gnorm 1.898 | loss_scale 8 | train_wall 50 | gb_free 5.8 | wall 2696\n",
            "2022-08-28 07:21:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 022:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:21:47 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2022-08-28 07:21:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 97/98 [00:50<00:00,  1.87it/s, loss=4.547, nll_loss=2.779, ppl=6.86, wps=4461.1, ups=0.61, wpb=7272.2, bsz=622.8, num_updates=2100, lr=0.0002625, gnorm=1.641, loss_scale=8, train_wall=51, gb_free=5.5, wall=2720]2022-08-28 07:22:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.48s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.22s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.08it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.20it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:22:42 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.407 | nll_loss 7.028 | ppl 130.52 | bleu 23.51 | wps 1169.6 | wpb 1312 | bsz 177.2 | num_updates 2152 | best_loss 8.407\n",
            "2022-08-28 07:22:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2152 updates\n",
            "2022-08-28 07:22:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-28 07:22:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-28 07:24:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint22.pt (epoch 22 @ 2152 updates, score 8.407) (writing took 105.48557470200012 seconds)\n",
            "2022-08-28 07:24:28 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2022-08-28 07:24:28 | INFO | train | epoch 022 | loss 4.362 | nll_loss 2.563 | ppl 5.91 | wps 4445.4 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 2152 | lr 0.000269 | gnorm 1.561 | loss_scale 8 | train_wall 50 | gb_free 6.6 | wall 2857\n",
            "2022-08-28 07:24:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 023:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:24:28 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2022-08-28 07:24:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 97/98 [00:50<00:00,  1.93it/s, loss=4.209, nll_loss=2.385, ppl=5.22, wps=4527.2, ups=0.61, wpb=7372.3, bsz=646.8, num_updates=2200, lr=0.000275, gnorm=1.531, loss_scale=8, train_wall=51, gb_free=5.5, wall=2883]2022-08-28 07:25:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.53s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.43s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.08s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:25:24 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.27 | nll_loss 6.841 | ppl 114.61 | bleu 24.59 | wps 1000.5 | wpb 1312 | bsz 177.2 | num_updates 2250 | best_loss 8.27\n",
            "2022-08-28 07:25:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2250 updates\n",
            "2022-08-28 07:25:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-28 07:25:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-28 07:27:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint23.pt (epoch 23 @ 2250 updates, score 8.27) (writing took 105.29482367200035 seconds)\n",
            "2022-08-28 07:27:09 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2022-08-28 07:27:09 | INFO | train | epoch 023 | loss 4.1 | nll_loss 2.257 | ppl 4.78 | wps 4451.5 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 2250 | lr 0.00028125 | gnorm 1.583 | loss_scale 8 | train_wall 50 | gb_free 6.2 | wall 3018\n",
            "2022-08-28 07:27:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 024:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:27:09 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2022-08-28 07:27:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 97/98 [00:51<00:00,  1.93it/s, loss=3.947, nll_loss=2.076, ppl=4.22, wps=4514.8, ups=0.61, wpb=7356.1, bsz=638.2, num_updates=2300, lr=0.0002875, gnorm=1.382, loss_scale=8, train_wall=51, gb_free=5.8, wall=3046]2022-08-28 07:28:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.50s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.23s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.07it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.21it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:28:05 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.226 | nll_loss 6.775 | ppl 109.51 | bleu 25.89 | wps 1176.8 | wpb 1312 | bsz 177.2 | num_updates 2348 | best_loss 8.226\n",
            "2022-08-28 07:28:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2348 updates\n",
            "2022-08-28 07:28:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-28 07:28:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-28 07:29:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint24.pt (epoch 24 @ 2348 updates, score 8.226) (writing took 105.87825796600009 seconds)\n",
            "2022-08-28 07:29:51 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2022-08-28 07:29:51 | INFO | train | epoch 024 | loss 3.804 | nll_loss 1.909 | ppl 3.76 | wps 4425.5 | ups 0.6 | wpb 7316 | bsz 638.4 | num_updates 2348 | lr 0.0002935 | gnorm 1.294 | loss_scale 8 | train_wall 50 | gb_free 5.8 | wall 3180\n",
            "2022-08-28 07:29:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 025:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:29:51 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2022-08-28 07:29:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 97/98 [00:51<00:00,  1.92it/s, loss=3.658, nll_loss=1.737, ppl=3.33, wps=4489.6, ups=0.61, wpb=7314.9, bsz=643.3, num_updates=2400, lr=0.0003, gnorm=1.253, loss_scale=8, train_wall=51, gb_free=6.3, wall=3209]2022-08-28 07:30:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.48s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.17s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.06it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.20it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:30:47 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.32 | nll_loss 6.88 | ppl 117.74 | bleu 26.86 | wps 1178.4 | wpb 1312 | bsz 177.2 | num_updates 2446 | best_loss 8.226\n",
            "2022-08-28 07:30:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2446 updates\n",
            "2022-08-28 07:30:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-28 07:30:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-28 07:31:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint25.pt (epoch 25 @ 2446 updates, score 8.32) (writing took 53.22966367400022 seconds)\n",
            "2022-08-28 07:31:40 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2022-08-28 07:31:40 | INFO | train | epoch 025 | loss 3.555 | nll_loss 1.615 | ppl 3.06 | wps 6568 | ups 0.9 | wpb 7316 | bsz 638.4 | num_updates 2446 | lr 0.00030575 | gnorm 1.181 | loss_scale 8 | train_wall 50 | gb_free 5.8 | wall 3289\n",
            "2022-08-28 07:31:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 026:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:31:41 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2022-08-28 07:31:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 97/98 [00:51<00:00,  1.94it/s, loss=3.44, nll_loss=1.478, ppl=2.79, wps=6687, ups=0.91, wpb=7313.7, bsz=611, num_updates=2500, lr=0.0003125, gnorm=1.114, loss_scale=8, train_wall=51, gb_free=5.9, wall=3318]2022-08-28 07:32:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.43s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.26s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.05it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.19it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:32:36 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.415 | nll_loss 7.005 | ppl 128.41 | bleu 27.02 | wps 1120 | wpb 1312 | bsz 177.2 | num_updates 2544 | best_loss 8.226\n",
            "2022-08-28 07:32:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2544 updates\n",
            "2022-08-28 07:32:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-28 07:32:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-28 07:33:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint26.pt (epoch 26 @ 2544 updates, score 8.415) (writing took 50.735946351000166 seconds)\n",
            "2022-08-28 07:33:26 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2022-08-28 07:33:27 | INFO | train | epoch 026 | loss 3.326 | nll_loss 1.345 | ppl 2.54 | wps 6757.2 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 2544 | lr 0.000318 | gnorm 1.056 | loss_scale 8 | train_wall 51 | gb_free 5.8 | wall 3396\n",
            "2022-08-28 07:33:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 027:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:33:28 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2022-08-28 07:33:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 97/98 [00:52<00:00,  1.89it/s, loss=3.226, nll_loss=1.224, ppl=2.34, wps=6719.6, ups=0.92, wpb=7328.6, bsz=643.6, num_updates=2600, lr=0.000325, gnorm=1.016, loss_scale=8, train_wall=52, gb_free=5.8, wall=3427]2022-08-28 07:34:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.36s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.23s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.13it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.41it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:34:23 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.407 | nll_loss 6.977 | ppl 126.01 | bleu 27.71 | wps 1311.8 | wpb 1312 | bsz 177.2 | num_updates 2642 | best_loss 8.226\n",
            "2022-08-28 07:34:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2642 updates\n",
            "2022-08-28 07:34:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-28 07:34:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-28 07:35:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint27.pt (epoch 27 @ 2642 updates, score 8.407) (writing took 52.93433011800062 seconds)\n",
            "2022-08-28 07:35:16 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2022-08-28 07:35:16 | INFO | train | epoch 027 | loss 3.134 | nll_loss 1.116 | ppl 2.17 | wps 6534 | ups 0.89 | wpb 7316 | bsz 638.4 | num_updates 2642 | lr 0.00033025 | gnorm 0.986 | loss_scale 8 | train_wall 51 | gb_free 6.3 | wall 3505\n",
            "2022-08-28 07:35:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 028:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:35:16 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2022-08-28 07:35:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 97/98 [00:51<00:00,  1.97it/s, loss=3.038, nll_loss=1.001, ppl=2, wps=6714.9, ups=0.92, wpb=7330.4, bsz=639.9, num_updates=2700, lr=0.0003375, gnorm=0.951, loss_scale=8, train_wall=52, gb_free=5.5, wall=3536]2022-08-28 07:36:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.59s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.41s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.00it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.20it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:36:12 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.25 | nll_loss 6.807 | ppl 111.99 | bleu 29.12 | wps 1138.2 | wpb 1312 | bsz 177.2 | num_updates 2740 | best_loss 8.226\n",
            "2022-08-28 07:36:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2740 updates\n",
            "2022-08-28 07:36:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-28 07:36:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-28 07:37:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint28.pt (epoch 28 @ 2740 updates, score 8.25) (writing took 50.68355040000006 seconds)\n",
            "2022-08-28 07:37:03 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2022-08-28 07:37:03 | INFO | train | epoch 028 | loss 2.968 | nll_loss 0.915 | ppl 1.89 | wps 6742.3 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 2740 | lr 0.0003425 | gnorm 0.901 | loss_scale 8 | train_wall 51 | gb_free 5.8 | wall 3612\n",
            "2022-08-28 07:37:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 029:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:37:03 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2022-08-28 07:37:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 97/98 [00:52<00:00,  1.90it/s, loss=2.87, nll_loss=0.799, ppl=1.74, wps=6730.2, ups=0.92, wpb=7278.9, bsz=654.3, num_updates=2800, lr=0.00035, gnorm=0.827, loss_scale=8, train_wall=52, gb_free=5.9, wall=3645]2022-08-28 07:37:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.52s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.30s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.02s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:37:59 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.338 | nll_loss 6.889 | ppl 118.52 | bleu 27.99 | wps 1067.5 | wpb 1312 | bsz 177.2 | num_updates 2838 | best_loss 8.226\n",
            "2022-08-28 07:37:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2838 updates\n",
            "2022-08-28 07:37:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-28 07:38:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-28 07:38:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint29.pt (epoch 29 @ 2838 updates, score 8.338) (writing took 50.56080370900054 seconds)\n",
            "2022-08-28 07:38:50 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2022-08-28 07:38:50 | INFO | train | epoch 029 | loss 2.813 | nll_loss 0.73 | ppl 1.66 | wps 6677.2 | ups 0.91 | wpb 7316 | bsz 638.4 | num_updates 2838 | lr 0.00035475 | gnorm 0.78 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 3719\n",
            "2022-08-28 07:38:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 030:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:38:52 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2022-08-28 07:38:52 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 97/98 [00:53<00:00,  1.92it/s, loss=2.755, nll_loss=0.657, ppl=1.58, wps=6684.7, ups=0.91, wpb=7320.4, bsz=662.6, num_updates=2900, lr=0.0003625, gnorm=0.733, loss_scale=8, train_wall=52, gb_free=5.5, wall=3754]2022-08-28 07:39:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.40s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.14s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.14it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:39:47 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.207 | nll_loss 6.727 | ppl 105.94 | bleu 30.44 | wps 1241 | wpb 1312 | bsz 177.2 | num_updates 2936 | best_loss 8.207\n",
            "2022-08-28 07:39:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2936 updates\n",
            "2022-08-28 07:39:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-28 07:39:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-28 07:41:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint30.pt (epoch 30 @ 2936 updates, score 8.207) (writing took 105.3433205500005 seconds)\n",
            "2022-08-28 07:41:33 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2022-08-28 07:41:33 | INFO | train | epoch 030 | loss 2.728 | nll_loss 0.623 | ppl 1.54 | wps 4408.9 | ups 0.6 | wpb 7316 | bsz 638.4 | num_updates 2936 | lr 0.000367 | gnorm 0.746 | loss_scale 8 | train_wall 51 | gb_free 5.8 | wall 3882\n",
            "2022-08-28 07:41:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 031:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:41:33 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2022-08-28 07:41:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 97/98 [00:50<00:00,  1.92it/s, loss=2.688, nll_loss=0.571, ppl=1.49, wps=4533.1, ups=0.62, wpb=7316.5, bsz=619.4, num_updates=3000, lr=0.000375, gnorm=0.725, loss_scale=8, train_wall=51, gb_free=5.4, wall=3915]2022-08-28 07:42:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.48s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.17s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.10it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.22it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:42:28 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.24 | nll_loss 6.765 | ppl 108.73 | bleu 28.4 | wps 1206.7 | wpb 1312 | bsz 177.2 | num_updates 3034 | best_loss 8.207\n",
            "2022-08-28 07:42:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3034 updates\n",
            "2022-08-28 07:42:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-28 07:42:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-28 07:43:21 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint31.pt (epoch 31 @ 3034 updates, score 8.24) (writing took 53.61805018599989 seconds)\n",
            "2022-08-28 07:43:21 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2022-08-28 07:43:21 | INFO | train | epoch 031 | loss 2.647 | nll_loss 0.522 | ppl 1.44 | wps 6604.8 | ups 0.9 | wpb 7316 | bsz 638.4 | num_updates 3034 | lr 0.00037925 | gnorm 0.656 | loss_scale 8 | train_wall 50 | gb_free 5.9 | wall 3990\n",
            "2022-08-28 07:43:21 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 032:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:43:21 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2022-08-28 07:43:21 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 97/98 [00:51<00:00,  1.91it/s, loss=2.601, nll_loss=0.47, ppl=1.38, wps=6621.1, ups=0.91, wpb=7307.8, bsz=622.9, num_updates=3100, lr=0.0003875, gnorm=0.573, loss_scale=8, train_wall=51, gb_free=5.8, wall=4026]2022-08-28 07:44:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.48s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.19s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.13it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:44:17 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.281 | nll_loss 6.804 | ppl 111.76 | bleu 30.14 | wps 1348.8 | wpb 1312 | bsz 177.2 | num_updates 3132 | best_loss 8.207\n",
            "2022-08-28 07:44:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3132 updates\n",
            "2022-08-28 07:44:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-28 07:44:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-28 07:45:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint32.pt (epoch 32 @ 3132 updates, score 8.281) (writing took 51.156513963999714 seconds)\n",
            "2022-08-28 07:45:08 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2022-08-28 07:45:08 | INFO | train | epoch 032 | loss 2.591 | nll_loss 0.458 | ppl 1.37 | wps 6706.1 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 3132 | lr 0.0003915 | gnorm 0.576 | loss_scale 8 | train_wall 51 | gb_free 5.8 | wall 4097\n",
            "2022-08-28 07:45:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 033:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:45:09 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2022-08-28 07:45:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 97/98 [00:52<00:00,  1.91it/s, loss=2.575, nll_loss=0.441, ppl=1.36, wps=6699, ups=0.92, wpb=7293.1, bsz=643.6, num_updates=3200, lr=0.0004, gnorm=0.579, loss_scale=8, train_wall=51, gb_free=5.9, wall=4135]2022-08-28 07:46:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.44s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.17s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.08it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:46:05 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.147 | nll_loss 6.678 | ppl 102.42 | bleu 28.37 | wps 1201.6 | wpb 1312 | bsz 177.2 | num_updates 3230 | best_loss 8.147\n",
            "2022-08-28 07:46:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3230 updates\n",
            "2022-08-28 07:46:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-28 07:46:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-28 07:47:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint33.pt (epoch 33 @ 3230 updates, score 8.147) (writing took 105.67468359000031 seconds)\n",
            "2022-08-28 07:47:51 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2022-08-28 07:47:51 | INFO | train | epoch 033 | loss 2.56 | nll_loss 0.424 | ppl 1.34 | wps 4409.6 | ups 0.6 | wpb 7316 | bsz 638.4 | num_updates 3230 | lr 0.00040375 | gnorm 0.564 | loss_scale 8 | train_wall 50 | gb_free 5.9 | wall 4260\n",
            "2022-08-28 07:47:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 034:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:47:51 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2022-08-28 07:47:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 97/98 [00:50<00:00,  1.95it/s, loss=2.534, nll_loss=0.401, ppl=1.32, wps=4583.4, ups=0.62, wpb=7412.5, bsz=667.7, num_updates=3300, lr=0.0004125, gnorm=0.535, loss_scale=8, train_wall=51, gb_free=5.4, wall=4296]2022-08-28 07:48:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.77s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.33s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.02s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.14it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:48:46 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.661 | nll_loss 7.254 | ppl 152.63 | bleu 26.81 | wps 1155.5 | wpb 1312 | bsz 177.2 | num_updates 3328 | best_loss 8.147\n",
            "2022-08-28 07:48:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3328 updates\n",
            "2022-08-28 07:48:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-28 07:48:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-28 07:49:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint34.pt (epoch 34 @ 3328 updates, score 8.661) (writing took 53.62889674200051 seconds)\n",
            "2022-08-28 07:49:40 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2022-08-28 07:49:40 | INFO | train | epoch 034 | loss 2.534 | nll_loss 0.401 | ppl 1.32 | wps 6581.1 | ups 0.9 | wpb 7316 | bsz 638.4 | num_updates 3328 | lr 0.000416 | gnorm 0.545 | loss_scale 8 | train_wall 50 | gb_free 6.3 | wall 4369\n",
            "2022-08-28 07:49:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 035:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:49:40 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2022-08-28 07:49:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 97/98 [00:51<00:00,  1.92it/s, loss=2.529, nll_loss=0.397, ppl=1.32, wps=6541.5, ups=0.91, wpb=7216.4, bsz=618.8, num_updates=3400, lr=0.000425, gnorm=0.54, loss_scale=8, train_wall=52, gb_free=5.5, wall=4407]2022-08-28 07:50:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.55s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.31s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.04it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.19it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:50:35 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.571 | nll_loss 7.155 | ppl 142.53 | bleu 28.53 | wps 1154.7 | wpb 1312 | bsz 177.2 | num_updates 3426 | best_loss 8.147\n",
            "2022-08-28 07:50:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3426 updates\n",
            "2022-08-28 07:50:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-28 07:50:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-28 07:51:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint35.pt (epoch 35 @ 3426 updates, score 8.571) (writing took 50.887958024 seconds)\n",
            "2022-08-28 07:51:26 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2022-08-28 07:51:26 | INFO | train | epoch 035 | loss 2.518 | nll_loss 0.386 | ppl 1.31 | wps 6737.7 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 3426 | lr 0.00042825 | gnorm 0.516 | loss_scale 8 | train_wall 51 | gb_free 6 | wall 4475\n",
            "2022-08-28 07:51:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 036:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:51:27 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2022-08-28 07:51:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 97/98 [00:52<00:00,  1.91it/s, loss=2.499, nll_loss=0.368, ppl=1.29, wps=6671.5, ups=0.92, wpb=7278.5, bsz=629.2, num_updates=3500, lr=0.0004375, gnorm=0.496, loss_scale=8, train_wall=52, gb_free=5.5, wall=4516]2022-08-28 07:52:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.41s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.24s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.12it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:52:23 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.353 | nll_loss 6.931 | ppl 121.99 | bleu 30.41 | wps 1210.7 | wpb 1312 | bsz 177.2 | num_updates 3524 | best_loss 8.147\n",
            "2022-08-28 07:52:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3524 updates\n",
            "2022-08-28 07:52:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-28 07:52:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-28 07:53:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint36.pt (epoch 36 @ 3524 updates, score 8.353) (writing took 51.01286492100007 seconds)\n",
            "2022-08-28 07:53:14 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2022-08-28 07:53:14 | INFO | train | epoch 036 | loss 2.489 | nll_loss 0.359 | ppl 1.28 | wps 6634.7 | ups 0.91 | wpb 7316 | bsz 638.4 | num_updates 3524 | lr 0.0004405 | gnorm 0.478 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 4583\n",
            "2022-08-28 07:53:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 037:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:53:16 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2022-08-28 07:53:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  99% 97/98 [00:53<00:00,  1.93it/s, loss=2.472, nll_loss=0.346, ppl=1.27, wps=6716.3, ups=0.91, wpb=7354.6, bsz=643.4, num_updates=3600, lr=0.00045, gnorm=0.452, loss_scale=8, train_wall=52, gb_free=5.4, wall=4625]2022-08-28 07:54:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.48s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.19s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.15it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.41it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:54:11 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.245 | nll_loss 6.809 | ppl 112.12 | bleu 30.35 | wps 1390.9 | wpb 1312 | bsz 177.2 | num_updates 3622 | best_loss 8.147\n",
            "2022-08-28 07:54:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3622 updates\n",
            "2022-08-28 07:54:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-28 07:54:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-28 07:55:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint37.pt (epoch 37 @ 3622 updates, score 8.245) (writing took 50.965583172999686 seconds)\n",
            "2022-08-28 07:55:02 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2022-08-28 07:55:02 | INFO | train | epoch 037 | loss 2.468 | nll_loss 0.343 | ppl 1.27 | wps 6646.2 | ups 0.91 | wpb 7316 | bsz 638.4 | num_updates 3622 | lr 0.00045275 | gnorm 0.455 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 4691\n",
            "2022-08-28 07:55:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 038:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:55:03 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2022-08-28 07:55:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  99% 97/98 [00:52<00:00,  1.90it/s, loss=2.463, nll_loss=0.34, ppl=1.27, wps=6713.3, ups=0.92, wpb=7298.2, bsz=639, num_updates=3700, lr=0.0004625, gnorm=0.456, loss_scale=8, train_wall=52, gb_free=5.6, wall=4734]2022-08-28 07:55:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.42s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.19s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.08it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.22it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:55:59 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.079 | nll_loss 6.623 | ppl 98.54 | bleu 31.03 | wps 1171.4 | wpb 1312 | bsz 177.2 | num_updates 3720 | best_loss 8.079\n",
            "2022-08-28 07:55:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3720 updates\n",
            "2022-08-28 07:55:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-28 07:56:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-28 07:57:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint38.pt (epoch 38 @ 3720 updates, score 8.079) (writing took 104.74896103300034 seconds)\n",
            "2022-08-28 07:57:44 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2022-08-28 07:57:46 | INFO | train | epoch 038 | loss 2.464 | nll_loss 0.342 | ppl 1.27 | wps 4428.1 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 3720 | lr 0.000465 | gnorm 0.46 | loss_scale 8 | train_wall 51 | gb_free 6.1 | wall 4853\n",
            "2022-08-28 07:57:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 039:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:57:46 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2022-08-28 07:57:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  99% 97/98 [00:50<00:00,  1.89it/s, loss=2.474, nll_loss=0.356, ppl=1.28, wps=4481.4, ups=0.62, wpb=7285.1, bsz=623.7, num_updates=3800, lr=0.000475, gnorm=0.517, loss_scale=8, train_wall=51, gb_free=5.5, wall=4897]2022-08-28 07:58:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.40s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.16it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.36it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 07:58:41 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.238 | nll_loss 6.818 | ppl 112.86 | bleu 30.58 | wps 1322.5 | wpb 1312 | bsz 177.2 | num_updates 3818 | best_loss 8.079\n",
            "2022-08-28 07:58:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3818 updates\n",
            "2022-08-28 07:58:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-28 07:58:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-28 07:59:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint39.pt (epoch 39 @ 3818 updates, score 8.238) (writing took 51.5801796400001 seconds)\n",
            "2022-08-28 07:59:32 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2022-08-28 07:59:32 | INFO | train | epoch 039 | loss 2.468 | nll_loss 0.35 | ppl 1.27 | wps 6756.7 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 3818 | lr 0.00047725 | gnorm 0.503 | loss_scale 8 | train_wall 50 | gb_free 6.3 | wall 4961\n",
            "2022-08-28 07:59:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 040:   0% 0/98 [00:00<?, ?it/s]2022-08-28 07:59:34 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2022-08-28 07:59:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  99% 97/98 [00:52<00:00,  1.91it/s, loss=2.429, nll_loss=0.311, ppl=1.24, wps=6738.4, ups=0.91, wpb=7392.1, bsz=659.6, num_updates=3900, lr=0.0004875, gnorm=0.391, loss_scale=8, train_wall=52, gb_free=5.4, wall=5006]2022-08-28 08:00:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.74s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.31s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.07it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:00:29 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.117 | nll_loss 6.686 | ppl 102.95 | bleu 28.57 | wps 1399.7 | wpb 1312 | bsz 177.2 | num_updates 3916 | best_loss 8.079\n",
            "2022-08-28 08:00:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3916 updates\n",
            "2022-08-28 08:00:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-28 08:00:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-28 08:01:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint40.pt (epoch 40 @ 3916 updates, score 8.117) (writing took 52.652695363999555 seconds)\n",
            "2022-08-28 08:01:22 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2022-08-28 08:01:22 | INFO | train | epoch 040 | loss 2.427 | nll_loss 0.309 | ppl 1.24 | wps 6540.9 | ups 0.89 | wpb 7316 | bsz 638.4 | num_updates 3916 | lr 0.0004895 | gnorm 0.396 | loss_scale 8 | train_wall 51 | gb_free 6.2 | wall 5071\n",
            "2022-08-28 08:01:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 041:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:01:22 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2022-08-28 08:01:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  99% 97/98 [00:51<00:00,  1.97it/s, loss=2.426, nll_loss=0.312, ppl=1.24, wps=6715.6, ups=0.92, wpb=7312.3, bsz=635.7, num_updates=4000, lr=0.0005, gnorm=0.392, loss_scale=8, train_wall=52, gb_free=5.4, wall=5115]2022-08-28 08:02:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.43s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.17it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.42it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:02:17 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.217 | nll_loss 6.802 | ppl 111.62 | bleu 30.53 | wps 1386.2 | wpb 1312 | bsz 177.2 | num_updates 4014 | best_loss 8.079\n",
            "2022-08-28 08:02:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 4014 updates\n",
            "2022-08-28 08:02:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-28 08:02:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-28 08:03:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint41.pt (epoch 41 @ 4014 updates, score 8.217) (writing took 52.845109140000204 seconds)\n",
            "2022-08-28 08:03:09 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2022-08-28 08:03:10 | INFO | train | epoch 041 | loss 2.423 | nll_loss 0.31 | ppl 1.24 | wps 6658.2 | ups 0.91 | wpb 7316 | bsz 638.4 | num_updates 4014 | lr 0.000499127 | gnorm 0.388 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 5179\n",
            "2022-08-28 08:03:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 042:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:03:10 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2022-08-28 08:03:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  99% 97/98 [00:51<00:00,  1.91it/s, loss=2.418, nll_loss=0.306, ppl=1.24, wps=6728.7, ups=0.92, wpb=7344.2, bsz=638.5, num_updates=4100, lr=0.000493865, gnorm=0.4, loss_scale=8, train_wall=52, gb_free=5.8, wall=5224]2022-08-28 08:04:01 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.38s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.10it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:04:05 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.037 | nll_loss 6.6 | ppl 97.02 | bleu 30.43 | wps 1190.8 | wpb 1312 | bsz 177.2 | num_updates 4112 | best_loss 8.037\n",
            "2022-08-28 08:04:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4112 updates\n",
            "2022-08-28 08:04:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-28 08:04:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-28 08:05:51 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint42.pt (epoch 42 @ 4112 updates, score 8.037) (writing took 105.8517991259996 seconds)\n",
            "2022-08-28 08:05:51 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2022-08-28 08:05:51 | INFO | train | epoch 042 | loss 2.416 | nll_loss 0.304 | ppl 1.23 | wps 4440.8 | ups 0.61 | wpb 7316 | bsz 638.4 | num_updates 4112 | lr 0.000493144 | gnorm 0.395 | loss_scale 8 | train_wall 51 | gb_free 5.9 | wall 5340\n",
            "2022-08-28 08:05:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 043:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:05:51 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2022-08-28 08:05:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  99% 97/98 [00:51<00:00,  1.94it/s, loss=2.402, nll_loss=0.293, ppl=1.23, wps=4493.9, ups=0.62, wpb=7294.8, bsz=632.7, num_updates=4200, lr=0.00048795, gnorm=0.363, loss_scale=8, train_wall=51, gb_free=5.5, wall=5387]2022-08-28 08:06:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.48s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.20s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.14it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.35it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:06:46 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.288 | nll_loss 6.893 | ppl 118.83 | bleu 27.75 | wps 1323.9 | wpb 1312 | bsz 177.2 | num_updates 4210 | best_loss 8.037\n",
            "2022-08-28 08:06:46 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4210 updates\n",
            "2022-08-28 08:06:46 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-28 08:06:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-28 08:07:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint43.pt (epoch 43 @ 4210 updates, score 8.288) (writing took 51.70281048800007 seconds)\n",
            "2022-08-28 08:07:38 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2022-08-28 08:07:38 | INFO | train | epoch 043 | loss 2.401 | nll_loss 0.292 | ppl 1.22 | wps 6701 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 4210 | lr 0.00048737 | gnorm 0.364 | loss_scale 8 | train_wall 50 | gb_free 5.8 | wall 5447\n",
            "2022-08-28 08:07:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 044:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:07:38 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2022-08-28 08:07:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  99% 97/98 [00:52<00:00,  1.90it/s, loss=2.387, nll_loss=0.281, ppl=1.21, wps=6684.3, ups=0.92, wpb=7302.1, bsz=644.3, num_updates=4300, lr=0.000482243, gnorm=0.35, loss_scale=8, train_wall=51, gb_free=5.4, wall=5496]2022-08-28 08:08:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.81s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  50% 2/4 [00:03<00:02,  1.49s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.15s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:08:35 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.274 | nll_loss 6.913 | ppl 120.52 | bleu 28.09 | wps 1010 | wpb 1312 | bsz 177.2 | num_updates 4308 | best_loss 8.037\n",
            "2022-08-28 08:08:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4308 updates\n",
            "2022-08-28 08:08:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-28 08:08:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-28 08:09:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint44.pt (epoch 44 @ 4308 updates, score 8.274) (writing took 50.48100160600006 seconds)\n",
            "2022-08-28 08:09:26 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2022-08-28 08:09:28 | INFO | train | epoch 044 | loss 2.386 | nll_loss 0.279 | ppl 1.21 | wps 6631 | ups 0.91 | wpb 7316 | bsz 638.4 | num_updates 4308 | lr 0.000481795 | gnorm 0.352 | loss_scale 8 | train_wall 50 | gb_free 6 | wall 5555\n",
            "2022-08-28 08:09:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 045:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:09:28 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2022-08-28 08:09:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  99% 97/98 [00:51<00:00,  1.91it/s, loss=2.385, nll_loss=0.283, ppl=1.22, wps=6687.1, ups=0.91, wpb=7326.3, bsz=641.4, num_updates=4400, lr=0.000476731, gnorm=0.36, loss_scale=8, train_wall=52, gb_free=5.8, wall=5606]2022-08-28 08:10:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.39s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.14s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.16it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.42it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:10:23 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.306 | nll_loss 6.951 | ppl 123.73 | bleu 31.05 | wps 1375.5 | wpb 1312 | bsz 177.2 | num_updates 4406 | best_loss 8.037\n",
            "2022-08-28 08:10:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4406 updates\n",
            "2022-08-28 08:10:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-28 08:10:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-28 08:11:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint45.pt (epoch 45 @ 4406 updates, score 8.306) (writing took 51.84140084000046 seconds)\n",
            "2022-08-28 08:11:15 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2022-08-28 08:11:15 | INFO | train | epoch 045 | loss 2.383 | nll_loss 0.281 | ppl 1.22 | wps 6724.1 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 4406 | lr 0.000476407 | gnorm 0.355 | loss_scale 8 | train_wall 51 | gb_free 6.1 | wall 5664\n",
            "2022-08-28 08:11:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 046:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:11:15 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2022-08-28 08:11:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  99% 97/98 [00:51<00:00,  1.90it/s, loss=2.364, nll_loss=0.265, ppl=1.2, wps=6784.7, ups=0.93, wpb=7317.7, bsz=629.5, num_updates=4500, lr=0.000471405, gnorm=0.319, loss_scale=8, train_wall=52, gb_free=5.5, wall=5713]2022-08-28 08:12:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.45s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.32s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.05it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.21it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:12:10 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.463 | nll_loss 7.124 | ppl 139.53 | bleu 29.21 | wps 1127.8 | wpb 1312 | bsz 177.2 | num_updates 4504 | best_loss 8.037\n",
            "2022-08-28 08:12:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4504 updates\n",
            "2022-08-28 08:12:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-28 08:12:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-28 08:13:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint46.pt (epoch 46 @ 4504 updates, score 8.463) (writing took 53.555496623999716 seconds)\n",
            "2022-08-28 08:13:04 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2022-08-28 08:13:04 | INFO | train | epoch 046 | loss 2.363 | nll_loss 0.264 | ppl 1.2 | wps 6576.6 | ups 0.9 | wpb 7316 | bsz 638.4 | num_updates 4504 | lr 0.000471195 | gnorm 0.322 | loss_scale 8 | train_wall 51 | gb_free 6.1 | wall 5773\n",
            "2022-08-28 08:13:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 047:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:13:04 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2022-08-28 08:13:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  99% 97/98 [00:51<00:00,  1.89it/s, loss=2.36, nll_loss=0.265, ppl=1.2, wps=6681.3, ups=0.91, wpb=7340.8, bsz=639.3, num_updates=4600, lr=0.000466252, gnorm=0.316, loss_scale=8, train_wall=52, gb_free=5.5, wall=5823]2022-08-28 08:13:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.42s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.16it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.27it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:13:59 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.944 | nll_loss 6.527 | ppl 92.24 | bleu 30.05 | wps 1247.3 | wpb 1312 | bsz 177.2 | num_updates 4602 | best_loss 7.944\n",
            "2022-08-28 08:13:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4602 updates\n",
            "2022-08-28 08:13:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-28 08:14:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-28 08:15:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint47.pt (epoch 47 @ 4602 updates, score 7.944) (writing took 106.3255523810003 seconds)\n",
            "2022-08-28 08:15:45 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2022-08-28 08:15:46 | INFO | train | epoch 047 | loss 2.359 | nll_loss 0.264 | ppl 1.2 | wps 4416.1 | ups 0.6 | wpb 7316 | bsz 638.4 | num_updates 4602 | lr 0.000466151 | gnorm 0.327 | loss_scale 8 | train_wall 50 | gb_free 6.2 | wall 5935\n",
            "2022-08-28 08:15:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 048:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:15:46 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2022-08-28 08:15:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  99% 97/98 [00:50<00:00,  1.91it/s]2022-08-28 08:16:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.62s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.26s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.11it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.37it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:16:41 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.492 | nll_loss 7.163 | ppl 143.28 | bleu 28.24 | wps 1382.4 | wpb 1312 | bsz 177.2 | num_updates 4700 | best_loss 7.944\n",
            "2022-08-28 08:16:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4700 updates\n",
            "2022-08-28 08:16:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-28 08:16:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-28 08:17:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint48.pt (epoch 48 @ 4700 updates, score 8.492) (writing took 53.16408286399928 seconds)\n",
            "2022-08-28 08:17:34 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2022-08-28 08:17:34 | INFO | train | epoch 048 | loss 2.364 | nll_loss 0.272 | ppl 1.21 | wps 6653.3 | ups 0.91 | wpb 7316 | bsz 638.4 | num_updates 4700 | lr 0.000461266 | gnorm 0.368 | loss_scale 8 | train_wall 50 | gb_free 5.9 | wall 6043\n",
            "2022-08-28 08:17:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 049:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:17:34 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2022-08-28 08:17:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  99% 97/98 [00:50<00:00,  1.92it/s]2022-08-28 08:18:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.60s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.28s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.08it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.36it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:18:29 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.431 | nll_loss 7.093 | ppl 136.56 | bleu 28.5 | wps 1335 | wpb 1312 | bsz 177.2 | num_updates 4798 | best_loss 7.944\n",
            "2022-08-28 08:18:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4798 updates\n",
            "2022-08-28 08:18:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-28 08:18:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-28 08:19:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint49.pt (epoch 49 @ 4798 updates, score 8.431) (writing took 51.15981729899977 seconds)\n",
            "2022-08-28 08:19:20 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2022-08-28 08:19:20 | INFO | train | epoch 049 | loss 2.339 | nll_loss 0.248 | ppl 1.19 | wps 6766.1 | ups 0.92 | wpb 7316 | bsz 638.4 | num_updates 4798 | lr 0.000456531 | gnorm 0.299 | loss_scale 8 | train_wall 50 | gb_free 6.1 | wall 6149\n",
            "2022-08-28 08:19:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 98\n",
            "epoch 050:   0% 0/98 [00:00<?, ?it/s]2022-08-28 08:19:20 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2022-08-28 08:19:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  99% 97/98 [00:52<00:00,  1.95it/s, loss=2.339, nll_loss=0.247, ppl=1.19, wps=4398.5, ups=0.6, wpb=7322.9, bsz=641.5, num_updates=4800, lr=0.000456435, gnorm=0.298, loss_scale=8, train_wall=51, gb_free=5.4, wall=6153]2022-08-28 08:20:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.40s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.24s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.04it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 08:20:17 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.445 | nll_loss 7.132 | ppl 140.27 | bleu 30.25 | wps 1105.9 | wpb 1312 | bsz 177.2 | num_updates 4896 | best_loss 7.944\n",
            "2022-08-28 08:20:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4896 updates\n",
            "2022-08-28 08:20:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-28 08:20:29 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-28 08:21:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint50.pt (epoch 50 @ 4896 updates, score 8.445) (writing took 52.886542584999916 seconds)\n",
            "2022-08-28 08:21:10 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2022-08-28 08:21:10 | INFO | train | epoch 050 | loss 2.337 | nll_loss 0.25 | ppl 1.19 | wps 6512.6 | ups 0.89 | wpb 7316 | bsz 638.4 | num_updates 4896 | lr 0.000451938 | gnorm 0.299 | loss_scale 8 | train_wall 50 | gb_free 5.9 | wall 6259\n",
            "2022-08-28 08:21:10 | INFO | fairseq_cli.train | done training in 6259.0 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:16:00.904977Z",
          "iopub.execute_input": "2022-07-24T19:16:00.905444Z",
          "iopub.status.idle": "2022-07-24T19:16:01.196986Z",
          "shell.execute_reply.started": "2022-07-24T19:16:00.905403Z",
          "shell.execute_reply": "2022-07-24T19:16:01.195842Z"
        },
        "trusted": true,
        "id": "yL7nIungdwcw",
        "outputId": "44265e80-a43c-41a0-f82d-85c7cff53714",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdata-bin\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang dsb --target-lang hsb \\\n",
        "      --beam 5 data-bin/wmt_dsb_hsb | grep -P \"D-[0-9]+\" | cut -f3 > target.txt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:05:22.918437Z",
          "iopub.execute_input": "2022-07-24T18:05:22.919517Z",
          "iopub.status.idle": "2022-07-24T18:06:13.344631Z",
          "shell.execute_reply.started": "2022-07-24T18:05:22.919454Z",
          "shell.execute_reply": "2022-07-24T18:06:13.343313Z"
        },
        "trusted": true,
        "id": "PdGwekMXdwcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6ff2543-9d1c-4725-f3fb-719c4b9cac41"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 08:27:10 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 524, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 524, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_dsb_hsb', 'source_lang': 'dsb', 'target_lang': 'hsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 08:27:11 | INFO | fairseq.tasks.translation | [dsb] dictionary: 104024 types\n",
            "2022-08-28 08:27:11 | INFO | fairseq.tasks.translation | [hsb] dictionary: 103512 types\n",
            "2022-08-28 08:27:11 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-28 08:27:18 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-28 08:27:18 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-28 08:27:18 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-28 08:27:24 | INFO | fairseq_cli.interactive | Total time: 14.057 seconds; translation time: 5.412\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat target.txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:13.348554Z",
          "iopub.execute_input": "2022-07-24T18:06:13.348954Z",
          "iopub.status.idle": "2022-07-24T18:06:14.121539Z",
          "shell.execute_reply.started": "2022-07-24T18:06:13.348910Z",
          "shell.execute_reply": "2022-07-24T18:06:14.108183Z"
        },
        "trusted": true,
        "id": "BtsGN-Fydwcy",
        "outputId": "f36f735c-97b0-48c7-a8de-6d8370f0cedc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ja změju dowol.\n",
            "Ja so myslu.\n",
            "Budźće dźakowni – wón je hnadny a jeho dobrota traje wěčnje.\n",
            "To a kozy su lochke wobsahuja.\n",
            "Knježe, ja so, přetož ja njejsym sej to, zo by ty pod moje třěchu a tohodla njejsym sej samoho za tebje rukować: zo by ty sam za tebje rukuje, móhł, zo by so mój wotročk móhł, zo by ja hižo to, zo by ja dał wot tebje rukuje, rosće, zo by ja dał wot tebje rukuje, rosće, zo by ty pod tym zo by ja hižo ani my ani my ani my ani my ani my ani my ani mjeno přidatka njezwisuja.\n",
            "W spódnjej kašćiku leža njepodpěruje.\n",
            "To njeje mój mantl.\n",
            "Na tymaj dwěmaj dalekubłanskimaj wisa cyły testament a krala.\n",
            "Jeho knihi su so chwilku do 25 rěčow.\n",
            "W tym času je Brězan započał, so na dźěle serbskeho ludu.\n",
            "Wot lěta dźěćo, hač do 1944 dyrbjał jako wojak němskeho wójska a jako tajki přišoł do města!\n",
            "Po wšěm zdaću je so Brězan do Łužicy wróćił a skutkowaše hač do lěta 1948 jako młodźinski Domowiny.\n",
            "Po jeho woli bě Němski muzej dźěła, dósta jeho literarne konwencije kotrež wopřija nas małkich hladachu.\n",
            "Po wšěm prócowanjach w Praze je so Brězanej poradźiło, w pólskim Wrócławju, na statnym gymnaziju, na kotrymž je so němski šewc.\n",
            "Jurij Brězan bě so 9. junija 1916 we Wojerowskej chorowni narodźi so 12. měrca 2006 w chorowni w chorowni wumrěła,\n",
            "Na dźěle poroda sem je jeho dźěle mjeno Georg zapisane.\n",
            "Pozdźišo da sej swoje serbske mjeno na zakładźe zakonja wo wobchowanju prawow serbskeje ludnosće z dnja 23. měrca 1948 změnić.\n",
            "Jurij Brězan płaći jako přesahowacy serbski spisowaćel druheje połojcy 20. lětstotka.\n",
            "Ze swojimi srjedu tež w němskej rěči započa sej twarješe móst k němskej literaturje.\n",
            "Předewšěm pisaše studenća zhašeć, powědanja a dźěćace knihu.\n",
            "Wot lěta spjelnjeja chodźeše na Budyski gymnazij serbskeho ludu.\n",
            "W Drježdźanach abituru bu jemu zakazane.\n",
            "Za swoje zažne tworićelske wotročka je tehdy tež spodźiwne.\n",
            "Po swojim přewróće do Łužicy bu swojeho politiskeho angažementa dla zaso w Drježdźanach zajaty.\n",
            "Je była zdobom ze zakazom přebywanskim w Łužicy.\n",
            "Hišće w samsnym lěće dnja nawoda wjednik w serbskim zarjedźe za kulturu a kubłanje ludu krajneho knježerstwa Sakskeje.\n",
            "W poziciji knjejstwoweho rady bě zamołwity za nowiny, rozhłós a film.\n",
            "spjelnjeja spjelnjeja stronje Němskeje studenća wučerstwa Jurij Brězan w lěće přesadźić.\n",
            "Wot lěta 1949 skutkowaše jako swobodny spisowaćel.\n",
            "W lěće 1964 měješe so čłonka němskeho wuraza a zapada.\n",
            "W lěće 1965 je była čłon Zarjadniskeho\n",
            "Wiceprezident Zwjazka była NDR bě Brězan wot 1969 do celow zawrjeny?\n",
            "Jurij Brězan dósta w času NDR rjad radźićelow,\n",
            "1951, 1964 a 1976 dósta narodne myto.\n",
            "W lěće 1962 dósta Myto krala.\n",
            "1974 dósta wuznamjenjenje přija.\n",
            "Mnohe romanow a stawiznow Brězana maja ptačokwasny elementy.\n",
            "K jeho serbskeho krala.\n",
            "Krabatowej tematice so wosebiće Jurij Brězan tež z bohateho wobstatka serbskich ludowych powěsćow.\n",
            "Najprjedy přełoženy bu 1955 załožene.\n",
            "Hač do swojeje smjerće bě Jurij Brězan blisko swojeje ródneje wsy žiwy.\n",
            "Hižo wjacore lěta do smjerći Brězana běše dźěle doprědka ćěri, zajim na jeho priwatnych inwesticijow wobeńć.\n",
            "Z tym spjelnjeja wažny archiw serbsku literaturu jako připóznaty dźěl swětoweje literatury.\n",
            "Jeho hrozny plan spjelni so.\n",
            "To by była wobužna so.\n",
            "Ale jako so mócny přiklesk. To so tón wětřik naraz wobroći.\n",
            "dźěle wódny mysli na wjećbu Vendetta.\n",
            "Mała Sonja Hrjehorjowa na swojeho nana, kotraž zaso raz nadhodźiny čini.\n",
            "Zo byšće čas so spěšnišo bórze pominy, sej přeco zaso nowe stawizny.\n",
            "Što wobsedźi we swojej dobrym hrodźe wšitko, štož je raz do rěki padnyło.\n",
            "Wšo zo na tołste, mjasne wački?\n",
            "Te pak je so horliwy lud dołho wokoło swojeho krala zhubili, žedźeli\n",
            "A jako te runje wjeseli so stać njemóže.\n",
            "Ale woni su jeho słowa.\n",
            "W mučny.\n",
            "Kraj spjelnjeja serbskeho lud podpěra je hłoda a krala.\n",
            "Hač drje božo – studenća wučerstwa za tym krala.\n",
            "Holčka sydnje swoje młodźata dołho sobu na Hižo do raja serbskeho krala.\n",
            "Wona so aktiwnje do podawki na jewišću zapřijima.\n",
            "Pušć k piću stwjelco stwjelco a studenća što so stanje.\n",
            "Z toho a studenća dźěći so přewjedźe.\n",
            "Do Kamjenski praša so přewjedźe.\n",
            "Što to je – zhubili, praša so rady wědźał!\n",
            "Mi so lubi.\n",
            "dźěle dołho a Witko přinjese kóždemu wulki lód.\n",
            "Kak so pinguina Kurta,\n",
            "To so jara wjeseli.\n",
            "To je překwapjenka je so poradźiła.\n",
            "Njejapce sakski stany a so rozžohnuje.\n",
            "Ale ja so kóždy wokomik dótkać.\n",
            "Namoluj a započeć?\n",
            "Spočatny so wosebje derje njerozumja.\n",
            "Wšitko zwjazuje sej ze nim.\n",
            "Kotry lětny čas a kotry měsac mamy?\n",
            "A do Krabatoweho młyna.\n",
            "A kak wupada z tym serbskim na wsy?\n",
            "A z toho započeć?\n",
            "Daj so myslu.\n",
            "Za wšo, dźěło była zbožowna, dyrbja so deleka napisane wobstarać.\n",
            "Nowe serbske programy za dodawk wotměwaja so w přichodźe na jewišću.\n",
            "Dźěćace zarjadnišća a zakładne principije móža so w Němsko-Serbskim ludowym dźiwadle pola knjeni Li za předstajenja přizjewić.\n",
            "We wšěm dobrym žiwjenjom. Hinak hač dyrbi so najprjedy njesydnu!\n",
            "Na nje předstaja dźěći a młodostni pilnje studenća hry rady zaso.\n",
            "Z pomocu njese dźěle a dalšich dźěle sćežorow, a tež přez mimiku a gestiku radźi so z małymi byrnjež hakle serbsce wuknu.\n",
            "dźěle su zwjetša dwurěčne serbsce a němski – a lětsa samo hišće zahrody.\n",
            "„Tam da wšudźe lód a sněh a zymne wjetši Serbow.\n",
            "A tak skutkuje wopyt do dźiwadła přez swoju intensitu a wosebitosć hišće dołho njeje.\n",
            "Konfiguracija móže so hromadźe, zo by so tež doma abo w dźěćacej dźiwadło hraje.\n",
            "Dwaj mjeńšej ruma a móličke kupjel steja tři- do tuteho dźěćimi k dispoziciji.\n",
            "Tu namakajće prašenja, z kotrymž móžeće so z Wašim dźěsćom do rozmołwy dać.\n",
            "Hač profesionelne abo lajske dźiwadło – z wulkej smilnosću. a wjeselu so při hraću tež wšitke sobuskutkowarjow, zo so tež jako dźiwadło we wšěch swojich formach tež něšto za serbsku rěč stawa.\n",
            "Tu so sposrědkuje wěda na žiwy a studenća wučerstwa w nowych zwiskach a hnuje zwjazane su.\n",
            "studenća zapřija dźěći direktnje a studenća je, zo by z nimi ze serbskeho krala rěčeli.\n",
            "Witko jemu wotmołwi: Hižo raz, hdyž so tak rjenje dźěle potom běžimy na swjedźeń abo do města!\n",
            "W nowym nastupanju wona tak samotna a požada za nowym šefredaktorom dokelž jeje najlěpša přećelka preč .\n",
            "Tute inscenacije wopytachu lětsa zastojnstwa so mjez nimi wjele hólcow a holcow ze serbskich dźěćacych dnjowych přebywanišćach.\n",
            "W zašłych lětdźesatkach pokazachu serbske a serbsce wuknjace dźěći a młodostni wjace generacijow wyše 30 wšelakich online-poskitkow.\n",
            "W minjenych lětdźesatkach pokazachu nimale 30 namakali, na kotrymž su so cyłe generacije serbskich dźěći a młodostnych wobdźělili.\n",
            "Je była ze šulerjemi serbskeho gymnazija Budyšin a wobeju serbskej šulow Worklecy a Budyšin nowy dźěl serbskeho ludu.\n",
            "Runje tak wobkedźbuja dźěći tež na druhich, kak studenća do wšelakich ćisnyć.\n",
            "Dźěći spjelnjeja so ze znjamjeńša tekst, zo by so, kostimy a rekwizity – cyle kaž w prawym přesadźić.\n",
            "Waše měnjenja, a skazanki sćelće prošu na Póstowe naměsto 2 w Budyšinje.\n",
            "Wjeršk kóždeho lěta je ale přeco zaso ptačokwasny program.\n",
            "Njedawa nawodom serbskeho hrajerja měnić, kotrež so dźěle była a Susann Domanja.\n",
            "We wšěch bjezposrědnje na te małe dźěle reagować, jich ptačokwasne a reflektować a je snano samo direktnje sobu do jednanja rěčnje na jewišću zapřijeć.\n",
            "Tak dźěle ze swojimi zaćišćemi so hromadźe, ale móža to widźane zhromadnje předźěłać a so z druhimi kónčinami.\n",
            "spjelnjeja tak, zo wšitke dźěći kubłanišća kubłanišća – tuž tež dalše 20 dźěći spěchujemy.\n",
            "Němsko-Serbske ludowe dźiwadło dźe z tym kruchom wědomje dźěćacych dnjowych přebywanišćach po puću docpě a zo móhli dźěći intensiwnišo dźěći do jednanja rěčnje a rěčnje wutworił.\n",
            "Dokelž serbske dźiwadło wobsah ze stron srědkami jako mimiku, gestiku a atraktiwnymi programami.\n",
            "A potom spěwaja někotre spěwy – mjez druhim k jutram bojeć. A při kotrym jich zajac – ale jenož hodźi so tež někotre naspomnili.\n",
            "Bórze du zaso ćiche po jutrownu wodu, čaje pozdźišo wopytuja, kak so hromada za chodojtypalenje a kak sej jich meju?\n",
            "Wot 9. julija hač do 12. julija 2015 wotměje so w Budyšinje, Chrósćicach a Hochozy zaso folklorny festiwal sakski musketěr.\n",
            "W zapisku wobsahi su slědowace dypki zapisane.\n",
            "Dźěćom so slědowacy ze započa kubłanišćach rěka Na dźěle a studenća zhubili, žedźeli a recept kradnyć.\n",
            "Witaj wěš što?\n",
            "Mam noweho přećela – krala.\n",
            "skazanku praša so Witko, studenća wučerstwa w kuchni.\n",
            "Što?\n",
            "Haj, tak móže so to rjec.\n",
            "Čemu serbskeho krala je.\n",
            "Hižo běhać přeco w čornym poskića.\n",
            "Chceš hić do celow zawrjeny?\n",
            "Nětko powěda jemu čmjeła Hana hrajerjow na jewišću a wot podawkow, kotryž hraja.\n",
            "Aha, tam je zawěsće rjenje.\n",
            "Ale nochce do dźiwadła.\n",
            "Wón chce zaso domoj.\n",
            "Wón bydli na južnym krala.\n",
            "Na to praša so kóždy zhubili, tebje tam zhubili, žedźeli\n",
            "To wotmołwi započeć?\n",
            "Mam tola husta a tołstu třibarbnu pod kožu.\n",
            "To ćopło mi dźerži.\n",
            "Pola was je jara rjenje.\n",
            "Łuki su pisane a kwětki tak rjenje wonja krasnje.\n",
            "Tajke něšto znał.\n",
            "A zrudnje serbskeho krala.\n",
            "Tu tola je tak jara ćopło.\n",
            "Ow, mjeno je bratrowska\n",
            "Snano sym započeć?\n",
            "Witko ma ideju.\n",
            "Njedawa mam něšto zymneho za tebje.\n",
            "To budźe ći pomhać.\n",
            "Hm, bě dobre a tak krasnje hraja.\n",
            "To běše wuspěšna była).\n",
            "Tym třom słodźi lód wulkotny derje.\n",
            "To pak dyrbju woteńć.\n",
            "Na pój, moje přećelow!\n",
            "Snano widźimy so raz na južnej skłoninje skalneje wyšiny.\n",
            "spjelnjeja so derje, bojeć. To je była a naš serbskeho krala.\n",
            "Zeleny spjelnjeja k piću stwjelco ze swojeho systemoweho dnjowym přebywanišćom a dwě dźěle wot strony přesahowacym skutkowanjom.\n",
            "Při komunikaciji tak daloko je.\n",
            "Nětko spjelnjeja na wobě zadnjej stronje regionalneje dźěle wotzamknjene.\n",
            "Tykń dołhi kónc k piću stwjelca do balonka a studenća jón ze swojimi datymi wosebitosćemi.\n",
            "Hru přez k piću chwilu trjebać.\n",
            "Hdyž sy jeho do toho hižo raz wuspytał.\n",
            "Nětko njedam.\n",
            "Hladaj so, zo so wěžu do hromady su.\n",
            "Husy a kački płuwaja na wodźe.\n",
            "Beno, serbskeho krala.\n",
            "Staj konje a kruwy na pastwu!\n",
            "Stajće konje a kruwy na pastwu!\n",
            "Kotre zwěrjata so hišće nic?\n",
            "Mi so dwór lubi.\n",
            "To so na mjedwjedźa.\n",
            "Njezłob so.\n",
            "Pójće, započa so do zastojnstwa je.\n",
            "Pójće, započa so do zastojnstwa je.\n",
            "Pokazajće so!\n",
            "Stańće a stajće stólc k započeć?\n",
            "To naprawo a krala.\n",
            "znamješko doprědka a dozady.\n",
            "Nastawki prawu a konik njetrjeba.\n",
            "Stajće prawu a lěwu šipkowu doprědka!\n",
            "Stajće prawu a lěwu brjowku nahrawać.\n",
            "Dźiće do krala.\n",
            "Sydń so na twój započeć?\n",
            "To so na mjedwjedźa.\n",
            "Tołsty čmjełu znaja a sydnje so na twój nós.\n",
            "Stańće a kóždy čas bjez krala.\n",
            "Nětko spjelnjeja ja a kóždy wokomik nad kruh.\n",
            "Spěšnje a mjelčo dachu naše wěcy rady.\n",
            "Tykń a studenća naprawo a nalěwo jenak daloko je.\n",
            "Myj sej myslu.\n",
            "To so krala.\n",
            "započa so ze znjamjeńša krala.\n",
            "Dźensa dźěle a pječemy w našej dźěćacej kuchni.\n",
            "My chlěba a pječemy je.\n",
            "Wočinjej a wzmi wulki Tveiranowy dom.\n",
            "Ow, mloko so wari!\n",
            "započa so kóždy wokomik wjedu.\n",
            "znamješko so w sportowym rumje.\n",
            "kóždy wy so na mjedwjedźa.\n",
            "Hru so mjelčo a za sobu!\n",
            "Štóž je hotowy, so na Torhošću zawrěć.\n",
            "spjelnjeja so po wšěm rěči,\n",
            "spjelnjeja so po wšěm rěči,\n",
            "spjelnjeja so w rěči,\n",
            "My póńdźemy do zahrody.\n",
            "To je so.\n",
            "Njedawa prawu a lěwu ruku.\n",
            "dźěle prawu a lěwu šipkowu njeje.\n",
            "spjelnjeja so po wšěm rěči,\n",
            "dźěle poněčim spěšnje.\n",
            "Dźerž kruće!\n",
            "Dźeržće so kruće!\n",
            "To so!\n",
            "To so!\n",
            "To so a započa sportowy toboła.\n",
            "Nastawki a přikryw.\n",
            "To so!\n",
            "znamješko so a začińće woči!\n",
            "Woblečće so a celow njepušćiš?\n",
            "Nastawki nochcu a přikryw.\n",
            "Mój tunl je so do hromady stajili.\n",
            "To nochcu ze zahrody.\n",
            "To nochcu ze zahrody.\n",
            "Daj so myslu.\n",
            "Štó dźe so ze mnu wobstać.\n",
            "Dźerž so z woběmaj mjedwjedźa.\n",
            "spjelnjeja so po wšěm rěči,\n",
            "To so po brjuše nadeńdźe.\n",
            "spjelnjeja so při wšěm rěči,\n",
            "Mucha so w dwěmaj započa\n",
            "To so!\n",
            "Wy sće so zhubił,\n",
            "To so na to!\n",
            "Pójće, wostanu so na pokoj!\n",
            "Nětko jěmy a pijemy.\n",
            "To ja a my započeć?\n",
            "Ja Chěža a my kóždy wokomik jědźa.\n",
            "Ja Chěža a my dźěle něšto pić?\n",
            "Wjeselu so na słódnu wječerju.\n",
            "Ja była a my zastojnstwa hromadźe.\n",
            "Nastawki dźěle a studenća na blido!\n",
            "Nastawki dźěle a studenća na blido!\n",
            "To spjelnjeja a krala.\n",
            "To spjelnjeja a krala.\n",
            "Dajće so pjekar\n",
            "Ćeta duchi ja sym so přewjedźe.\n",
            "Ćeta duchi ja sym so přewjedźe.\n",
            "Nam je była dźakujemy so.\n",
            "Nastawki a kóždy wokomik wobsahuja.\n",
            "To nochcu a nochcu studenća wučerstwa\n",
            "W zahrodźe wisa skónčić?\n",
            "Mać by była a krala.\n",
            "Dźěd a wowka pomhataj na wopyt.\n",
            "Dźěd a my wšitke rěči,\n",
            "Hrajemy z nanom přidawki, da so!\n",
            "Prajimoj staršimaj: a studenća Dobru nóc!\n",
            "Chcemy so woblec.\n",
            "To so!\n",
            "To so!\n",
            "Dyrbiš so wuslěkać njechamy.\n",
            "Sleč so!\n",
            "Łuka\n",
            "Nastawki a trajne nadawki maja dźěła.\n",
            "Baćony a studenća so k nam nawróća.\n",
            "Młodźi ptački so krala.\n",
            "Kajke woraja woni a celow je.\n",
            "Wjeselu so na lěće.\n",
            "Wichor so zběha.\n",
            "Na njebju so błyska a studenća wučerstwa\n",
            "To je so.\n",
            "To njepřińdźe\n",
            "Na łuce so syno dźěła.\n",
            "My so myslu.\n",
            "Zdrasćimy so kupanski woblek.\n",
            "Zdrasćimy so kupanski cholowy njezabudu.\n",
            "spjelnjeja so ze w rěči, što?\n",
            "To je so.\n",
            "My so myslu.\n",
            "Dźensa so w basenku rěči,\n",
            "Wětřik duje a husto je jara wichorojte.\n",
            "W nazymje so husto rěči,\n",
            "To spjelnjeja a kóždy wokomik tu.\n",
            "Dźěći, dźěle a dźěd pytaja hriby.\n",
            "Zyma so zbliža.\n",
            "Wulke sněženki so na moju ruku.\n",
            "spjelnjeja a symjenja do krala.\n",
            "Na haće krala.\n",
            "Holcy so ze sněhom pokryte.\n",
            "Wjeselimy so na hody.\n",
            "Dyrbiće so do rjadu stajić.\n",
            "spjelnjeja so do celow zawrjeny?\n",
            "Ja započeć?\n",
            "kóždy a započa swój chwilku nam krala.\n",
            "Wšitcy wjeselo so na jutrońčku.\n",
            "A ja měniš?\n",
            "To pomjenuje so přewjedźe.\n",
            "W meji a aprylu bu spjećowanje meje a mejemjetanje su.\n",
            "Na wšěm zdaću je započeć?\n",
            "Hromada so pali a dźěći so wjesela.\n",
            "Na pój, so mejski štom Božoh' dźěsćowy štom?\n",
            "Meja so mjedwjedźa.\n",
            "Nad čim sy so najbóle dojednałoj?\n",
            "Nad čim sy so najbóle dojednałoj?\n",
            "dźěle kulu a wuhladawši na pokoj!\n",
            "Njeboj so přewjedźe.\n",
            "To so hodźi.\n",
            "Psyk a kóčka so w korbiku.\n",
            "spjelnjeja a kóždy lećitej w rěči,\n",
            "Widźiš ty ryby a kralestwo w spjelnjeja je.\n",
            "To kóčka a była zbožowna, być.\n",
            "tutej kokoš a studenća pytaja njepodpěruje.\n",
            "Hru kačka a studenća płuwaja na haće.\n",
            "spjelnjeja huso a studenća trjebaja studenća wučerstwa\n",
            "wšěm sanc, sančka, a hołbik sedźa na třěše.\n",
            "kóždy dźěle a studenća leža na łuce.\n",
            "Kozole, a nazhonjeja zhromadnje hrajkaja.\n",
            "Hada a wšěch su we wosebitym domje.\n",
            "To je so.\n",
            "Sydń so na pokoj!\n",
            "To so na mjedwjedźa.\n",
            "Na dźěle a studenća leža studenća hromadźa prěnje a studenća hromadźa\n",
            "spjelnjeja kóždy wokomik a wuskutk swojeho dźerža so w załožichu je.\n",
            "To je železnica a tam je była za spěšne awta.\n",
            "dźěle spjelnjeja a studenća dźě honač husy pase, a kački bojeć.\n",
            "Tu steja była a wšěch awta awta a studenća a kolesa.\n",
            "dźěle běše blido a studenća wučerstwa kamor kamor a studenća steja w kuchni.\n",
            "Lěsnik za sorny, sorny, zajacy, a dźiwje swinje picu do lěsa třěšneho zwjazka.\n",
            "Hru so ptačokwasny a narodna drasta drasta a přewjedźe so ćah předstajić.\n",
            "W wjelbiku su studenća wučerstwa a hrajne stawy hrali.\n",
            "Dźěći pjeku truskalcowy twarja hród a puće, studenća wučerstwa pěsk a studenća wjes.\n",
            "W napojišću leža serbskorěčne kubłarske cyłki, morcheje a rěpy.\n",
            "Nastawki spjelnjeja a kóždy wokomik započa zhladuje swinjo a wjewjerčka su lěsne zwěrjata.\n",
            "Tu kruwy, haj.\n",
            "W lěsu.\n",
            "We wjelbiku steja niske blida, małe wobstaranja polcy a kamory za hrajki a wulke blido a stólc za kubłarku podšmórnje.\n",
            "We wulkej kamorje su cedlku a studenća wučerstwa poboku.\n",
            "W zahrodźe je pěskowy kašćik, studenća wučerstwa a jednory kuzłar w skupinkach a róžk k sedźenju za fota wobrazowki k započeć?\n",
            "W małej rěči, su: rano, šalki, a studenća wučerstwa a studenća a studenća wučerstwa poboku.\n",
            "Witaj k nam!\n",
            "Přińdź strowy zaso!\n",
            "Přińdźće strowe zaso!\n",
            "Šikwany kónc krala.\n",
            "Wjesołe truskalcy.\n",
            "Wjesołe jutry my wam přejemy.\n",
            "To su wosoby w swójbje je.\n",
            "To je naš nan.\n",
            "To je moja mama.\n",
            "To je naša mama.\n",
            "To stej mojej zhubili,\n",
            "Staršej matej syna a dźowku.\n",
            "Syn je hólc a dźowka holca.\n",
            "Mam kóždy wokomik serbskeho krala.\n",
            "Ja mam sotru,\n",
            "To je mój dźěd.\n",
            "To je naš dźěd.\n",
            "To je moja wowka.\n",
            "To je naša wowka.\n",
            "Ja mam dźens pobych.\n",
            "Wuj je naš zhubił,\n",
            "To je naš zhubili,\n",
            "Ćeta je naša krala.\n",
            "To je naša krala.\n",
            "Witaj, mjeno rěka Jurij Wuješ\n",
            "Mi so Marta Šimanowa je.\n",
            "Knjez Nowak je naš nan.\n",
            "Knjeni dźěle je naša mama.\n",
            "Mojej wulkej sotry praja wudźerjo je.\n",
            "Našej swójbje rěka wosebiće započeć?\n",
            "To je naše bydlenje.\n",
            "Naša swójba bydli na wsy.\n",
            "Naša swójba bydli w měsće.\n",
            "My mamy bratrowska dom.\n",
            "My mamy bydlenje\n",
            "My bydlimy na pokoj!\n",
            "My mamy chěžku, wobsahuja.\n",
            "Mamy tež serbskeho krala započa studenća wučerstwa a zahrodu.\n",
            "Tu je započeć?\n",
            "Nastawki ma so kóždy wokomik\n",
            "Na wšěch wisa njetrjeba.\n",
            "To ma tež spodźiwne.\n",
            "W kašćiku leža moje rukajcy.\n",
            "W koridorje steja spjelnjeja ze wšěch rěči, kamor kamor je.\n",
            "Na wšěch wisa njetrjeba.\n",
            "Na wšěch wisa njetrjeba.\n",
            "W kamorje steja črije a domjace črije.\n",
            "W witachu steji započeć?\n",
            "Wočiń započeć?\n",
            "Začiń započeć?\n",
            "To je dołha stawizna.\n",
            "W mučny.\n",
            "W mučny.\n",
            "Na myjadle leži studenća\n",
            "Na myjadle leži studenća zhubili,\n",
            "Pódla spjelnjeja wisaja a studenća wučerstwa\n",
            "We wobłuku je ćopła woda.\n",
            "My so rady zhubili, a studenća hromadźa\n",
            "spjelnjeja mašina steji tež w kupjeli.\n",
            "To je naša kuchnja.\n",
            "W porjadku.\n",
            "Na dźěle serbskeho krala.\n",
            "Mać w kuchni snědać, zo so wječer přihotuje.\n",
            "Mać spjelnjeja kóždy wokomik rěči, wari a pječe.\n",
            "To je bydlenska\n",
            "W prěnjej stwě steja ptačokwasny konopej a sydła, blido a skónčić?\n",
            "Na kamorje steji naš započeć?\n",
            "My rady njedam.\n",
            "Wječor pěskowčika.\n",
            "W kamorje su knihi, wideja a cejdejki.\n",
            "Radijo je w rěči,\n",
            "Mój bratr sedźi w sydle nalistuje a poska hudźbu.\n",
            "Na sćěnje wisaja časnik a wobrazy strukturuja.\n",
            "Pod woknom je bratrowska\n",
            "My nimamy ale w róžku steji započeć?\n",
            "Na blidźe leži spočatnje a steji waza z toho nastatu škodu.\n",
            "Tu je była stwa.\n",
            "Mać dźěła w dźěłanskej přitulnej stwičce,\n",
            "Tu steji naš krala.\n",
            "Mać pisa ze wšeje a ja hraju na kompjuteru hry.\n",
            "Poněčim steji na pisanskim blidźe.\n",
            "W dźěłanskej stwě je tež tak.\n",
            "W kamorje su wobrazowe knihi, Braniborskeje.\n",
            "Tam je była stwa.\n",
            "spjelnjeja tajki kamor a studenća su w lěhalni stwě.\n",
            "W łožu leži studenća\n",
            "Na łožu suche su studenća a poslešćo.\n",
            "To je moja dźěćaca stwa.\n",
            "W dźěćacej stwě dźěći sej hrajkaja, domjace nadawki a spja.\n",
            "W regalu leža žiwidła być.\n",
            "Na špundowanju steja awta a studenća hromadźa\n",
            "Na dźěle steji studenća wobsaha njedam.\n",
            "To nimam chwile.\n",
            "We stwě steji wulka zhašeja.\n",
            "To steji na małej kamorje.\n",
            "Faluja hišće započeć?\n",
            "Pod třěchu je załožił\n",
            "Na wšěm zdaću je započeć?\n",
            "W pincy je była rum.\n",
            "Tež sanje steja w pincy.\n",
            "Na polcy steja škleńcy z hosćom a marmeladu.\n",
            "Awto stajimy do garaže.\n",
            "Naš dwór je mały.\n",
            "Naša zahrodka je wulka.\n",
            "W zahrodźe mamy domčk, ja měnju.\n",
            "To je naša pěstowarnja.\n",
            "Dobre ranje, kak ty rěkaš?\n",
            "Rěkam Dušana.\n",
            "Kak wam ty rěkaš?\n",
            "Ja sym knjeni krala.\n",
            "Dobre ranje, Jan a studenća hromadźa\n",
            "Dobre ranje, Tereza a krala.\n",
            "Dobre ranje, knjeni krala.\n",
            "Dobre ranje, ćeta krala.\n",
            "Witaj do dźěći!\n",
            "To sy ty zaso započeć?\n",
            "Sy ty wčera pře zašmjatane a poprawom njejsu.\n",
            "Halo, ja je.\n",
            "Hladaj, mam nowe črije.\n",
            "Pokaž raz, maš nowe ja we wšěm zdaću nichtó wjedu.\n",
            "Štó je tebje dźensa sobu do celow zawrjeny?\n",
            "Štó dźensa po tebje přińdźe?\n",
            "Sy dźensa nochcu dźěćo?\n",
            "Dźi do započeć?\n",
            "Dźiće do započeć?\n",
            "Nastawki spjelnjeja kóždy wokomik a krala.\n",
            "spjelnjeja dźěle a změna do swojeho krala.\n",
            "spjelnjeja to do swojeho krala.\n",
            "Sleč so instalowali.\n",
            "Powěsń tak.\n",
            "Powěsń rěče.\n",
            "Spjelń so přewjedźe.\n",
            "Rozrisajće so přewjedźe.\n",
            "Zuj so črije, dołho a škórnje!\n",
            "Staj črije porjadnje do zahrody.\n",
            "Stajće to!\n",
            "Tykń sej myslu.\n",
            "Tykńće so myslu.\n",
            "Wzmi so swoju prěnju sobu!\n",
            "Wzmiće sej swoju prěnju sobu!\n",
            "Dźi do swojeje zastojnstwa\n",
            "Dźiće do swojeje wólby.\n",
            "Dźi z kóždy wokomik a započa so do zastojnstwa su.\n",
            "Wzmi Hanku sobu do krala.\n",
            "Přińdź zaso wróćo z kupjele a ze zahrody.\n",
            "Wočinjej tu.\n",
            "Wočińće durje!\n",
            "Začiń durje přepytuju.\n",
            "Začińće durje mjelčo!\n",
            "Dźi pomału!\n",
            "Přińdź sobu!\n",
            "Tu je naša skupinska stwa.\n",
            "To je naša skupinska stwa.\n",
            "Našej skupinskej stwě rěka spjelnjeja je.\n",
            "Na špundowanju leži studenća\n",
            "W róžku steja studenća domčk, studenća wučerstwa a krala.\n",
            "Mamy rjane hrajka.\n",
            "Zady steji klankodźiwadło.\n",
            "W regalu su burski statok, rěče, studenća wučerstwa a wobrazowe krala.\n",
            "Na kamorje sydaja klanki, a plyšowe skoćatka.\n",
            "Prědku w rěči, što?\n",
            "To abo dźěćo to měniš?\n",
            "Přińdź nutř!\n",
            "Přińdźće nutř!\n",
            "Ja sym dźensa započeć?\n",
            "Ja steju prědku!\n",
            "Ja steju kwětka.\n",
            "Budź započeć?\n",
            "Sydń so na stólc, na přestrjenc a na Hižo dźiwać.\n",
            "Što započeć?\n",
            "Z čim chceš so kóždy wokomik\n",
            "Z čim chceće so kóždy wokomik\n",
            "Wzmi so to ze znjamjeńša a ze započa so to ze spěwom\n",
            "Wzmiće sej to!\n",
            "Hladaj, zo ći hrajki serbskeho krala.\n",
            "Hladajće, zo wam hrajki započeć?\n",
            "Ty sy to...\n",
            "Ty sy započeć?\n",
            "Ty porjadnje započeć?\n",
            "Ty pak rjenje započeć?\n",
            "Ty sy započeć?\n",
            "Ty sy hrajki porjadnje knyskota.\n",
            "Naše zaběry w skupinskej stwě.\n",
            "Jurij započeć?\n",
            "To nochcu ze serbskeho krala.\n",
            "To skoro njedam.\n",
            "Zrumujće započeć?\n",
            "To njepřińdźe\n",
            "Ja chcu hrajkać z klanku a krala.\n",
            "Radšo ja nochcu zdźeržu.\n",
            "Moja njedosahnu.\n",
            "Ja ju hladam\n",
            "To nochcu do celow zawrjeny?\n",
            "To ja tebje njedam.\n",
            "To skoro krala.\n",
            "Klanka je chora.\n",
            "Moja klanka\n",
            "Čehodla ty płakaš?\n",
            "Boli tebje započeć?\n",
            "Ow, ty maš kašel a studenća wučerstwa\n",
            "Dyrbju z tobu k lěkarjej hić.\n",
            "To nochcu ze zahrody.\n",
            "Dobry dźeń, moja Milenka je chora.\n",
            "Dyrbju twoju přepytować.\n",
            "Twoja je so krala.\n",
            "Sotra budźe skónčić?\n",
            "To tajki lěkar pomhać.\n",
            "Dam najprjedy była kóždy wokomik a studenća njewotpowěduje.\n",
            "směrje nochcu do riće kałnje.\n",
            "To so tak!\n",
            "Nastawki za tři dny zaso!\n",
            "Bórze budźe twoja klanka\n",
            "Z awtom a klockami hrajkać.\n",
            "Ja ze serbskeho krala.\n",
            "Beno je mój přećel.\n",
            "Moje awto wjedźe kóždy wokomik na spjelnjeja je.\n",
            "Beno twari wysoki rady.\n",
            "Stajam dźěle na pokoj!\n",
            "Awto nima žadyn bencin wjace.\n",
            "Jědu z awtom k tankowni.\n",
            "Ow, bencin je puće!\n",
            "Nastawki swoje nakładne awto do garaže.\n",
            "Beno, wosobje do krala.\n",
            "Stajće jedyn zhubili, na wšěch mučny.\n",
            "Moje dźěle su štyriróžkate a studenća hromadźa\n",
            "ja z direktnym poradźiło njeje.\n",
            "Kajke burski statok wopytali.\n",
            "Za burski statok trjebamy dom, styki swinjacu to doprědka ćěrili.\n",
            "W hródźi su kruwy a studenća wowcy wowcy a bratrowska\n",
            "Kruwy mjedwjedźa.\n",
            "W mučny.\n",
            "W wjesce hródźe su kokoše a započa so ze znjamjeńša toho su.\n",
            "W wosobje su była a konje.\n",
            "myslu.\n",
            "W bróžni leži syno a studenća wučerstwa\n",
            "Přinjes kašćik ze zahrody.\n",
            "njedam.\n",
            "Kotre zwěrjata hižo škaber\n",
            "To tak měniš?\n",
            "mučny.\n",
            "Kotre zwěrjata hišće znaješ?\n",
            "Mam zwěrjata jara rady.\n",
            "Moja wowka ma kokoše a studenća wučerstwa\n",
            "Ja pola rěči, kokoše a studenća wučerstwa\n",
            "mjedwjedźa.\n",
            "Štó ze serbskeho krala.\n",
            "Zady bróžnje bě pastwa.\n",
            "Što zwěrjata na pastwje žiwe być.\n",
            "Na dworje su kački a skónčić?\n",
            "Burik jědźe z tohole po dworje.\n",
            "Burowka započeć?\n",
            "Hdźe schowa so kóčka z znamješkow zestaja.\n",
            "Tu je pjekar\n",
            "Psyk leži před znjamjeńša krala.\n",
            "Je burski statok njepodpěruje.\n",
            "Dźěći,\n",
            "Na žadyn pad!\n",
            "To je statok wostanje stejo.\n",
            "Chcemy hrać njetrjeba.\n",
            "Zahrajmy sej raz započeć?\n",
            "Štó hraje sobu?\n",
            "Hdźe su móžno!\n",
            "Štó rozdźěla\n",
            "Mam hižo škaber\n",
            "Ach, njejsym so rady njejsym.\n",
            "Hura, ja sym započeć?\n",
            "Ja sym započeć?\n",
            "žana karty do započeć?\n",
            "Chcemy hrajkać z skónčić?\n",
            "Beata je předawarka a Tereza nakupuje.\n",
            "Dobry dźeń, njepodpěruje.\n",
            "Što ty chceš činić!\n",
            "Daj mi prošu jednu w kachlach.\n",
            "Trjebaš hišće něšto?\n",
            "Trjebam hišće mloko, marmeladu a studenća wučerstwa\n",
            "Mój korbik je połny.\n",
            "Hdźe je moja započeć?\n",
            "Kak wjele mam móžno!\n",
            "Ty dyrbiš dźesać eurow a pjeć zahrody.\n",
            "Na započeć?\n",
            "Přińdźće ke mni!\n",
            "Sym Jank, a tu je mój salon.\n",
            "Što sebi přeješ?\n",
            "Prošu ja mjeno a tykń mjeno někotre mjeno je.\n",
            "Aw, to měniš?\n",
            "Njebudź tak je.\n",
            "dźěle so ći twoja nowa započa\n",
            "Haj, haj, sym započeć?\n",
            "Prošu jara, nětko tež hišće njepodpěruje.\n",
            "Hdźe su moje přikaznje hladam a studenća wučerstwa\n",
            "A što mam so ze zbytkom\n",
            "kóždy mjeno prošu hubje a krala.\n",
            "To ja mjeno prošu jara.\n",
            "Sym hotowy.\n",
            "Sy spokojom?\n",
            "Haj, sym spokojom.\n",
            "Sym jara spodźiwne.\n",
            "A što mam započeć?\n",
            "To płaći pjeć eurow.\n",
            "Hdźe je započeć?\n",
            "Ja tebje była wěsta kožuch ma.\n",
            "znamješko mi prošu jara.\n",
            "Pohladaj do zahrody.\n",
            "Smy w kruhu.\n",
            "To přiwjedź mi knižku z krala.\n",
            "Ow, wy rjenje ćicho\n",
            "To chce wam zhubili,\n",
            "Kak rěkaše započa so w kuchni.\n",
            "spjelnjeja sej wobrazki w knize.\n",
            "Što widźiš na pokoj!\n",
            "Što widźiće na pokoj!\n",
            "To konik mje!\n",
            "To by knihu zaso do zastojnstwa\n",
            "Jedyn za druhim mjeno spodźiwnje.\n",
            "Štó znaje kóždy čas hra?\n",
            "Skedźbniće tołsty kožuch.\n",
            "Rěč sobu!\n",
            "Rěčće sobu!\n",
            "Praj to ty!\n",
            "To sy rjenje činił.\n",
            "Dźensa dźěle nowy spěw.\n",
            "Hej, spěw mam na mjedwjedźa.\n",
            "Znajeće tón mjedwjedźa.\n",
            "To je lóštny spěw.\n",
            "Hdźe je moja započeć?\n",
            "Ja wam spěw dźěl poskajće na mnje!\n",
            "To sobu!\n",
            "To skoro njedam.\n",
            "To nochcu spać.\n",
            "Nětko my započeć?\n",
            "To hižo derje klinči.\n",
            "Spěwamy hišće raz a studenća sobu.\n",
            "spjelnjeja so do kruha a započa so za wšěch zhubili, žedźeli\n",
            "Činimy jednu kročel naprawo a jednu linku.\n",
            "To so na mjedwjedźa.\n",
            "Pušćće\n",
            "Ow, Feliks je padnył!\n",
            "To by była jemu njedam.\n",
            "Njedawa bolosće ći něšto?\n",
            "Ně, mjeno ničo njeboli.\n",
            "Dźiće po swój započeć?\n",
            "Hdźe je mój započeć?\n",
            "Pytaj swój započeć?\n",
            "Sym swój regiment namakał.\n",
            "Tykń swój krala.\n",
            "Chcemy započeć?\n",
            "Začińće woči!\n",
            "Słyšiće zwónčk?\n",
            "Předstajće sej, kak by so kóždy na łuce zjewi.\n",
            "Hru wětřik kwětki a studenća wučerstwa\n",
            "To wětřik tež krala.\n",
            "Nastawki ptački njedam.\n",
            "Smy při njemu a studenća hromadźa\n",
            "Trjebamy ze serbskeho krala.\n",
            "Mam dobreje nadźije, zo bych studenća wučerstwa a krala.\n",
            "Nětko psalimy serbsku skónčić?\n",
            "Dźiće po wšěm dźěle a połožće jón na swoje městno.\n",
            "Woblečće sej myslu.\n",
            "Paslimy serbsku započeć?\n",
            "To wam łopjeno.\n",
            "Što widźiš na pokoj!\n",
            "Cyle prawje, widźiš tři smužki.\n",
            "Kotru barbu maja te wuměnjenje njespjelnja.\n",
            "Serbska chorhoj je była čerwjena a běła.\n",
            "Najprjedy trjebamy Tveiranowy módry a studenća wučerstwa\n",
            "Nastawki tu hornju linku mjedwjedźa.\n",
            "Magiska přez započeć?\n",
            "Dźerž tón pisak\n",
            "Wzmi pisak do praweje abo lěweje wuzwolena.\n",
            "Tule čerwjeny t-shirt.\n",
            "Ale Beno, chceš ty trawu molować?\n",
            "Tež ty sy doma.\n",
            "Tež ty sy jow.\n",
            "Hižo ty zaso.\n",
            "Hižo sy ty jow.\n",
            "Při tym wona dźěła.\n",
            "Při tym wona mysli.\n",
            "Wona je jow.\n",
            "Wona je doma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:14.128164Z",
          "iopub.execute_input": "2022-07-24T18:06:14.128710Z",
          "iopub.status.idle": "2022-07-24T18:06:15.443914Z",
          "shell.execute_reply.started": "2022-07-24T18:06:14.128640Z",
          "shell.execute_reply": "2022-07-24T18:06:15.442820Z"
        },
        "trusted": true,
        "id": "bxoLU0pqdwcz",
        "outputId": "e640b919-e902-40cf-879f-ec636a73ed4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdrive\u001b[0m/    \u001b[01;34mgdrive\u001b[0m/       target.txt                \u001b[01;34mwandb\u001b[0m/\n",
            "\u001b[01;34mdata-bin\u001b[0m/     \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  test_dsb_hsb.dsb_src.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:08:11.485595Z",
          "iopub.execute_input": "2022-07-24T18:08:11.486787Z",
          "iopub.status.idle": "2022-07-24T18:08:12.475192Z",
          "shell.execute_reply.started": "2022-07-24T18:08:11.486730Z",
          "shell.execute_reply": "2022-07-24T18:08:12.474153Z"
        },
        "trusted": true,
        "id": "NnEBR0Xhdwc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb -l dsb-hsb -m bleu chrf ter)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:16.590743Z",
          "iopub.execute_input": "2022-07-24T18:06:16.591679Z",
          "iopub.status.idle": "2022-07-24T18:06:19.209461Z",
          "shell.execute_reply.started": "2022-07-24T18:06:16.591625Z",
          "shell.execute_reply": "2022-07-24T18:06:19.208405Z"
        },
        "trusted": true,
        "id": "bqOJKYxXdwc2",
        "outputId": "92000a21-471d-465c-bf07-4f32e12a249b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 26.9,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\",\n",
            " \"verbose_score\": \"54.2/32.8/21.0/14.1 (BP = 1.000 ratio = 1.103 hyp_len = 5606 ref_len = 5082)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"chrF2\",\n",
            " \"score\": 47.2,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"yes\",\n",
            " \"nc\": \"6\",\n",
            " \"nw\": \"0\",\n",
            " \"space\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"TER\",\n",
            " \"score\": 57.1,\n",
            " \"signature\": \"nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"lc\",\n",
            " \"tok\": \"tercom\",\n",
            " \"norm\": \"no\",\n",
            " \"punct\": \"yes\",\n",
            " \"asian\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "}\n",
            "]\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading blind test data\n",
        "!gdown --id 1JG4suwkLYt0b3E-jLqqQAxHlMU87YKeh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD0Zhyg5Mmtd",
        "outputId": "839f7c02-72fb-4844-8ba5-aaf12b5dd7d5"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JG4suwkLYt0b3E-jLqqQAxHlMU87YKeh\n",
            "To: /content/test_dsb_hsb.dsb_src.txt\n",
            "100% 87.7k/87.7k [00:00<00:00, 62.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KaC4S2-T8VU",
        "outputId": "6026ffbb-4461-4977-c149-5d7a7c680961"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/test_dsb_hsb.dsb_src.txt --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang dsb --target-lang hsb \\\n",
        "      --beam 5 data-bin/wmt_dsb_hsb | grep -P \"D-[0-9]+\" | cut -f3 > /content/gdrive/MyDrive/target_dsb-hsb_sup_submission.txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr0yH7NZUF1B",
        "outputId": "332bc44e-c915-4224-b984-8bfbaf922af7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 08:22:30 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/test_dsb_hsb.dsb_src.txt'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_dsb_hsb', 'source_lang': 'dsb', 'target_lang': 'hsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 08:22:30 | INFO | fairseq.tasks.translation | [dsb] dictionary: 104024 types\n",
            "2022-08-28 08:22:30 | INFO | fairseq.tasks.translation | [hsb] dictionary: 103512 types\n",
            "2022-08-28 08:22:30 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-28 08:22:50 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-28 08:22:50 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-28 08:22:50 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-28 08:23:01 | INFO | fairseq_cli.interactive | Total time: 31.308 seconds; translation time: 9.033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "id": "9Zw2XTYgbzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu /content/gdrive/MyDrive/target_dsb-hsb_sup_submission.txt -i /content/test_dsb_hsb.dsb_src.txt -l dsb-hsb -m bleu chrf ter)"
      ],
      "metadata": {
        "id": "7LNPRlDQb4ER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}