{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "de_hsb_transformers_supervised_final_11am.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# create a seperate folder to store everything\n",
        "# !mkdir wmt\n",
        "# %cd wmt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:45.876120Z",
          "iopub.execute_input": "2022-07-24T19:01:45.876582Z",
          "iopub.status.idle": "2022-07-24T19:01:46.194317Z",
          "shell.execute_reply.started": "2022-07-24T19:01:45.876499Z",
          "shell.execute_reply": "2022-07-24T19:01:46.192944Z"
        },
        "trusted": true,
        "id": "J9o_gWJIdwcc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BFmSadnVoSa",
        "outputId": "03380282-8d30-402a-cd9b-116d8f51e277"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tree\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:47.842831Z",
          "iopub.execute_input": "2022-07-24T19:01:47.843475Z",
          "iopub.status.idle": "2022-07-24T19:03:13.957220Z",
          "shell.execute_reply.started": "2022-07-24T19:01:47.843443Z",
          "shell.execute_reply": "2022-07-24T19:03:13.955881Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sULkb42Jdwci",
        "outputId": "28fee1e2-9f71-44b4-ded7-84616eef5041"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 1s (50.4 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 28.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 58.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=440c591099854ea98b51f67c8f7a391f2bcf275409da1e69a2ed5b0215079462\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, portalocker, morfessor, colorama, tensorboardX, sacremoses, sacrebleu, mock, indic-nlp-library\n",
            "Successfully installed colorama-0.4.5 indic-nlp-library-0.81 mock-4.0.3 morfessor-2.0.6 portalocker-2.5.1 sacrebleu-2.2.0 sacremoses-0.0.53 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0 tensorboardX-2.5.1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 32239, done.\u001b[K\n",
            "remote: Total 32239 (delta 0), reused 0 (delta 0), pack-reused 32239\u001b[K\n",
            "Receiving objects: 100% (32239/32239), 22.42 MiB | 16.65 MiB/s, done.\n",
            "Resolving deltas: 100% (23642/23642), done.\n",
            "/content/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.32)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 35.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.1+cu113)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2.2.0)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 72.5 MB/s \n",
            "\u001b[?25hCollecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.9.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 57.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.5)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.8.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=dfb8352acc5e88211bc17f004a41f0054950a838029ee87660edebd805b5e70f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 fairseq hydra-core-1.0.7 omegaconf-2.0.6\n",
            "/content\n",
            "/env/python\n",
            "/env/python:/content/fairseq/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:13.960095Z",
          "iopub.execute_input": "2022-07-24T19:03:13.960531Z",
          "iopub.status.idle": "2022-07-24T19:03:14.252882Z",
          "shell.execute_reply.started": "2022-07-24T19:03:13.960500Z",
          "shell.execute_reply": "2022-07-24T19:03:14.251592Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XleJnPzidwck",
        "outputId": "2a948a35-dc6d-4245-cb8b-015d2b80b85c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U --no-cache-dir gdown --pre"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:14.254120Z",
          "iopub.execute_input": "2022-07-24T19:03:14.254498Z",
          "iopub.status.idle": "2022-07-24T19:03:36.192512Z",
          "shell.execute_reply.started": "2022-07-24T19:03:14.254462Z",
          "shell.execute_reply": "2022-07-24T19:03:36.191399Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIuDbtOgdwcm",
        "outputId": "5cd04cb4-4ae0-41ca-b88c-f6d23ea07a3b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=f09e574b2714c74547da2a1abe5427233f125081ef97f678b9d8df8a31e0db96\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-8035ld2t/wheels/3d/ec/b0/a96d1d126183f98570a785e6bf8789fca559853a9260e928e1\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNjo_Q8_IlXz",
        "outputId": "3661d59b-acbc-40dd-a59d-55c2f37815eb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir drive\n",
        "%cd drive"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.194917Z",
          "iopub.execute_input": "2022-07-24T19:03:36.195189Z",
          "iopub.status.idle": "2022-07-24T19:03:36.494103Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.195162Z",
          "shell.execute_reply": "2022-07-24T19:03:36.492910Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AopzC4DBdwcn",
        "outputId": "cd9afbfc-9768-40e4-fc61-af3b9c02922c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 19UHtgPcAX-Qe5axVxmBQAutsssovCfs4 --folder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.495274Z",
          "iopub.execute_input": "2022-07-24T19:03:36.495519Z",
          "iopub.status.idle": "2022-07-24T19:03:47.662284Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.495496Z",
          "shell.execute_reply": "2022-07-24T19:03:47.661258Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa1K1LT9dwcn",
        "outputId": "702d4bd7-08dc-4f94-d40a-89c68c4f0613"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Retrieving folder list\n",
            "Retrieving folder 1QmE3jZSejMy_HqdHEqS5IfviAhXtmPZa 2020_train_hsb-de\n",
            "Processing file 15eZFhy5rbstwhWmrVCqYPCwiWmxPg6iK 2020_train_hsb-de.de\n",
            "Processing file 19CESCme_ZpyfT9WgUF8GcT5EhkAXhIq5 2020_train_hsb-de.hsb\n",
            "Retrieving folder 1aK3ovMgA4dBB_1RGJULQlHy_x9MJF0dO 2021_train_hsb-de\n",
            "Processing file 1czOC4vGy1ZyXOc_5uJY5twES2JyIIWV4 2021_train_hsb-de.de\n",
            "Processing file 19gU_eTd_SkpwDxX-pnvR_wUvaHAdd40o 2021_train_hsb-de.hsb\n",
            "Retrieving folder 1wEa03dYM4X0BB1wVYOlU3Il9pJHZEr8j 2022_train_hsb-de\n",
            "Processing file 1_jN89Xh0zX4a00_dzuSYxio9FAhSKNkH 2022_train_hsb-de.tsv\n",
            "Retrieving folder 1h1qZwHdc2tLo58H9TkXh9XPXKAi6vVns monolingual\n",
            "Processing file 1dRkADoRBI1o_nRlyZajjc_5CQBWwWSco de.tok\n",
            "Processing file 19LS6bKVZd9lPOeDtGQsMjVc7h6j9Cfxk HSB_monolingual.txt\n",
            "Processing file 1EziUIMT-DWI70SjRGqvH5pIQRUbSwg1G sorbian_institute_monolingual.hsb\n",
            "Processing file 12INmfPgh4h5gknEI1lFpywIof3W0BpPM web_monolingual.hsb\n",
            "Processing file 157UsiqIWKcbwqZcAPgz4Hv9DBY_WYJsP witaj_monolingual.hsb\n",
            "Retrieving folder 16ctwnqWbpnQWfU9xwcN24_7nyuGDKBug valid_hsb-de\n",
            "Processing file 1wvaCRisGwxtdia6UycAs6vmtTryqUnM_ 2020_devel_hsb-de.de\n",
            "Processing file 1HSvlBw2N52qJPhklsC1WEMXB66rYFgmX 2020_devel_hsb-de.hsb\n",
            "Processing file 1FKhss4pZ45fWIsXVnrvr9BiSPY78aQjh 2020_devel_test_hsb-de.de\n",
            "Processing file 1Z9OoFqsb45ZmBBFDYlVYyxIVfCm3pMsF 2020_devel_test_hsb-de.hsb\n",
            "Processing file 1-mL4dwB3-xQUU1gTs-IZ6gbMRTQqyFdo 2022_valid_hsb-de.tsv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15eZFhy5rbstwhWmrVCqYPCwiWmxPg6iK\n",
            "To: /content/drive/hsb-de/2020_train_hsb-de/2020_train_hsb-de.de\n",
            "100% 5.14M/5.14M [00:00<00:00, 126MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CESCme_ZpyfT9WgUF8GcT5EhkAXhIq5\n",
            "To: /content/drive/hsb-de/2020_train_hsb-de/2020_train_hsb-de.hsb\n",
            "100% 4.71M/4.71M [00:00<00:00, 148MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1czOC4vGy1ZyXOc_5uJY5twES2JyIIWV4\n",
            "To: /content/drive/hsb-de/2021_train_hsb-de/2021_train_hsb-de.de\n",
            "100% 8.62M/8.62M [00:00<00:00, 141MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19gU_eTd_SkpwDxX-pnvR_wUvaHAdd40o\n",
            "To: /content/drive/hsb-de/2021_train_hsb-de/2021_train_hsb-de.hsb\n",
            "100% 7.93M/7.93M [00:00<00:00, 54.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_jN89Xh0zX4a00_dzuSYxio9FAhSKNkH\n",
            "To: /content/drive/hsb-de/2022_train_hsb-de/2022_train_hsb-de.tsv\n",
            "100% 53.2M/53.2M [00:00<00:00, 144MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dRkADoRBI1o_nRlyZajjc_5CQBWwWSco\n",
            "To: /content/drive/hsb-de/monolingual/de.tok\n",
            "100% 7.37M/7.37M [00:00<00:00, 125MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19LS6bKVZd9lPOeDtGQsMjVc7h6j9Cfxk\n",
            "To: /content/drive/hsb-de/monolingual/HSB_monolingual.txt\n",
            "100% 41.9M/41.9M [00:00<00:00, 161MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EziUIMT-DWI70SjRGqvH5pIQRUbSwg1G\n",
            "To: /content/drive/hsb-de/monolingual/sorbian_institute_monolingual.hsb\n",
            "100% 37.3M/37.3M [00:00<00:00, 59.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12INmfPgh4h5gknEI1lFpywIof3W0BpPM\n",
            "To: /content/drive/hsb-de/monolingual/web_monolingual.hsb\n",
            "100% 11.7M/11.7M [00:00<00:00, 22.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=157UsiqIWKcbwqZcAPgz4Hv9DBY_WYJsP\n",
            "To: /content/drive/hsb-de/monolingual/witaj_monolingual.hsb\n",
            "100% 19.4M/19.4M [00:00<00:00, 31.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wvaCRisGwxtdia6UycAs6vmtTryqUnM_\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_hsb-de.de\n",
            "100% 174k/174k [00:00<00:00, 118MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HSvlBw2N52qJPhklsC1WEMXB66rYFgmX\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_hsb-de.hsb\n",
            "100% 160k/160k [00:00<00:00, 81.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FKhss4pZ45fWIsXVnrvr9BiSPY78aQjh\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_test_hsb-de.de\n",
            "100% 178k/178k [00:00<00:00, 105MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Z9OoFqsb45ZmBBFDYlVYyxIVfCm3pMsF\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_test_hsb-de.hsb\n",
            "100% 164k/164k [00:00<00:00, 72.5MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-mL4dwB3-xQUU1gTs-IZ6gbMRTQqyFdo\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2022_valid_hsb-de.tsv\n",
            "100% 344k/344k [00:00<00:00, 114MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUdiQyJdJBdt",
        "outputId": "ef4a6f0e-c1c0-44fe-9f6d-ec20956c3cc6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:47.663751Z",
          "iopub.execute_input": "2022-07-24T19:03:47.664065Z",
          "iopub.status.idle": "2022-07-24T19:03:47.671365Z",
          "shell.execute_reply.started": "2022-07-24T19:03:47.664036Z",
          "shell.execute_reply": "2022-07-24T19:03:47.670044Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXt8nnHHdwco",
        "outputId": "5ff4d877-f0f6-4b6b-d506-3c014a96ca4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H751-LW3XII0",
        "outputId": "675142fc-a4e1-49bf-dc21-1067bff27f56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/hsb-de/2022_train_hsb-de/2022_train_hsb-de.tsv', delimiter='\\t')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XtaWvwKcW6ja",
        "outputId": "69a27eb7-0358-4f6e-8368-40e7c432ec99"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 hsb  \\\n",
              "0  Nano, ja će prošu, jónu přidać, zo móžeš tež t...   \n",
              "1  Kotre kompozitoriske srědki zasadźi Schumann k...   \n",
              "2  Komunistiskim mocam wizita kardinala Wojtyły z...   \n",
              "3  Tohodla chcemy za tute wodźizny krućiše naroki...   \n",
              "4  Chcu so cyle skrótka na podawk dźensa popołdnj...   \n",
              "\n",
              "                                                  de  \n",
              "0  Papa, ich bitte dich, einmal einzuräumen, dass...  \n",
              "1  Welche kompositorischen Mittel setzt Schumann ...  \n",
              "2  Den kommunistischen Mächten war die Visite des...  \n",
              "3  Deshalb wollen wir für diese Gewässer strenger...  \n",
              "4  Ich möchte mich ganz kurz auf einen Vorgang he...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61a546d8-fc62-4133-bad9-3424642d4c54\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hsb</th>\n",
              "      <th>de</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nano, ja će prošu, jónu přidać, zo móžeš tež t...</td>\n",
              "      <td>Papa, ich bitte dich, einmal einzuräumen, dass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kotre kompozitoriske srědki zasadźi Schumann k...</td>\n",
              "      <td>Welche kompositorischen Mittel setzt Schumann ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Komunistiskim mocam wizita kardinala Wojtyły z...</td>\n",
              "      <td>Den kommunistischen Mächten war die Visite des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tohodla chcemy za tute wodźizny krućiše naroki...</td>\n",
              "      <td>Deshalb wollen wir für diese Gewässer strenger...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chcu so cyle skrótka na podawk dźensa popołdnj...</td>\n",
              "      <td>Ich möchte mich ganz kurz auf einen Vorgang he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61a546d8-fc62-4133-bad9-3424642d4c54')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61a546d8-fc62-4133-bad9-3424642d4c54 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61a546d8-fc62-4133-bad9-3424642d4c54');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_hsb-de.hsb', 'w') as f:\n",
        "  for i in range(70000):\n",
        "    f.write(df['hsb'][i]+\"\\n\")\n",
        "with open('train_hsb-de.de', 'w') as f:\n",
        "  for i in range(70000):\n",
        "    f.write(df['de'][i]+\"\\n\")"
      ],
      "metadata": {
        "id": "Y7WeR9pIW_ZJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = pd.read_csv('/content/drive/hsb-de/valid_hsb-de/2022_valid_hsb-de.tsv', delimiter='\\t')\n",
        "df_valid.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LgMXiehCW_58",
        "outputId": "6584b047-b700-4bb4-8813-bb126779cfd9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 hsb  \\\n",
              "0  A z kozu abo kruwu hodźeše so mloko a twaroh p...   \n",
              "1  Jednu družinu zorna picujemy skotej, z druhich...   \n",
              "2  W nim zaběra bamž mjez druhim k mnoho diskutow...   \n",
              "3  Štó, hdyž nic křesćanske institucije bychu tut...   \n",
              "4  Sym kruće přeswědčeny, zo wobsedźi Zjednoćene ...   \n",
              "\n",
              "                                                  de  \n",
              "0  Und mit Ziege oder Kuh ließen sich auch Milch ...  \n",
              "1  Die einen Körner verfüttern wir dem Viehzeug, ...  \n",
              "2  In ihm nimmt der Papst unter anderem zu einem ...  \n",
              "3  Wer, wenn nicht christliche Institutionen würd...  \n",
              "4  Ich bin der festen Überzeugung, dass das Verei...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-fbce2bd4-f00a-4779-b9e6-64a0e7218ba0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hsb</th>\n",
              "      <th>de</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A z kozu abo kruwu hodźeše so mloko a twaroh p...</td>\n",
              "      <td>Und mit Ziege oder Kuh ließen sich auch Milch ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jednu družinu zorna picujemy skotej, z druhich...</td>\n",
              "      <td>Die einen Körner verfüttern wir dem Viehzeug, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>W nim zaběra bamž mjez druhim k mnoho diskutow...</td>\n",
              "      <td>In ihm nimmt der Papst unter anderem zu einem ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Štó, hdyž nic křesćanske institucije bychu tut...</td>\n",
              "      <td>Wer, wenn nicht christliche Institutionen würd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sym kruće přeswědčeny, zo wobsedźi Zjednoćene ...</td>\n",
              "      <td>Ich bin der festen Überzeugung, dass das Verei...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fbce2bd4-f00a-4779-b9e6-64a0e7218ba0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-fbce2bd4-f00a-4779-b9e6-64a0e7218ba0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-fbce2bd4-f00a-4779-b9e6-64a0e7218ba0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('valid_hsb-de.hsb', 'w') as f:\n",
        "  for i in range(len(df_valid)):\n",
        "    f.write(df_valid['hsb'][i]+\"\\n\")\n",
        "with open('valid_hsb-de.de', 'w') as f:\n",
        "  for i in range(len(df_valid)):\n",
        "    f.write(df_valid['de'][i]+\"\\n\")"
      ],
      "metadata": {
        "id": "t0FxklB_XEUj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:04:11.972219Z",
          "iopub.execute_input": "2022-07-24T19:04:11.972913Z",
          "iopub.status.idle": "2022-07-24T19:04:12.256665Z",
          "shell.execute_reply.started": "2022-07-24T19:04:11.972888Z",
          "shell.execute_reply": "2022-07-24T19:04:12.255254Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ihwjB6Gdwcp",
        "outputId": "42d399bd-86ff-4233-be2e-9f384c5d7318"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mhsb-de\u001b[0m/  train_hsb-de.de  train_hsb-de.hsb  valid_hsb-de.de  valid_hsb-de.hsb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8F3ZpFtX6Vo",
        "outputId": "5367cda2-427d-43c2-bab2-985a9b1c3298"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-preprocess \\\n",
        "    --source-lang de --target-lang hsb \\\n",
        "    --trainpref drive/train_hsb-de --validpref drive/valid_hsb-de \\\n",
        "    --destdir data-bin/wmt_de_hsb --thresholdtgt 0 --thresholdsrc 0 \\\n",
        "    --workers 20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:14:48.909980Z",
          "iopub.execute_input": "2022-07-24T19:14:48.910354Z",
          "iopub.status.idle": "2022-07-24T19:15:18.024931Z",
          "shell.execute_reply.started": "2022-07-24T19:14:48.910330Z",
          "shell.execute_reply": "2022-07-24T19:15:18.024044Z"
        },
        "trusted": true,
        "id": "XxPJ-2-zdwcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c582eba1-bb7e-419f-80ec-1ae7d65c8d2f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 06:33:21 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wmt_de_hsb', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='de', srcdict=None, suppress_crashes=False, target_lang='hsb', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='drive/train_hsb-de', use_plasma_view=False, user_dir=None, validpref='drive/valid_hsb-de', wandb_project=None, workers=20)\n",
            "2022-08-29 06:33:32 | INFO | fairseq_cli.preprocess | [de] Dictionary: 105688 types\n",
            "2022-08-29 06:33:42 | INFO | fairseq_cli.preprocess | [de] drive/train_hsb-de.de: 70000 sents, 996299 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-29 06:33:42 | INFO | fairseq_cli.preprocess | [de] Dictionary: 105688 types\n",
            "2022-08-29 06:33:45 | INFO | fairseq_cli.preprocess | [de] drive/valid_hsb-de.de: 2000 sents, 27607 tokens, 6.06% replaced (by <unk>)\n",
            "2022-08-29 06:33:45 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 136232 types\n",
            "2022-08-29 06:33:55 | INFO | fairseq_cli.preprocess | [hsb] drive/train_hsb-de.hsb: 70000 sents, 883083 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-29 06:33:55 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 136232 types\n",
            "2022-08-29 06:33:59 | INFO | fairseq_cli.preprocess | [hsb] drive/valid_hsb-de.hsb: 2000 sents, 24731 tokens, 8.65% replaced (by <unk>)\n",
            "2022-08-29 06:33:59 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt_de_hsb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uol2ncymJWFr",
        "outputId": "a20e4dfe-39a0-43bf-91ff-84c2863409b9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 33.2 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 66.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 63.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 76.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.1 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.2 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 72.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=77e0c42c660a396356c5db74bac493d4a5097efc5e6ead396623bebff83bdc2b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"wmt2022_de-hsb_transformers_supervised_final_11am\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:18.026892Z",
          "iopub.execute_input": "2022-07-24T19:15:18.027225Z",
          "iopub.status.idle": "2022-07-24T19:15:26.733355Z",
          "shell.execute_reply.started": "2022-07-24T19:15:18.027200Z",
          "shell.execute_reply": "2022-07-24T19:15:26.732531Z"
        },
        "trusted": true,
        "id": "hoUXe8Kndwcr",
        "outputId": "aa46a573-2d98-4df1-f05e-7cff2b5d3680",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220829_063522-2e4jcrgs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/rahul2023_usa/wmt2022_de-hsb_transformers_supervised_final_11am/runs/2e4jcrgs\" target=\"_blank\">pretty-pyramid-1</a></strong> to <a href=\"https://wandb.ai/rahul2023_usa/wmt2022_de-hsb_transformers_supervised_final_11am\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rahul2023_usa/wmt2022_de-hsb_transformers_supervised_final_11am/runs/2e4jcrgs?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f1cc881fdd0>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Finetuning the model\n",
        "\n",
        "# # pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
        "\n",
        "\n",
        "# # some notable args:\n",
        "# # --max-update=1000     -> for this example, to demonstrate how to finetune we are only training for 1000 steps. You should increase this when finetuning\n",
        "# # --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# # --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# # --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# # --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\n",
        "# # --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\n",
        "# # --max_tokns           -> this is max tokens per batch\n",
        "\n",
        "\n",
        "# !( fairseq-train data-bin/wmt_dsb_de \\\n",
        "# --max-source-positions=210 \\\n",
        "# --max-target-positions=210 \\\n",
        "# --max-update=1000 \\\n",
        "# --save-interval=1 \\\n",
        "# --arch=transformer \\\n",
        "# --criterion=label_smoothed_cross_entropy \\\n",
        "# --source-lang=dsb \\\n",
        "# --lr-scheduler=inverse_sqrt \\\n",
        "# --target-lang=de \\\n",
        "# --label-smoothing=0.1 \\\n",
        "# --optimizer adam \\\n",
        "# --adam-betas \"(0.9, 0.98)\" \\\n",
        "# --clip-norm 1.0 \\\n",
        "# --warmup-init-lr 1e-07 \\\n",
        "# --warmup-updates 4000 \\\n",
        "# --dropout 0.2 \\\n",
        "# --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "# --save-dir checkpoints/model \\\n",
        "# --keep-last-epochs 5 \\\n",
        "# --patience 5 \\\n",
        "# --skip-invalid-size-inputs-valid-test \\\n",
        "# --fp16 \\\n",
        "# --update-freq=2 \\\n",
        "# --distributed-world-size 1 \\\n",
        "# --max-tokens 1024 \\\n",
        "# --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "# --lr 5e-4 \\\n",
        "# --reset-lr-scheduler \\\n",
        "# --reset-meters \\\n",
        "# --reset-dataloader \\\n",
        "# --reset-optimizer \\\n",
        "# --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w4cGb93Bdwct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!( fairseq-train data-bin/wmt_de_hsb \\\n",
        "    --arch transformer \\\n",
        "    --max-epoch=50 \\\n",
        "    --criterion=label_smoothed_cross_entropy \\\n",
        "    --source-lang=de \\\n",
        "    --lr-scheduler=inverse_sqrt \\\n",
        "    --target-lang=hsb \\\n",
        "    --label-smoothing=0.1 \\\n",
        "    --optimizer adam \\\n",
        "    --adam-betas \"(0.9, 0.98)\" \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --dropout 0.2 \\\n",
        "    --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "    --wandb-project 'wmt2022_de-hsb_transformers_supervised_final_11am' \\\n",
        "    --save-dir checkpoints/model \\\n",
        "    --keep-last-epochs 5 \\\n",
        "    --fp16 \\\n",
        "    --update-freq=2 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --lr 5e-4 \\\n",
        "    --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "    --reset-lr-scheduler \\\n",
        "    --reset-meters \\\n",
        "    --reset-dataloader \\\n",
        "    --reset-optimizer \\\n",
        "    --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:42.672724Z",
          "iopub.execute_input": "2022-07-24T19:15:42.673143Z",
          "iopub.status.idle": "2022-07-24T19:16:00.902554Z",
          "shell.execute_reply.started": "2022-07-24T19:15:42.673112Z",
          "shell.execute_reply": "2022-07-24T19:16:00.901385Z"
        },
        "trusted": true,
        "id": "U2IouMGudwcv",
        "outputId": "f1f3c875-ea30-4990-d8e8-234461eb47e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 06:35:30 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../../../tmp/tensorboard-wandb', 'wandb_project': 'wmt2022_de-hsb_transformers_supervised_final_11am', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/model', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wmt_de_hsb', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=True, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=50, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='checkpoints/model', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='de', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='hsb', task='translation', tensorboard_logdir='../../../tmp/tensorboard-wandb', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='wmt2022_de-hsb_transformers_supervised_final_11am', warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/wmt_de_hsb', 'source_lang': 'de', 'target_lang': 'hsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-29 06:35:30 | INFO | fairseq.tasks.translation | [de] dictionary: 105688 types\n",
            "2022-08-29 06:35:30 | INFO | fairseq.tasks.translation | [hsb] dictionary: 136232 types\n",
            "2022-08-29 06:35:34 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(105688, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(136232, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=136232, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-08-29 06:35:34 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-08-29 06:35:34 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-08-29 06:35:34 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-08-29 06:35:34 | INFO | fairseq_cli.train | num. shared model params: 237,752,320 (num. trained: 237,752,320)\n",
            "2022-08-29 06:35:34 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-08-29 06:35:34 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data-bin/wmt_de_hsb/valid.de-hsb.de\n",
            "2022-08-29 06:35:34 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data-bin/wmt_de_hsb/valid.de-hsb.hsb\n",
            "2022-08-29 06:35:34 | INFO | fairseq.tasks.translation | data-bin/wmt_de_hsb valid de-hsb 2000 examples\n",
            "2022-08-29 06:35:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-29 06:35:39 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-08-29 06:35:39 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-29 06:35:39 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-08-29 06:35:39 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-08-29 06:35:39 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/model/checkpoint_last.pt\n",
            "2022-08-29 06:35:39 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/model/checkpoint_last.pt\n",
            "2022-08-29 06:35:39 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-08-29 06:35:39 | INFO | fairseq.data.data_utils | loaded 70,000 examples from: data-bin/wmt_de_hsb/train.de-hsb.de\n",
            "2022-08-29 06:35:39 | INFO | fairseq.data.data_utils | loaded 70,000 examples from: data-bin/wmt_de_hsb/train.de-hsb.hsb\n",
            "2022-08-29 06:35:39 | INFO | fairseq.tasks.translation | data-bin/wmt_de_hsb train de-hsb 70000 examples\n",
            "2022-08-29 06:35:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 001:   0% 0/130 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul2023_usa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220829_063539-16bn3o9z\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_de-hsb_transformers_supervised_final_11am\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_de-hsb_transformers_supervised_final_11am/runs/16bn3o9z\u001b[0m\n",
            "2022-08-29 06:35:40 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-08-29 06:35:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2022-08-29 06:35:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   1% 1/130 [00:05<12:24,  5.77s/it]2022-08-29 06:35:45 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   4% 5/130 [00:08<02:19,  1.11s/it]2022-08-29 06:35:48 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  78% 102/130 [01:04<00:16,  1.72it/s]2022-08-29 06:36:46 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "epoch 001:  99% 129/130 [01:25<00:00,  1.69it/s, loss=16.287, nll_loss=16.115, ppl=70987.6, wps=11625.2, ups=1.71, wpb=6808, bsz=529.8, num_updates=100, lr=1.25e-05, gnorm=3.093, loss_scale=16, train_wall=63, gb_free=3.4, wall=65]2022-08-29 06:37:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 1/9 [00:00<00:06,  1.27it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 2/9 [00:01<00:04,  1.54it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 3/9 [00:01<00:03,  1.65it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 4/9 [00:02<00:03,  1.67it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 5/9 [00:03<00:02,  1.66it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 6/9 [00:03<00:01,  1.69it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 7/9 [00:04<00:01,  1.71it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 8/9 [00:04<00:00,  1.73it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 9/9 [00:04<00:00,  2.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:37:09 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 15.242 | nll_loss 14.941 | ppl 31464.3 | bleu 0 | wps 5214.3 | wpb 2747.9 | bsz 222.2 | num_updates 127\n",
            "2022-08-29 06:37:09 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 127 updates\n",
            "2022-08-29 06:37:09 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-29 06:37:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-29 06:38:05 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint1.pt (epoch 1 @ 127 updates, score 15.242) (writing took 55.47066657700009 seconds)\n",
            "2022-08-29 06:38:05 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-08-29 06:38:05 | INFO | train | epoch 001 | loss 16.088 | nll_loss 15.894 | ppl 60895 | wps 6152.5 | ups 0.91 | wpb 6783.6 | bsz 533.4 | num_updates 127 | lr 1.5875e-05 | gnorm 2.71 | loss_scale 16 | train_wall 79 | gb_free 5.3 | wall 146\n",
            "2022-08-29 06:38:05 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 002:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:38:05 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-08-29 06:38:05 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 129/130 [01:18<00:00,  1.69it/s, loss=14.967, nll_loss=14.651, ppl=25726.9, wps=5509.7, ups=0.81, wpb=6786.9, bsz=541.8, num_updates=200, lr=2.5e-05, gnorm=1.71, loss_scale=16, train_wall=59, gb_free=3.6, wall=191]2022-08-29 06:39:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 1/9 [00:01<00:08,  1.02s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 2/9 [00:01<00:06,  1.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 3/9 [00:02<00:05,  1.12it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 4/9 [00:03<00:04,  1.09it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 5/9 [00:05<00:04,  1.08s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 6/9 [00:06<00:03,  1.05s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 7/9 [00:07<00:02,  1.03s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 8/9 [00:08<00:01,  1.03s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 9/9 [00:08<00:00,  1.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:39:32 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 14.06 | nll_loss 13.61 | ppl 12501.5 | bleu 0.03 | wps 2962.7 | wpb 2747.9 | bsz 222.2 | num_updates 257 | best_loss 14.06\n",
            "2022-08-29 06:39:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 257 updates\n",
            "2022-08-29 06:39:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-29 06:39:46 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-29 06:40:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint2.pt (epoch 2 @ 257 updates, score 14.06) (writing took 51.232888097000114 seconds)\n",
            "2022-08-29 06:40:23 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-08-29 06:40:23 | INFO | train | epoch 002 | loss 14.552 | nll_loss 14.192 | ppl 18721.4 | wps 6378.3 | ups 0.94 | wpb 6792.9 | bsz 538.5 | num_updates 257 | lr 3.2125e-05 | gnorm 2.208 | loss_scale 16 | train_wall 78 | gb_free 4.9 | wall 285\n",
            "2022-08-29 06:40:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 003:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:40:24 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-08-29 06:40:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 129/130 [01:18<00:00,  1.69it/s, loss=13.953, nll_loss=13.525, ppl=11790.2, wps=5649.1, ups=0.83, wpb=6817, bsz=538.7, num_updates=300, lr=3.75e-05, gnorm=2.25, loss_scale=16, train_wall=60, gb_free=4.5, wall=311]2022-08-29 06:41:43 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.81s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.68s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.70s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.74s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 5/9 [00:08<00:07,  1.79s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.84s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.88s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.95s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 9/9 [00:15<00:00,  1.56s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:41:58 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 13.024 | nll_loss 12.437 | ppl 5545.53 | bleu 0.09 | wps 1590.3 | wpb 2747.9 | bsz 222.2 | num_updates 387 | best_loss 13.024\n",
            "2022-08-29 06:41:58 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 387 updates\n",
            "2022-08-29 06:41:58 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-29 06:42:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-29 06:42:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint3.pt (epoch 3 @ 387 updates, score 13.024) (writing took 56.912010621000036 seconds)\n",
            "2022-08-29 06:42:55 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-08-29 06:42:55 | INFO | train | epoch 003 | loss 13.345 | nll_loss 12.832 | ppl 7291.28 | wps 5823.8 | ups 0.86 | wpb 6792.9 | bsz 538.5 | num_updates 387 | lr 4.8375e-05 | gnorm 2.149 | loss_scale 16 | train_wall 78 | gb_free 4.6 | wall 436\n",
            "2022-08-29 06:42:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 004:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:42:55 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-08-29 06:42:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 129/130 [01:18<00:00,  1.65it/s, loss=12.673, nll_loss=12.033, ppl=4191.75, wps=11236.7, ups=1.65, wpb=6800.6, bsz=550.6, num_updates=500, lr=6.25e-05, gnorm=2.134, loss_scale=16, train_wall=60, gb_free=3.8, wall=505]2022-08-29 06:44:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 1/9 [00:01<00:11,  1.43s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  22% 2/9 [00:02<00:09,  1.29s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.41s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.44s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.52s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.59s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.65s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.73s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.40s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:44:27 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 12.615 | nll_loss 11.917 | ppl 3867.94 | bleu 0.17 | wps 1819.3 | wpb 2747.9 | bsz 222.2 | num_updates 517 | best_loss 12.615\n",
            "2022-08-29 06:44:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 517 updates\n",
            "2022-08-29 06:44:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-29 06:44:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-29 06:45:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint4.pt (epoch 4 @ 517 updates, score 12.615) (writing took 52.51992453399998 seconds)\n",
            "2022-08-29 06:45:20 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-08-29 06:45:20 | INFO | train | epoch 004 | loss 12.696 | nll_loss 12.06 | ppl 4270.13 | wps 6105.2 | ups 0.9 | wpb 6792.9 | bsz 538.5 | num_updates 517 | lr 6.4625e-05 | gnorm 2.22 | loss_scale 16 | train_wall 78 | gb_free 3.8 | wall 581\n",
            "2022-08-29 06:45:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 005:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:45:20 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-08-29 06:45:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 129/130 [01:18<00:00,  1.71it/s, loss=12.398, nll_loss=11.706, ppl=3340.24, wps=5395.6, ups=0.79, wpb=6857.8, bsz=543.7, num_updates=600, lr=7.5e-05, gnorm=2.243, loss_scale=16, train_wall=60, gb_free=4.1, wall=632]2022-08-29 06:46:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.93s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.77s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.76s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 4/9 [00:07<00:09,  1.81s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.83s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 6/9 [00:11<00:05,  1.86s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.88s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 8/9 [00:15<00:01,  1.94s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 9/9 [00:15<00:00,  1.60s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:46:54 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.225 | nll_loss 11.462 | ppl 2820.78 | bleu 0.29 | wps 1564.9 | wpb 2747.9 | bsz 222.2 | num_updates 647 | best_loss 12.225\n",
            "2022-08-29 06:46:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 647 updates\n",
            "2022-08-29 06:46:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-29 06:47:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-29 06:47:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint5.pt (epoch 5 @ 647 updates, score 12.225) (writing took 53.81978280400017 seconds)\n",
            "2022-08-29 06:47:48 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-08-29 06:47:48 | INFO | train | epoch 005 | loss 12.324 | nll_loss 11.619 | ppl 3145.43 | wps 5950.4 | ups 0.88 | wpb 6792.9 | bsz 538.5 | num_updates 647 | lr 8.0875e-05 | gnorm 2.104 | loss_scale 16 | train_wall 78 | gb_free 4.8 | wall 729\n",
            "2022-08-29 06:47:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 006:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:47:48 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-08-29 06:47:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 129/130 [01:18<00:00,  1.66it/s, loss=12.141, nll_loss=11.407, ppl=2715.77, wps=5138.8, ups=0.77, wpb=6651.9, bsz=535.5, num_updates=700, lr=8.75e-05, gnorm=2.005, loss_scale=16, train_wall=59, gb_free=4.5, wall=761]2022-08-29 06:49:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.08s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  22% 2/9 [00:03<00:13,  1.89s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 3/9 [00:05<00:11,  1.90s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 4/9 [00:07<00:09,  1.98s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 5/9 [00:10<00:08,  2.05s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 6/9 [00:12<00:06,  2.12s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  78% 7/9 [00:14<00:04,  2.20s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 8/9 [00:17<00:02,  2.30s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 9/9 [00:17<00:00,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:49:25 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 11.858 | nll_loss 11.03 | ppl 2090.96 | bleu 0.37 | wps 1377.7 | wpb 2747.9 | bsz 222.2 | num_updates 777 | best_loss 11.858\n",
            "2022-08-29 06:49:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 777 updates\n",
            "2022-08-29 06:49:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-29 06:49:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-29 06:50:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint6.pt (epoch 6 @ 777 updates, score 11.858) (writing took 53.56946521000009 seconds)\n",
            "2022-08-29 06:50:19 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-08-29 06:50:19 | INFO | train | epoch 006 | loss 11.954 | nll_loss 11.191 | ppl 2338.42 | wps 5866.5 | ups 0.86 | wpb 6792.9 | bsz 538.5 | num_updates 777 | lr 9.7125e-05 | gnorm 2.074 | loss_scale 16 | train_wall 78 | gb_free 4.5 | wall 880\n",
            "2022-08-29 06:50:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 007:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:50:19 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-08-29 06:50:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 129/130 [01:19<00:00,  1.71it/s, loss=11.455, nll_loss=10.62, ppl=1573.36, wps=11176.3, ups=1.64, wpb=6802.1, bsz=539.3, num_updates=900, lr=0.0001125, gnorm=1.911, loss_scale=16, train_wall=60, gb_free=4.1, wall=956]2022-08-29 06:51:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.76s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.64s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.65s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.74s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 5/9 [00:08<00:07,  1.82s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 6/9 [00:11<00:05,  1.98s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  78% 7/9 [00:13<00:04,  2.04s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 8/9 [00:15<00:02,  2.13s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 9/9 [00:16<00:00,  1.69s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:51:54 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.341 | nll_loss 10.442 | ppl 1391.05 | bleu 1.06 | wps 1496 | wpb 2747.9 | bsz 222.2 | num_updates 907 | best_loss 11.341\n",
            "2022-08-29 06:51:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 907 updates\n",
            "2022-08-29 06:51:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-29 06:52:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-29 06:52:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint7.pt (epoch 7 @ 907 updates, score 11.341) (writing took 52.9910740790001 seconds)\n",
            "2022-08-29 06:52:47 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-08-29 06:52:47 | INFO | train | epoch 007 | loss 11.483 | nll_loss 10.652 | ppl 1608.94 | wps 5934.4 | ups 0.87 | wpb 6792.9 | bsz 538.5 | num_updates 907 | lr 0.000113375 | gnorm 1.957 | loss_scale 16 | train_wall 78 | gb_free 5.4 | wall 1029\n",
            "2022-08-29 06:52:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 008:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:52:48 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-08-29 06:52:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 129/130 [01:18<00:00,  1.66it/s, loss=10.985, nll_loss=10.087, ppl=1087.56, wps=5209.7, ups=0.77, wpb=6796.1, bsz=550.2, num_updates=1000, lr=0.000125, gnorm=1.848, loss_scale=16, train_wall=60, gb_free=4.1, wall=1086]2022-08-29 06:54:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.82s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.69s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.71s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 4/9 [00:07<00:08,  1.79s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.86s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 6/9 [00:11<00:05,  1.94s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  78% 7/9 [00:13<00:04,  2.01s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 8/9 [00:15<00:02,  2.12s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 9/9 [00:16<00:00,  1.69s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:54:23 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 10.841 | nll_loss 9.861 | ppl 929.64 | bleu 2.19 | wps 1495.7 | wpb 2747.9 | bsz 222.2 | num_updates 1037 | best_loss 10.841\n",
            "2022-08-29 06:54:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1037 updates\n",
            "2022-08-29 06:54:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-29 06:54:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-29 06:55:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint8.pt (epoch 8 @ 1037 updates, score 10.841) (writing took 53.689901770999995 seconds)\n",
            "2022-08-29 06:55:17 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-08-29 06:55:17 | INFO | train | epoch 008 | loss 10.933 | nll_loss 10.027 | ppl 1043.35 | wps 5907.9 | ups 0.87 | wpb 6792.9 | bsz 538.5 | num_updates 1037 | lr 0.000129625 | gnorm 1.813 | loss_scale 16 | train_wall 78 | gb_free 4.2 | wall 1178\n",
            "2022-08-29 06:55:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 009:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:55:17 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-08-29 06:55:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 129/130 [01:18<00:00,  1.67it/s, loss=10.626, nll_loss=9.678, ppl=819.05, wps=5155.5, ups=0.76, wpb=6773.8, bsz=518.6, num_updates=1100, lr=0.0001375, gnorm=1.889, loss_scale=16, train_wall=60, gb_free=4, wall=1217]2022-08-29 06:56:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.97s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.81s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.82s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 4/9 [00:07<00:10,  2.08s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 5/9 [00:10<00:08,  2.07s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 6/9 [00:12<00:06,  2.09s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  78% 7/9 [00:14<00:04,  2.10s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 8/9 [00:16<00:02,  2.15s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 9/9 [00:17<00:00,  1.69s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:56:54 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 10.484 | nll_loss 9.446 | ppl 697.25 | bleu 2.95 | wps 1427.9 | wpb 2747.9 | bsz 222.2 | num_updates 1167 | best_loss 10.484\n",
            "2022-08-29 06:56:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1167 updates\n",
            "2022-08-29 06:56:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-29 06:57:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-29 06:57:47 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint9.pt (epoch 9 @ 1167 updates, score 10.484) (writing took 53.51278863100015 seconds)\n",
            "2022-08-29 06:57:47 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-08-29 06:57:47 | INFO | train | epoch 009 | loss 10.402 | nll_loss 9.423 | ppl 686.55 | wps 5878.4 | ups 0.87 | wpb 6792.9 | bsz 538.5 | num_updates 1167 | lr 0.000145875 | gnorm 2.006 | loss_scale 16 | train_wall 78 | gb_free 4.2 | wall 1328\n",
            "2022-08-29 06:57:47 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 010:   0% 0/130 [00:00<?, ?it/s]2022-08-29 06:57:47 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-08-29 06:57:47 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 129/130 [01:18<00:00,  1.69it/s, loss=10.18, nll_loss=9.171, ppl=576.48, wps=5150.9, ups=0.76, wpb=6796.4, bsz=560, num_updates=1200, lr=0.00015, gnorm=2.015, loss_scale=16, train_wall=60, gb_free=4, wall=1349]2022-08-29 06:59:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.78s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.64s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 3/9 [00:05<00:09,  1.66s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.72s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 5/9 [00:08<00:07,  1.77s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.83s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.89s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.98s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 9/9 [00:15<00:00,  1.58s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:59:22 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 10.05 | nll_loss 8.953 | ppl 495.68 | bleu 4.67 | wps 1584.2 | wpb 2747.9 | bsz 222.2 | num_updates 1297 | best_loss 10.05\n",
            "2022-08-29 06:59:22 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1297 updates\n",
            "2022-08-29 06:59:22 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-29 06:59:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-29 07:00:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint10.pt (epoch 10 @ 1297 updates, score 10.05) (writing took 54.43495526600009 seconds)\n",
            "2022-08-29 07:00:17 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-08-29 07:00:17 | INFO | train | epoch 010 | loss 9.879 | nll_loss 8.828 | ppl 454.55 | wps 5910.2 | ups 0.87 | wpb 6792.9 | bsz 538.5 | num_updates 1297 | lr 0.000162125 | gnorm 2.027 | loss_scale 16 | train_wall 78 | gb_free 4.6 | wall 1478\n",
            "2022-08-29 07:00:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 011:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:00:17 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2022-08-29 07:00:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 129/130 [01:18<00:00,  1.62it/s, loss=9.355, nll_loss=8.233, ppl=300.87, wps=11122.2, ups=1.65, wpb=6735.1, bsz=544.8, num_updates=1400, lr=0.000175, gnorm=2.011, loss_scale=16, train_wall=60, gb_free=3.5, wall=1541]2022-08-29 07:01:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.75s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.68s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.60s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.61s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.62s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 6/9 [00:09<00:05,  1.68s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.70s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.76s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.41s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:01:50 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 9.756 | nll_loss 8.593 | ppl 386.17 | bleu 7.26 | wps 1744.4 | wpb 2747.9 | bsz 222.2 | num_updates 1427 | best_loss 9.756\n",
            "2022-08-29 07:01:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1427 updates\n",
            "2022-08-29 07:01:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-29 07:02:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-29 07:02:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint11.pt (epoch 11 @ 1427 updates, score 9.756) (writing took 52.46886054400011 seconds)\n",
            "2022-08-29 07:02:43 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-08-29 07:02:43 | INFO | train | epoch 011 | loss 9.337 | nll_loss 8.212 | ppl 296.48 | wps 6048.1 | ups 0.89 | wpb 6792.9 | bsz 538.5 | num_updates 1427 | lr 0.000178375 | gnorm 2 | loss_scale 16 | train_wall 78 | gb_free 4.5 | wall 1624\n",
            "2022-08-29 07:02:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 012:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:02:43 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2022-08-29 07:02:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 129/130 [01:18<00:00,  1.69it/s, loss=8.95, nll_loss=7.772, ppl=218.54, wps=5395.2, ups=0.78, wpb=6905, bsz=534.8, num_updates=1500, lr=0.0001875, gnorm=2.012, loss_scale=16, train_wall=60, gb_free=3.4, wall=1669]2022-08-29 07:04:02 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.69s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.53s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.57s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.64s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.66s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 6/9 [00:09<00:05,  1.71s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.75s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.81s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:04:16 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 9.331 | nll_loss 8.102 | ppl 274.69 | bleu 8.33 | wps 1716.1 | wpb 2747.9 | bsz 222.2 | num_updates 1557 | best_loss 9.331\n",
            "2022-08-29 07:04:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1557 updates\n",
            "2022-08-29 07:04:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-29 07:04:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-29 07:05:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint12.pt (epoch 12 @ 1557 updates, score 9.331) (writing took 53.986480865999965 seconds)\n",
            "2022-08-29 07:05:10 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-08-29 07:05:10 | INFO | train | epoch 012 | loss 8.805 | nll_loss 7.606 | ppl 194.83 | wps 5984.9 | ups 0.88 | wpb 6792.9 | bsz 538.5 | num_updates 1557 | lr 0.000194625 | gnorm 2.01 | loss_scale 16 | train_wall 78 | gb_free 4.2 | wall 1771\n",
            "2022-08-29 07:05:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 013:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:05:10 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2022-08-29 07:05:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 129/130 [01:18<00:00,  1.69it/s, loss=8.565, nll_loss=7.332, ppl=161.14, wps=5237.5, ups=0.77, wpb=6763.2, bsz=553.4, num_updates=1600, lr=0.0002, gnorm=2.063, loss_scale=16, train_wall=60, gb_free=3.7, wall=1798]2022-08-29 07:06:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.95s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.63s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.63s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.67s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.71s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.74s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.77s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.83s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:06:44 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 9.017 | nll_loss 7.721 | ppl 211.03 | bleu 10.13 | wps 1701.7 | wpb 2747.9 | bsz 222.2 | num_updates 1687 | best_loss 9.017\n",
            "2022-08-29 07:06:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1687 updates\n",
            "2022-08-29 07:06:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-29 07:06:58 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-29 07:07:39 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint13.pt (epoch 13 @ 1687 updates, score 9.017) (writing took 55.30942235500015 seconds)\n",
            "2022-08-29 07:07:39 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2022-08-29 07:07:39 | INFO | train | epoch 013 | loss 8.296 | nll_loss 7.025 | ppl 130.25 | wps 5922.3 | ups 0.87 | wpb 6792.9 | bsz 538.5 | num_updates 1687 | lr 0.000210875 | gnorm 2.107 | loss_scale 16 | train_wall 78 | gb_free 4.3 | wall 1921\n",
            "2022-08-29 07:07:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 014:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:07:39 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2022-08-29 07:07:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 129/130 [01:18<00:00,  1.72it/s, loss=7.79, nll_loss=6.447, ppl=87.26, wps=11147.4, ups=1.65, wpb=6747.5, bsz=537.6, num_updates=1800, lr=0.000225, gnorm=2.104, loss_scale=16, train_wall=60, gb_free=3.8, wall=1990]2022-08-29 07:08:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.53s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.56s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.59s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.59s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  67% 6/9 [00:09<00:05,  1.69s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.81s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.84s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:09:13 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 8.729 | nll_loss 7.368 | ppl 165.24 | bleu 12.63 | wps 1716.1 | wpb 2747.9 | bsz 222.2 | num_updates 1817 | best_loss 8.729\n",
            "2022-08-29 07:09:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1817 updates\n",
            "2022-08-29 07:09:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-29 07:09:27 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-29 07:10:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint14.pt (epoch 14 @ 1817 updates, score 8.729) (writing took 49.961847118999685 seconds)\n",
            "2022-08-29 07:10:03 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2022-08-29 07:10:03 | INFO | train | epoch 014 | loss 7.769 | nll_loss 6.423 | ppl 85.83 | wps 6152.4 | ups 0.91 | wpb 6792.9 | bsz 538.5 | num_updates 1817 | lr 0.000227125 | gnorm 2.012 | loss_scale 16 | train_wall 78 | gb_free 3.9 | wall 2064\n",
            "2022-08-29 07:10:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 015:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:10:03 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2022-08-29 07:10:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 129/130 [01:18<00:00,  1.67it/s, loss=7.336, nll_loss=5.927, ppl=60.84, wps=5417.7, ups=0.8, wpb=6785.3, bsz=541.4, num_updates=1900, lr=0.0002375, gnorm=1.934, loss_scale=16, train_wall=60, gb_free=3.3, wall=2115]2022-08-29 07:11:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  11% 1/9 [00:01<00:12,  1.56s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.70s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.75s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.74s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.68s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  67% 6/9 [00:10<00:04,  1.65s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.63s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.68s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.35s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:11:36 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 8.796 | nll_loss 7.432 | ppl 172.67 | bleu 14.33 | wps 1741 | wpb 2747.9 | bsz 222.2 | num_updates 1947 | best_loss 8.729\n",
            "2022-08-29 07:11:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1947 updates\n",
            "2022-08-29 07:11:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-29 07:11:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-29 07:12:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint15.pt (epoch 15 @ 1947 updates, score 8.796) (writing took 36.887162928999714 seconds)\n",
            "2022-08-29 07:12:13 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2022-08-29 07:12:13 | INFO | train | epoch 015 | loss 7.269 | nll_loss 5.849 | ppl 57.65 | wps 6783.4 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 1947 | lr 0.000243375 | gnorm 1.987 | loss_scale 16 | train_wall 78 | gb_free 4.9 | wall 2194\n",
            "2022-08-29 07:12:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 016:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:12:13 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2022-08-29 07:12:13 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 129/130 [01:18<00:00,  1.67it/s, loss=7.031, nll_loss=5.576, ppl=47.72, wps=6088.9, ups=0.89, wpb=6824.4, bsz=535.4, num_updates=2000, lr=0.00025, gnorm=1.955, loss_scale=16, train_wall=60, gb_free=3.7, wall=2227]2022-08-29 07:13:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.87s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.67s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.67s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.75s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  56% 5/9 [00:08<00:07,  1.86s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.90s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.86s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.87s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 9/9 [00:15<00:00,  1.49s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:13:47 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 8.223 | nll_loss 6.773 | ppl 109.35 | bleu 15.87 | wps 1622 | wpb 2747.9 | bsz 222.2 | num_updates 2077 | best_loss 8.223\n",
            "2022-08-29 07:13:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2077 updates\n",
            "2022-08-29 07:13:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-29 07:14:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-29 07:14:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint16.pt (epoch 16 @ 2077 updates, score 8.223) (writing took 56.888247316999696 seconds)\n",
            "2022-08-29 07:14:44 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2022-08-29 07:14:44 | INFO | train | epoch 016 | loss 6.773 | nll_loss 5.281 | ppl 38.89 | wps 5831.7 | ups 0.86 | wpb 6792.9 | bsz 538.5 | num_updates 2077 | lr 0.000259625 | gnorm 1.82 | loss_scale 16 | train_wall 78 | gb_free 3.8 | wall 2346\n",
            "2022-08-29 07:14:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 017:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:14:45 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2022-08-29 07:14:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 129/130 [01:18<00:00,  1.67it/s, loss=6.269, nll_loss=4.701, ppl=26, wps=11259.2, ups=1.64, wpb=6850.9, bsz=545.4, num_updates=2200, lr=0.000275, gnorm=1.81, loss_scale=16, train_wall=60, gb_free=3.6, wall=2421]2022-08-29 07:16:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.96s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.64s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.63s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.64s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.64s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.65s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.70s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.76s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.41s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:16:18 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 8.06 | nll_loss 6.576 | ppl 95.4 | bleu 18.09 | wps 1765.7 | wpb 2747.9 | bsz 222.2 | num_updates 2207 | best_loss 8.06\n",
            "2022-08-29 07:16:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2207 updates\n",
            "2022-08-29 07:16:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-29 07:16:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-29 07:17:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint17.pt (epoch 17 @ 2207 updates, score 8.06) (writing took 94.5876543899999 seconds)\n",
            "2022-08-29 07:17:53 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2022-08-29 07:17:53 | INFO | train | epoch 017 | loss 6.281 | nll_loss 4.715 | ppl 26.26 | wps 4693.7 | ups 0.69 | wpb 6792.9 | bsz 538.5 | num_updates 2207 | lr 0.000275875 | gnorm 1.84 | loss_scale 16 | train_wall 78 | gb_free 4.6 | wall 2534\n",
            "2022-08-29 07:17:53 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 018:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:17:53 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2022-08-29 07:17:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 129/130 [01:18<00:00,  1.69it/s, loss=5.839, nll_loss=4.207, ppl=18.47, wps=3950.1, ups=0.59, wpb=6707.5, bsz=524.8, num_updates=2300, lr=0.0002875, gnorm=1.674, loss_scale=16, train_wall=60, gb_free=3.4, wall=2591]2022-08-29 07:19:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.77s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.58s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.61s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.65s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.69s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.72s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.74s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.78s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:19:26 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 7.982 | nll_loss 6.46 | ppl 88.05 | bleu 18.28 | wps 1724.5 | wpb 2747.9 | bsz 222.2 | num_updates 2337 | best_loss 7.982\n",
            "2022-08-29 07:19:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2337 updates\n",
            "2022-08-29 07:19:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-29 07:19:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-29 07:21:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint18.pt (epoch 18 @ 2337 updates, score 7.982) (writing took 96.65149217199996 seconds)\n",
            "2022-08-29 07:21:03 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2022-08-29 07:21:03 | INFO | train | epoch 018 | loss 5.785 | nll_loss 4.144 | ppl 17.67 | wps 4645.9 | ups 0.68 | wpb 6792.9 | bsz 538.5 | num_updates 2337 | lr 0.000292125 | gnorm 1.629 | loss_scale 16 | train_wall 78 | gb_free 3.9 | wall 2724\n",
            "2022-08-29 07:21:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 019:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:21:03 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2022-08-29 07:21:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 129/130 [01:18<00:00,  1.70it/s, loss=5.459, nll_loss=3.769, ppl=13.64, wps=3982.1, ups=0.58, wpb=6841.8, bsz=545, num_updates=2400, lr=0.0003, gnorm=1.578, loss_scale=16, train_wall=60, gb_free=4.8, wall=2763]2022-08-29 07:22:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.74s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.71s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 3/9 [00:05<00:09,  1.65s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.65s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.66s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.70s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.72s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.76s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.38s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:22:36 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 7.796 | nll_loss 6.234 | ppl 75.29 | bleu 20.77 | wps 1739.5 | wpb 2747.9 | bsz 222.2 | num_updates 2467 | best_loss 7.796\n",
            "2022-08-29 07:22:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 2467 updates\n",
            "2022-08-29 07:22:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-29 07:22:51 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-29 07:24:12 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint19.pt (epoch 19 @ 2467 updates, score 7.796) (writing took 95.65173289099994 seconds)\n",
            "2022-08-29 07:24:12 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2022-08-29 07:24:12 | INFO | train | epoch 019 | loss 5.344 | nll_loss 3.633 | ppl 12.41 | wps 4670.8 | ups 0.69 | wpb 6792.9 | bsz 538.5 | num_updates 2467 | lr 0.000308375 | gnorm 1.64 | loss_scale 16 | train_wall 78 | gb_free 4.1 | wall 2913\n",
            "2022-08-29 07:24:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 020:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:24:12 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2022-08-29 07:24:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 129/130 [01:18<00:00,  1.69it/s, loss=5.233, nll_loss=3.503, ppl=11.34, wps=3963.4, ups=0.58, wpb=6791.5, bsz=530.4, num_updates=2500, lr=0.0003125, gnorm=1.637, loss_scale=16, train_wall=60, gb_free=3.9, wall=2934]2022-08-29 07:25:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.74s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.54s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.56s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.56s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.58s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.58s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.58s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.62s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.29s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:25:45 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 7.643 | nll_loss 6.029 | ppl 65.3 | bleu 23.12 | wps 1877.1 | wpb 2747.9 | bsz 222.2 | num_updates 2597 | best_loss 7.643\n",
            "2022-08-29 07:25:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2597 updates\n",
            "2022-08-29 07:25:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-29 07:25:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-29 07:27:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint20.pt (epoch 20 @ 2597 updates, score 7.643) (writing took 94.88794661700013 seconds)\n",
            "2022-08-29 07:27:19 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2022-08-29 07:27:20 | INFO | train | epoch 020 | loss 4.898 | nll_loss 3.116 | ppl 8.67 | wps 4704.6 | ups 0.69 | wpb 6792.9 | bsz 538.5 | num_updates 2597 | lr 0.000324625 | gnorm 1.47 | loss_scale 16 | train_wall 78 | gb_free 3.9 | wall 3101\n",
            "2022-08-29 07:27:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 021:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:27:20 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2022-08-29 07:27:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 129/130 [01:20<00:00,  1.70it/s, loss=4.448, nll_loss=2.596, ppl=6.04, wps=11196.9, ups=1.64, wpb=6808.1, bsz=537.9, num_updates=2700, lr=0.0003375, gnorm=1.365, loss_scale=16, train_wall=60, gb_free=4.9, wall=3166]2022-08-29 07:28:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.63s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.44s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.47s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.49s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.67s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.63s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.60s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.62s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.29s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:28:54 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 7.623 | nll_loss 6.024 | ppl 65.08 | bleu 24.62 | wps 1869.1 | wpb 2747.9 | bsz 222.2 | num_updates 2727 | best_loss 7.623\n",
            "2022-08-29 07:28:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2727 updates\n",
            "2022-08-29 07:28:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-29 07:29:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-29 07:30:29 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint21.pt (epoch 21 @ 2727 updates, score 7.623) (writing took 94.79041934199995 seconds)\n",
            "2022-08-29 07:30:29 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2022-08-29 07:30:29 | INFO | train | epoch 021 | loss 4.462 | nll_loss 2.609 | ppl 6.1 | wps 4665.5 | ups 0.69 | wpb 6792.9 | bsz 538.5 | num_updates 2727 | lr 0.000340875 | gnorm 1.378 | loss_scale 16 | train_wall 78 | gb_free 5.4 | wall 3290\n",
            "2022-08-29 07:30:29 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 022:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:30:29 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2022-08-29 07:30:29 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 129/130 [01:20<00:00,  1.69it/s, loss=4.151, nll_loss=2.247, ppl=4.75, wps=3967.7, ups=0.59, wpb=6781.9, bsz=536, num_updates=2800, lr=0.00035, gnorm=1.275, loss_scale=16, train_wall=60, gb_free=4.5, wall=3337]2022-08-29 07:31:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.67s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.49s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.50s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.52s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.59s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.65s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.70s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.33s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:32:03 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 7.554 | nll_loss 5.898 | ppl 59.64 | bleu 25.01 | wps 1853.4 | wpb 2747.9 | bsz 222.2 | num_updates 2857 | best_loss 7.554\n",
            "2022-08-29 07:32:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2857 updates\n",
            "2022-08-29 07:32:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-29 07:32:17 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-29 07:33:38 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint22.pt (epoch 22 @ 2857 updates, score 7.554) (writing took 95.23522576800042 seconds)\n",
            "2022-08-29 07:33:38 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2022-08-29 07:33:38 | INFO | train | epoch 022 | loss 4.069 | nll_loss 2.149 | ppl 4.44 | wps 4657.7 | ups 0.69 | wpb 6792.9 | bsz 538.5 | num_updates 2857 | lr 0.000357125 | gnorm 1.265 | loss_scale 16 | train_wall 78 | gb_free 4.8 | wall 3480\n",
            "2022-08-29 07:33:39 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 023:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:33:39 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2022-08-29 07:33:39 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 129/130 [01:20<00:00,  1.62it/s, loss=3.934, nll_loss=1.987, ppl=3.97, wps=3949.3, ups=0.58, wpb=6761, bsz=526.2, num_updates=2900, lr=0.0003625, gnorm=1.237, loss_scale=16, train_wall=60, gb_free=3.4, wall=3508]2022-08-29 07:34:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.49s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.50s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.51s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  67% 6/9 [00:09<00:05,  1.71s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.66s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.68s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.33s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:35:13 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 7.521 | nll_loss 5.851 | ppl 57.73 | bleu 25.79 | wps 1845.3 | wpb 2747.9 | bsz 222.2 | num_updates 2987 | best_loss 7.521\n",
            "2022-08-29 07:35:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2987 updates\n",
            "2022-08-29 07:35:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-29 07:35:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-29 07:36:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint23.pt (epoch 23 @ 2987 updates, score 7.521) (writing took 95.78736491199925 seconds)\n",
            "2022-08-29 07:36:49 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2022-08-29 07:36:49 | INFO | train | epoch 023 | loss 3.694 | nll_loss 1.707 | ppl 3.26 | wps 4643.5 | ups 0.68 | wpb 6792.9 | bsz 538.5 | num_updates 2987 | lr 0.000373375 | gnorm 1.135 | loss_scale 16 | train_wall 78 | gb_free 4.3 | wall 3670\n",
            "2022-08-29 07:36:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 024:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:36:49 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2022-08-29 07:36:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 129/130 [01:18<00:00,  1.71it/s, loss=3.398, nll_loss=1.353, ppl=2.55, wps=11306.5, ups=1.65, wpb=6870.1, bsz=548.5, num_updates=3100, lr=0.0003875, gnorm=1.102, loss_scale=16, train_wall=60, gb_free=4.1, wall=3739]2022-08-29 07:38:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.67s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.47s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.50s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.50s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.50s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.52s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.52s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.56s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.25s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:38:20 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 7.571 | nll_loss 5.927 | ppl 60.84 | bleu 26.29 | wps 1953.4 | wpb 2747.9 | bsz 222.2 | num_updates 3117 | best_loss 7.521\n",
            "2022-08-29 07:38:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3117 updates\n",
            "2022-08-29 07:38:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-29 07:38:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-29 07:39:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint24.pt (epoch 24 @ 3117 updates, score 7.571) (writing took 39.17348595099975 seconds)\n",
            "2022-08-29 07:39:00 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2022-08-29 07:39:00 | INFO | train | epoch 024 | loss 3.403 | nll_loss 1.359 | ppl 2.57 | wps 6737.4 | ups 0.99 | wpb 6792.9 | bsz 538.5 | num_updates 3117 | lr 0.000389625 | gnorm 1.099 | loss_scale 16 | train_wall 78 | gb_free 5.4 | wall 3801\n",
            "2022-08-29 07:39:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 025:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:39:00 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2022-08-29 07:39:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 129/130 [01:18<00:00,  1.72it/s, loss=3.174, nll_loss=1.082, ppl=2.12, wps=5983.2, ups=0.88, wpb=6784.6, bsz=544.9, num_updates=3200, lr=0.0004, gnorm=0.964, loss_scale=16, train_wall=60, gb_free=3.4, wall=3852]2022-08-29 07:40:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.75s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.58s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.54s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.54s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.54s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.56s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.57s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.76s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.38s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:40:33 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 7.611 | nll_loss 5.948 | ppl 61.75 | bleu 25.64 | wps 1824 | wpb 2747.9 | bsz 222.2 | num_updates 3247 | best_loss 7.521\n",
            "2022-08-29 07:40:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3247 updates\n",
            "2022-08-29 07:40:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-29 07:40:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-29 07:41:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint25.pt (epoch 25 @ 3247 updates, score 7.611) (writing took 37.15549959799955 seconds)\n",
            "2022-08-29 07:41:10 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2022-08-29 07:41:10 | INFO | train | epoch 025 | loss 3.153 | nll_loss 1.055 | ppl 2.08 | wps 6786.5 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 3247 | lr 0.000405875 | gnorm 0.965 | loss_scale 16 | train_wall 78 | gb_free 4.8 | wall 3931\n",
            "2022-08-29 07:41:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 026:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:41:10 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2022-08-29 07:41:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 129/130 [01:19<00:00,  1.66it/s, loss=3.05, nll_loss=0.925, ppl=1.9, wps=6024.2, ups=0.9, wpb=6728.5, bsz=515.2, num_updates=3300, lr=0.0004125, gnorm=0.873, loss_scale=16, train_wall=60, gb_free=3.9, wall=3964]2022-08-29 07:42:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.65s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.45s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.47s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.50s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.52s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.53s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.56s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.23s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:42:42 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 7.545 | nll_loss 5.88 | ppl 58.91 | bleu 26.74 | wps 1972.2 | wpb 2747.9 | bsz 222.2 | num_updates 3377 | best_loss 7.521\n",
            "2022-08-29 07:42:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 3377 updates\n",
            "2022-08-29 07:42:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-29 07:42:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-29 07:43:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint26.pt (epoch 26 @ 3377 updates, score 7.545) (writing took 33.66788020900003 seconds)\n",
            "2022-08-29 07:43:16 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2022-08-29 07:43:16 | INFO | train | epoch 026 | loss 2.955 | nll_loss 0.81 | ppl 1.75 | wps 7010.8 | ups 1.03 | wpb 6792.9 | bsz 538.5 | num_updates 3377 | lr 0.000422125 | gnorm 0.797 | loss_scale 16 | train_wall 78 | gb_free 4.6 | wall 4057\n",
            "2022-08-29 07:43:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 027:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:43:16 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2022-08-29 07:43:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 129/130 [01:18<00:00,  1.65it/s, loss=2.85, nll_loss=0.68, ppl=1.6, wps=11106.5, ups=1.64, wpb=6771.5, bsz=537, num_updates=3500, lr=0.0004375, gnorm=0.732, loss_scale=16, train_wall=60, gb_free=4, wall=4132]2022-08-29 07:44:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.75s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.51s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.50s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.53s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.54s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.56s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.58s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.25s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:44:48 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 7.586 | nll_loss 5.893 | ppl 59.44 | bleu 25.85 | wps 1939.5 | wpb 2747.9 | bsz 222.2 | num_updates 3507 | best_loss 7.521\n",
            "2022-08-29 07:44:48 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 3507 updates\n",
            "2022-08-29 07:44:48 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-29 07:45:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-29 07:45:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint27.pt (epoch 27 @ 3507 updates, score 7.586) (writing took 37.65210826200018 seconds)\n",
            "2022-08-29 07:45:26 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2022-08-29 07:45:26 | INFO | train | epoch 027 | loss 2.843 | nll_loss 0.67 | ppl 1.59 | wps 6788.9 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 3507 | lr 0.000438375 | gnorm 0.729 | loss_scale 16 | train_wall 78 | gb_free 4.8 | wall 4187\n",
            "2022-08-29 07:45:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 028:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:45:26 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2022-08-29 07:45:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 129/130 [01:18<00:00,  1.68it/s, loss=2.802, nll_loss=0.616, ppl=1.53, wps=6080.5, ups=0.89, wpb=6806, bsz=541.8, num_updates=3600, lr=0.00045, gnorm=0.753, loss_scale=16, train_wall=60, gb_free=3.9, wall=4244]2022-08-29 07:46:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.85s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.65s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.75s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.72s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.72s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.70s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.71s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.74s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.39s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:47:00 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 7.55 | nll_loss 5.895 | ppl 59.51 | bleu 26.2 | wps 1730.2 | wpb 2747.9 | bsz 222.2 | num_updates 3637 | best_loss 7.521\n",
            "2022-08-29 07:47:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 3637 updates\n",
            "2022-08-29 07:47:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-29 07:47:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-29 07:47:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint28.pt (epoch 28 @ 3637 updates, score 7.55) (writing took 36.59294690899969 seconds)\n",
            "2022-08-29 07:47:36 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2022-08-29 07:47:36 | INFO | train | epoch 028 | loss 2.801 | nll_loss 0.617 | ppl 1.53 | wps 6763.1 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 3637 | lr 0.000454625 | gnorm 0.732 | loss_scale 16 | train_wall 78 | gb_free 4.6 | wall 4318\n",
            "2022-08-29 07:47:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 029:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:47:37 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2022-08-29 07:47:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 129/130 [01:18<00:00,  1.64it/s, loss=2.754, nll_loss=0.563, ppl=1.48, wps=6027.7, ups=0.89, wpb=6756.7, bsz=544.2, num_updates=3700, lr=0.0004625, gnorm=0.679, loss_scale=16, train_wall=60, gb_free=3.7, wall=4356]2022-08-29 07:48:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.47s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.49s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.51s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.53s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.55s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.26s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:49:08 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 7.572 | nll_loss 5.934 | ppl 61.16 | bleu 27.17 | wps 1939.3 | wpb 2747.9 | bsz 222.2 | num_updates 3767 | best_loss 7.521\n",
            "2022-08-29 07:49:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 3767 updates\n",
            "2022-08-29 07:49:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-29 07:49:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-29 07:49:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint29.pt (epoch 29 @ 3767 updates, score 7.572) (writing took 37.09329757599971 seconds)\n",
            "2022-08-29 07:49:46 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2022-08-29 07:49:46 | INFO | train | epoch 029 | loss 2.738 | nll_loss 0.546 | ppl 1.46 | wps 6838.8 | ups 1.01 | wpb 6792.9 | bsz 538.5 | num_updates 3767 | lr 0.000470875 | gnorm 0.646 | loss_scale 16 | train_wall 78 | gb_free 4.5 | wall 4447\n",
            "2022-08-29 07:49:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 030:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:49:46 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2022-08-29 07:49:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 129/130 [01:18<00:00,  1.71it/s, loss=2.736, nll_loss=0.546, ppl=1.46, wps=6187.8, ups=0.9, wpb=6846.3, bsz=536.6, num_updates=3800, lr=0.000475, gnorm=0.646, loss_scale=16, train_wall=60, gb_free=3.8, wall=4467]2022-08-29 07:51:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.69s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.46s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.45s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.48s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.60s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.58s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.57s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.60s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.26s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:51:18 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 7.547 | nll_loss 5.935 | ppl 61.17 | bleu 26.64 | wps 1917.9 | wpb 2747.9 | bsz 222.2 | num_updates 3897 | best_loss 7.521\n",
            "2022-08-29 07:51:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 3897 updates\n",
            "2022-08-29 07:51:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-29 07:51:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-29 07:51:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint30.pt (epoch 30 @ 3897 updates, score 7.547) (writing took 36.841909301999294 seconds)\n",
            "2022-08-29 07:51:54 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2022-08-29 07:51:54 | INFO | train | epoch 030 | loss 2.706 | nll_loss 0.515 | ppl 1.43 | wps 6850.7 | ups 1.01 | wpb 6792.9 | bsz 538.5 | num_updates 3897 | lr 0.000487125 | gnorm 0.615 | loss_scale 16 | train_wall 78 | gb_free 4.8 | wall 4576\n",
            "2022-08-29 07:51:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 031:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:51:55 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2022-08-29 07:51:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 129/130 [01:18<00:00,  1.67it/s, loss=2.664, nll_loss=0.47, ppl=1.39, wps=11230.8, ups=1.65, wpb=6810.3, bsz=537.8, num_updates=4000, lr=0.0005, gnorm=0.571, loss_scale=16, train_wall=60, gb_free=4.1, wall=4639]2022-08-29 07:53:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.66s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.44s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.47s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.49s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.51s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.55s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.27s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:53:27 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 7.614 | nll_loss 5.989 | ppl 63.51 | bleu 27.09 | wps 1943.5 | wpb 2747.9 | bsz 222.2 | num_updates 4027 | best_loss 7.521\n",
            "2022-08-29 07:53:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4027 updates\n",
            "2022-08-29 07:53:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-29 07:53:41 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-29 07:54:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint31.pt (epoch 31 @ 4027 updates, score 7.614) (writing took 33.90538168200055 seconds)\n",
            "2022-08-29 07:54:01 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2022-08-29 07:54:01 | INFO | train | epoch 031 | loss 2.676 | nll_loss 0.487 | ppl 1.4 | wps 6998.5 | ups 1.03 | wpb 6792.9 | bsz 538.5 | num_updates 4027 | lr 0.000498321 | gnorm 0.578 | loss_scale 16 | train_wall 78 | gb_free 3.8 | wall 4702\n",
            "2022-08-29 07:54:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 032:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:54:01 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2022-08-29 07:54:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 129/130 [01:18<00:00,  1.70it/s, loss=2.649, nll_loss=0.457, ppl=1.37, wps=6347, ups=0.93, wpb=6824, bsz=519, num_updates=4100, lr=0.000493865, gnorm=0.54, loss_scale=16, train_wall=60, gb_free=3.7, wall=4747]2022-08-29 07:55:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.48s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.46s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.48s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.51s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.52s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.54s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.73s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.36s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:55:33 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 7.559 | nll_loss 5.967 | ppl 62.56 | bleu 26.41 | wps 1872.9 | wpb 2747.9 | bsz 222.2 | num_updates 4157 | best_loss 7.521\n",
            "2022-08-29 07:55:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 4157 updates\n",
            "2022-08-29 07:55:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-29 07:55:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-29 07:56:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint32.pt (epoch 32 @ 4157 updates, score 7.559) (writing took 36.951736469000025 seconds)\n",
            "2022-08-29 07:56:10 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2022-08-29 07:56:10 | INFO | train | epoch 032 | loss 2.638 | nll_loss 0.448 | ppl 1.36 | wps 6824.5 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 4157 | lr 0.000490467 | gnorm 0.521 | loss_scale 16 | train_wall 78 | gb_free 3.8 | wall 4831\n",
            "2022-08-29 07:56:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 033:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:56:10 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2022-08-29 07:56:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 129/130 [01:18<00:00,  1.66it/s, loss=2.626, nll_loss=0.438, ppl=1.36, wps=6100.1, ups=0.9, wpb=6805.6, bsz=557.3, num_updates=4200, lr=0.00048795, gnorm=0.51, loss_scale=16, train_wall=60, gb_free=4.3, wall=4858]2022-08-29 07:57:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.03s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.63s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.55s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.56s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.57s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.57s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.56s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.58s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.25s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:57:43 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 7.592 | nll_loss 6.015 | ppl 64.66 | bleu 26.87 | wps 1942.8 | wpb 2747.9 | bsz 222.2 | num_updates 4287 | best_loss 7.521\n",
            "2022-08-29 07:57:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 4287 updates\n",
            "2022-08-29 07:57:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-29 07:57:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-29 07:58:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint33.pt (epoch 33 @ 4287 updates, score 7.592) (writing took 37.27766998399966 seconds)\n",
            "2022-08-29 07:58:20 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2022-08-29 07:58:20 | INFO | train | epoch 033 | loss 2.611 | nll_loss 0.424 | ppl 1.34 | wps 6802.3 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 4287 | lr 0.000482973 | gnorm 0.509 | loss_scale 16 | train_wall 78 | gb_free 4.6 | wall 4961\n",
            "2022-08-29 07:58:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 034:   0% 0/130 [00:00<?, ?it/s]2022-08-29 07:58:20 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2022-08-29 07:58:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 129/130 [01:18<00:00,  1.68it/s, loss=2.581, nll_loss=0.397, ppl=1.32, wps=11251.4, ups=1.65, wpb=6832, bsz=546.3, num_updates=4400, lr=0.000476731, gnorm=0.469, loss_scale=16, train_wall=60, gb_free=4.2, wall=5030]2022-08-29 07:59:39 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.78s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.60s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.55s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.57s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.62s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.75s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.81s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.78s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.39s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:59:53 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 7.582 | nll_loss 6.009 | ppl 64.39 | bleu 27.3 | wps 1745.6 | wpb 2747.9 | bsz 222.2 | num_updates 4417 | best_loss 7.521\n",
            "2022-08-29 07:59:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 4417 updates\n",
            "2022-08-29 07:59:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-29 08:00:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-29 08:00:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint34.pt (epoch 34 @ 4417 updates, score 7.582) (writing took 33.97978157899979 seconds)\n",
            "2022-08-29 08:00:27 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2022-08-29 08:00:27 | INFO | train | epoch 034 | loss 2.584 | nll_loss 0.401 | ppl 1.32 | wps 6947.2 | ups 1.02 | wpb 6792.9 | bsz 538.5 | num_updates 4417 | lr 0.000475813 | gnorm 0.47 | loss_scale 16 | train_wall 78 | gb_free 4.8 | wall 5088\n",
            "2022-08-29 08:00:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 035:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:00:27 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2022-08-29 08:00:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 129/130 [01:18<00:00,  1.68it/s, loss=2.558, nll_loss=0.374, ppl=1.3, wps=6207.7, ups=0.92, wpb=6782.4, bsz=539.8, num_updates=4500, lr=0.000471405, gnorm=0.449, loss_scale=16, train_wall=60, gb_free=3.4, wall=5139]2022-08-29 08:01:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.96s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.60s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.54s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.54s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.55s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.56s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.57s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.57s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.24s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:01:59 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 7.625 | nll_loss 6.085 | ppl 67.89 | bleu 27.07 | wps 1950.5 | wpb 2747.9 | bsz 222.2 | num_updates 4547 | best_loss 7.521\n",
            "2022-08-29 08:01:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 4547 updates\n",
            "2022-08-29 08:01:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-29 08:02:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-29 08:02:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint35.pt (epoch 35 @ 4547 updates, score 7.625) (writing took 37.23118471900034 seconds)\n",
            "2022-08-29 08:02:36 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2022-08-29 08:02:36 | INFO | train | epoch 035 | loss 2.562 | nll_loss 0.382 | ppl 1.3 | wps 6823.6 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 4547 | lr 0.000468962 | gnorm 0.451 | loss_scale 16 | train_wall 78 | gb_free 5.4 | wall 5218\n",
            "2022-08-29 08:02:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 036:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:02:37 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2022-08-29 08:02:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 129/130 [01:18<00:00,  1.64it/s, loss=2.555, nll_loss=0.377, ppl=1.3, wps=6124.8, ups=0.9, wpb=6784.2, bsz=523.2, num_updates=4600, lr=0.000466252, gnorm=0.435, loss_scale=16, train_wall=60, gb_free=3.3, wall=5250]2022-08-29 08:03:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.55s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.54s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.54s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.56s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.58s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.59s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.61s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.27s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:04:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 7.595 | nll_loss 6.06 | ppl 66.72 | bleu 26.87 | wps 1890.2 | wpb 2747.9 | bsz 222.2 | num_updates 4677 | best_loss 7.521\n",
            "2022-08-29 08:04:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 4677 updates\n",
            "2022-08-29 08:04:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-29 08:04:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-29 08:04:46 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint36.pt (epoch 36 @ 4677 updates, score 7.595) (writing took 37.58958479800003 seconds)\n",
            "2022-08-29 08:04:46 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2022-08-29 08:04:46 | INFO | train | epoch 036 | loss 2.543 | nll_loss 0.367 | ppl 1.29 | wps 6811.8 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 4677 | lr 0.000462398 | gnorm 0.427 | loss_scale 16 | train_wall 78 | gb_free 3.7 | wall 5347\n",
            "2022-08-29 08:04:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 037:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:04:46 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2022-08-29 08:04:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  99% 129/130 [01:18<00:00,  1.65it/s, loss=2.537, nll_loss=0.369, ppl=1.29, wps=11123.6, ups=1.65, wpb=6729.1, bsz=538.5, num_updates=4800, lr=0.000456435, gnorm=0.44, loss_scale=16, train_wall=60, gb_free=4.8, wall=5422]2022-08-29 08:06:05 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.49s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.63s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.61s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.60s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.58s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.25s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:06:18 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 7.529 | nll_loss 5.985 | ppl 63.34 | bleu 27.45 | wps 1894.7 | wpb 2747.9 | bsz 222.2 | num_updates 4807 | best_loss 7.521\n",
            "2022-08-29 08:06:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 4807 updates\n",
            "2022-08-29 08:06:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-29 08:06:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-29 08:06:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint37.pt (epoch 37 @ 4807 updates, score 7.529) (writing took 37.73475996199977 seconds)\n",
            "2022-08-29 08:06:56 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2022-08-29 08:06:56 | INFO | train | epoch 037 | loss 2.531 | nll_loss 0.36 | ppl 1.28 | wps 6808.8 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 4807 | lr 0.000456103 | gnorm 0.428 | loss_scale 16 | train_wall 78 | gb_free 3.8 | wall 5477\n",
            "2022-08-29 08:06:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 038:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:06:56 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2022-08-29 08:06:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  99% 129/130 [01:18<00:00,  1.72it/s, loss=2.498, nll_loss=0.324, ppl=1.25, wps=6130.5, ups=0.9, wpb=6837.2, bsz=534.2, num_updates=4900, lr=0.000451754, gnorm=0.378, loss_scale=16, train_wall=60, gb_free=3.5, wall=5534]2022-08-29 08:08:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.67s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.48s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.46s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.49s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.51s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.56s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.56s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.25s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:08:27 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 7.597 | nll_loss 6.084 | ppl 67.85 | bleu 27.35 | wps 1938.3 | wpb 2747.9 | bsz 222.2 | num_updates 4937 | best_loss 7.521\n",
            "2022-08-29 08:08:27 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 4937 updates\n",
            "2022-08-29 08:08:27 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-29 08:08:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-29 08:09:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint38.pt (epoch 38 @ 4937 updates, score 7.597) (writing took 33.356165348000104 seconds)\n",
            "2022-08-29 08:09:01 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2022-08-29 08:09:01 | INFO | train | epoch 038 | loss 2.505 | nll_loss 0.335 | ppl 1.26 | wps 7050.6 | ups 1.04 | wpb 6792.9 | bsz 538.5 | num_updates 4937 | lr 0.000450058 | gnorm 0.386 | loss_scale 16 | train_wall 78 | gb_free 4.3 | wall 5602\n",
            "2022-08-29 08:09:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 039:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:09:01 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2022-08-29 08:09:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  99% 129/130 [01:18<00:00,  1.68it/s, loss=2.498, nll_loss=0.331, ppl=1.26, wps=6319.4, ups=0.93, wpb=6807.2, bsz=551.7, num_updates=5000, lr=0.000447214, gnorm=0.381, loss_scale=16, train_wall=60, gb_free=3.3, wall=5641]2022-08-29 08:10:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.76s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.54s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.52s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.53s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.55s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.66s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.64s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.65s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.30s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:10:33 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 7.561 | nll_loss 6.057 | ppl 66.56 | bleu 27.31 | wps 1864 | wpb 2747.9 | bsz 222.2 | num_updates 5067 | best_loss 7.521\n",
            "2022-08-29 08:10:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 5067 updates\n",
            "2022-08-29 08:10:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-29 08:10:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-29 08:11:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint39.pt (epoch 39 @ 5067 updates, score 7.561) (writing took 36.88368213400008 seconds)\n",
            "2022-08-29 08:11:10 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2022-08-29 08:11:10 | INFO | train | epoch 039 | loss 2.493 | nll_loss 0.328 | ppl 1.26 | wps 6827 | ups 1.01 | wpb 6792.9 | bsz 538.5 | num_updates 5067 | lr 0.000444247 | gnorm 0.374 | loss_scale 16 | train_wall 78 | gb_free 4.7 | wall 5732\n",
            "2022-08-29 08:11:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 040:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:11:11 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2022-08-29 08:11:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  99% 129/130 [01:18<00:00,  1.68it/s, loss=2.494, nll_loss=0.331, ppl=1.26, wps=6085.3, ups=0.9, wpb=6730.4, bsz=529.4, num_updates=5100, lr=0.000442807, gnorm=0.376, loss_scale=16, train_wall=59, gb_free=3.4, wall=5752]2022-08-29 08:12:29 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.73s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.53s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.53s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.54s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.55s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.57s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.58s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.61s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.27s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:12:43 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 7.63 | nll_loss 6.136 | ppl 70.32 | bleu 27.09 | wps 1901.5 | wpb 2747.9 | bsz 222.2 | num_updates 5197 | best_loss 7.521\n",
            "2022-08-29 08:12:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 5197 updates\n",
            "2022-08-29 08:12:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-29 08:12:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-29 08:13:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint40.pt (epoch 40 @ 5197 updates, score 7.63) (writing took 36.39218336700014 seconds)\n",
            "2022-08-29 08:13:19 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2022-08-29 08:13:19 | INFO | train | epoch 040 | loss 2.482 | nll_loss 0.321 | ppl 1.25 | wps 6865.6 | ups 1.01 | wpb 6792.9 | bsz 538.5 | num_updates 5197 | lr 0.000438656 | gnorm 0.367 | loss_scale 16 | train_wall 78 | gb_free 5.3 | wall 5860\n",
            "2022-08-29 08:13:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 041:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:13:19 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2022-08-29 08:13:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  99% 129/130 [01:18<00:00,  1.75it/s, loss=2.464, nll_loss=0.303, ppl=1.23, wps=11210.1, ups=1.64, wpb=6840.5, bsz=544.6, num_updates=5300, lr=0.000434372, gnorm=0.354, loss_scale=16, train_wall=60, gb_free=3.4, wall=5923]2022-08-29 08:14:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.67s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.50s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.52s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.57s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.57s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.69s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.33s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:14:51 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 7.574 | nll_loss 6.08 | ppl 67.63 | bleu 27.54 | wps 1866.5 | wpb 2747.9 | bsz 222.2 | num_updates 5327 | best_loss 7.521\n",
            "2022-08-29 08:14:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 5327 updates\n",
            "2022-08-29 08:14:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-29 08:15:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-29 08:15:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint41.pt (epoch 41 @ 5327 updates, score 7.574) (writing took 36.419425047000004 seconds)\n",
            "2022-08-29 08:15:28 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2022-08-29 08:15:28 | INFO | train | epoch 041 | loss 2.47 | nll_loss 0.312 | ppl 1.24 | wps 6854.4 | ups 1.01 | wpb 6792.9 | bsz 538.5 | num_updates 5327 | lr 0.00043327 | gnorm 0.358 | loss_scale 16 | train_wall 78 | gb_free 4.8 | wall 5989\n",
            "2022-08-29 08:15:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 042:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:15:28 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2022-08-29 08:15:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  99% 129/130 [01:18<00:00,  1.64it/s, loss=2.461, nll_loss=0.304, ppl=1.23, wps=6120.3, ups=0.91, wpb=6749.3, bsz=525.4, num_updates=5400, lr=0.000430331, gnorm=0.343, loss_scale=16, train_wall=60, gb_free=4, wall=6034]2022-08-29 08:16:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.71s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.51s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.50s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.53s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.57s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.56s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.25s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:17:00 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 7.668 | nll_loss 6.179 | ppl 72.47 | bleu 27.17 | wps 1929.8 | wpb 2747.9 | bsz 222.2 | num_updates 5457 | best_loss 7.521\n",
            "2022-08-29 08:17:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 5457 updates\n",
            "2022-08-29 08:17:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-29 08:17:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-29 08:17:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint42.pt (epoch 42 @ 5457 updates, score 7.668) (writing took 33.09815827000057 seconds)\n",
            "2022-08-29 08:17:33 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2022-08-29 08:17:33 | INFO | train | epoch 042 | loss 2.462 | nll_loss 0.308 | ppl 1.24 | wps 7057.1 | ups 1.04 | wpb 6792.9 | bsz 538.5 | num_updates 5457 | lr 0.000428078 | gnorm 0.362 | loss_scale 16 | train_wall 78 | gb_free 3.9 | wall 6114\n",
            "2022-08-29 08:17:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 043:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:17:33 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2022-08-29 08:17:33 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  99% 129/130 [01:18<00:00,  1.76it/s, loss=2.462, nll_loss=0.31, ppl=1.24, wps=6385.4, ups=0.93, wpb=6835, bsz=552.6, num_updates=5500, lr=0.000426401, gnorm=0.371, loss_scale=16, train_wall=60, gb_free=4, wall=6141]2022-08-29 08:18:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.65s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.46s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.45s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.49s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.51s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.52s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.53s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.56s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.22s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:19:05 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 7.653 | nll_loss 6.163 | ppl 71.68 | bleu 27.36 | wps 1971.7 | wpb 2747.9 | bsz 222.2 | num_updates 5587 | best_loss 7.521\n",
            "2022-08-29 08:19:05 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 5587 updates\n",
            "2022-08-29 08:19:05 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-29 08:19:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-29 08:19:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint43.pt (epoch 43 @ 5587 updates, score 7.653) (writing took 36.19011718399997 seconds)\n",
            "2022-08-29 08:19:41 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2022-08-29 08:19:41 | INFO | train | epoch 043 | loss 2.454 | nll_loss 0.302 | ppl 1.23 | wps 6897.9 | ups 1.02 | wpb 6792.9 | bsz 538.5 | num_updates 5587 | lr 0.000423068 | gnorm 0.339 | loss_scale 16 | train_wall 78 | gb_free 4.1 | wall 6242\n",
            "2022-08-29 08:19:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 044:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:19:41 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2022-08-29 08:19:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  99% 129/130 [01:18<00:00,  1.67it/s, loss=2.441, nll_loss=0.293, ppl=1.22, wps=11260.4, ups=1.65, wpb=6812.8, bsz=538.6, num_updates=5700, lr=0.000418854, gnorm=0.333, loss_scale=16, train_wall=60, gb_free=3.3, wall=6311]2022-08-29 08:21:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.08s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.69s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.60s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.62s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.62s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.63s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.64s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.66s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.30s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:21:14 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 7.607 | nll_loss 6.126 | ppl 69.84 | bleu 27.55 | wps 1860.3 | wpb 2747.9 | bsz 222.2 | num_updates 5717 | best_loss 7.521\n",
            "2022-08-29 08:21:14 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 5717 updates\n",
            "2022-08-29 08:21:14 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-29 08:21:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-29 08:21:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint44.pt (epoch 44 @ 5717 updates, score 7.607) (writing took 35.8299811199995 seconds)\n",
            "2022-08-29 08:21:50 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2022-08-29 08:21:50 | INFO | train | epoch 044 | loss 2.444 | nll_loss 0.296 | ppl 1.23 | wps 6871.5 | ups 1.01 | wpb 6792.9 | bsz 538.5 | num_updates 5717 | lr 0.000418231 | gnorm 0.342 | loss_scale 16 | train_wall 78 | gb_free 3.9 | wall 6371\n",
            "2022-08-29 08:21:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 045:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:21:50 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2022-08-29 08:21:50 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  99% 129/130 [01:18<00:00,  1.68it/s, loss=2.436, nll_loss=0.288, ppl=1.22, wps=6173.1, ups=0.9, wpb=6840.3, bsz=543.2, num_updates=5800, lr=0.000415227, gnorm=0.335, loss_scale=16, train_wall=60, gb_free=4.1, wall=6422]2022-08-29 08:23:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.93s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.84s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  33% 3/9 [00:05<00:11,  1.87s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  44% 4/9 [00:07<00:08,  1.80s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.76s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.74s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.73s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.74s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.36s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:23:23 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 7.557 | nll_loss 6.083 | ppl 67.81 | bleu 27.65 | wps 1706.3 | wpb 2747.9 | bsz 222.2 | num_updates 5847 | best_loss 7.521\n",
            "2022-08-29 08:23:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 5847 updates\n",
            "2022-08-29 08:23:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-29 08:23:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-29 08:24:00 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint45.pt (epoch 45 @ 5847 updates, score 7.557) (writing took 36.66713999299918 seconds)\n",
            "2022-08-29 08:24:00 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2022-08-29 08:24:00 | INFO | train | epoch 045 | loss 2.437 | nll_loss 0.291 | ppl 1.22 | wps 6785.8 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 5847 | lr 0.000413555 | gnorm 0.334 | loss_scale 16 | train_wall 78 | gb_free 4.6 | wall 6501\n",
            "2022-08-29 08:24:00 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 046:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:24:00 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2022-08-29 08:24:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  99% 129/130 [01:18<00:00,  1.69it/s, loss=2.438, nll_loss=0.294, ppl=1.23, wps=5997.7, ups=0.9, wpb=6698.4, bsz=550.1, num_updates=5900, lr=0.000411693, gnorm=0.35, loss_scale=16, train_wall=59, gb_free=4.1, wall=6533]2022-08-29 08:25:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.78s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.54s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.52s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.65s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.61s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.61s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.61s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.72s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.39s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:25:32 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 7.584 | nll_loss 6.118 | ppl 69.47 | bleu 27.89 | wps 1799.3 | wpb 2747.9 | bsz 222.2 | num_updates 5977 | best_loss 7.521\n",
            "2022-08-29 08:25:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 5977 updates\n",
            "2022-08-29 08:25:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-29 08:25:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-29 08:26:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint46.pt (epoch 46 @ 5977 updates, score 7.584) (writing took 33.99734882099983 seconds)\n",
            "2022-08-29 08:26:07 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2022-08-29 08:26:07 | INFO | train | epoch 046 | loss 2.433 | nll_loss 0.29 | ppl 1.22 | wps 6962.6 | ups 1.02 | wpb 6792.9 | bsz 538.5 | num_updates 5977 | lr 0.000409033 | gnorm 0.333 | loss_scale 16 | train_wall 78 | gb_free 5 | wall 6628\n",
            "2022-08-29 08:26:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 047:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:26:07 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2022-08-29 08:26:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  99% 129/130 [01:18<00:00,  1.67it/s, loss=2.425, nll_loss=0.286, ppl=1.22, wps=11290.5, ups=1.65, wpb=6843.6, bsz=540.3, num_updates=6100, lr=0.000404888, gnorm=0.318, loss_scale=16, train_wall=60, gb_free=3.6, wall=6703]2022-08-29 08:27:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.49s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.53s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.55s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.56s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.26s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:27:39 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 7.613 | nll_loss 6.153 | ppl 71.17 | bleu 27.33 | wps 1928.4 | wpb 2747.9 | bsz 222.2 | num_updates 6107 | best_loss 7.521\n",
            "2022-08-29 08:27:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 6107 updates\n",
            "2022-08-29 08:27:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-29 08:27:53 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-29 08:28:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint47.pt (epoch 47 @ 6107 updates, score 7.613) (writing took 38.3571088510007 seconds)\n",
            "2022-08-29 08:28:17 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2022-08-29 08:28:17 | INFO | train | epoch 047 | loss 2.422 | nll_loss 0.281 | ppl 1.21 | wps 6765.8 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 6107 | lr 0.000404656 | gnorm 0.314 | loss_scale 16 | train_wall 78 | gb_free 3.9 | wall 6758\n",
            "2022-08-29 08:28:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 048:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:28:17 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2022-08-29 08:28:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  99% 129/130 [01:18<00:00,  1.69it/s, loss=2.411, nll_loss=0.27, ppl=1.21, wps=6077.9, ups=0.89, wpb=6833.7, bsz=546.6, num_updates=6200, lr=0.00040161, gnorm=0.304, loss_scale=16, train_wall=60, gb_free=4.8, wall=6815]2022-08-29 08:29:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.75s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.56s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.54s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.56s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.58s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  67% 6/9 [00:09<00:05,  1.70s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.70s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.71s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.34s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:29:50 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 7.614 | nll_loss 6.158 | ppl 71.43 | bleu 27.51 | wps 1808.7 | wpb 2747.9 | bsz 222.2 | num_updates 6237 | best_loss 7.521\n",
            "2022-08-29 08:29:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 6237 updates\n",
            "2022-08-29 08:29:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-29 08:30:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-29 08:30:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint48.pt (epoch 48 @ 6237 updates, score 7.614) (writing took 37.57382692700048 seconds)\n",
            "2022-08-29 08:30:27 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2022-08-29 08:30:27 | INFO | train | epoch 048 | loss 2.418 | nll_loss 0.279 | ppl 1.21 | wps 6781.5 | ups 1 | wpb 6792.9 | bsz 538.5 | num_updates 6237 | lr 0.000400417 | gnorm 0.314 | loss_scale 16 | train_wall 78 | gb_free 4.5 | wall 6889\n",
            "2022-08-29 08:30:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 049:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:30:27 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2022-08-29 08:30:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  99% 129/130 [01:18<00:00,  1.68it/s, loss=2.417, nll_loss=0.278, ppl=1.21, wps=6008.3, ups=0.9, wpb=6684.3, bsz=520.5, num_updates=6300, lr=0.00039841, gnorm=0.319, loss_scale=16, train_wall=59, gb_free=4.6, wall=6927]2022-08-29 08:31:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.72s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.52s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.52s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.53s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.55s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.56s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.57s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.27s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:31:59 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 7.651 | nll_loss 6.207 | ppl 73.88 | bleu 27.32 | wps 1908.4 | wpb 2747.9 | bsz 222.2 | num_updates 6367 | best_loss 7.521\n",
            "2022-08-29 08:31:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 6367 updates\n",
            "2022-08-29 08:31:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-29 08:32:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-29 08:32:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint49.pt (epoch 49 @ 6367 updates, score 7.651) (writing took 37.00022012499994 seconds)\n",
            "2022-08-29 08:32:36 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2022-08-29 08:32:36 | INFO | train | epoch 049 | loss 2.416 | nll_loss 0.279 | ppl 1.21 | wps 6850.6 | ups 1.01 | wpb 6792.9 | bsz 538.5 | num_updates 6367 | lr 0.000396308 | gnorm 0.316 | loss_scale 16 | train_wall 78 | gb_free 4.2 | wall 7018\n",
            "2022-08-29 08:32:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 130\n",
            "epoch 050:   0% 0/130 [00:00<?, ?it/s]2022-08-29 08:32:36 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2022-08-29 08:32:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  99% 129/130 [01:18<00:00,  1.71it/s, loss=2.414, nll_loss=0.281, ppl=1.21, wps=6144.5, ups=0.9, wpb=6842, bsz=561, num_updates=6400, lr=0.000395285, gnorm=0.31, loss_scale=16, train_wall=60, gb_free=3.9, wall=7038]2022-08-29 08:33:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.71s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.51s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.54s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.57s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.58s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.61s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.27s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:34:08 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 7.621 | nll_loss 6.177 | ppl 72.37 | bleu 27.31 | wps 1913.3 | wpb 2747.9 | bsz 222.2 | num_updates 6497 | best_loss 7.521\n",
            "2022-08-29 08:34:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 6497 updates\n",
            "2022-08-29 08:34:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-29 08:34:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-29 08:34:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint50.pt (epoch 50 @ 6497 updates, score 7.621) (writing took 33.71283264199974 seconds)\n",
            "2022-08-29 08:34:42 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2022-08-29 08:34:42 | INFO | train | epoch 050 | loss 2.406 | nll_loss 0.271 | ppl 1.21 | wps 7015.4 | ups 1.03 | wpb 6792.9 | bsz 538.5 | num_updates 6497 | lr 0.000392323 | gnorm 0.297 | loss_scale 16 | train_wall 78 | gb_free 3.8 | wall 7143\n",
            "2022-08-29 08:34:42 | INFO | fairseq_cli.train | done training in 7143.6 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:16:00.904977Z",
          "iopub.execute_input": "2022-07-24T19:16:00.905444Z",
          "iopub.status.idle": "2022-07-24T19:16:01.196986Z",
          "shell.execute_reply.started": "2022-07-24T19:16:00.905403Z",
          "shell.execute_reply": "2022-07-24T19:16:01.195842Z"
        },
        "trusted": true,
        "id": "yL7nIungdwcw",
        "outputId": "ddbc667a-deea-4f91-a98c-482006f6b1ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdata-bin\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/drive/valid_hsb-de.de --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang de --target-lang hsb \\\n",
        "      --beam 5 data-bin/wmt_de_hsb | grep -P \"D-[0-9]+\" | cut -f3 > target.txt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:05:22.918437Z",
          "iopub.execute_input": "2022-07-24T18:05:22.919517Z",
          "iopub.status.idle": "2022-07-24T18:06:13.344631Z",
          "shell.execute_reply.started": "2022-07-24T18:05:22.919454Z",
          "shell.execute_reply": "2022-07-24T18:06:13.343313Z"
        },
        "trusted": true,
        "id": "PdGwekMXdwcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7f46f75-9f11-4230-dbf8-bda9192f700c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 08:34:57 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/drive/valid_hsb-de.de'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_de_hsb', 'source_lang': 'de', 'target_lang': 'hsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-29 08:34:57 | INFO | fairseq.tasks.translation | [de] dictionary: 105688 types\n",
            "2022-08-29 08:34:57 | INFO | fairseq.tasks.translation | [hsb] dictionary: 136232 types\n",
            "2022-08-29 08:34:57 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-29 08:35:22 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-29 08:35:22 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-29 08:35:22 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-29 08:35:45 | INFO | fairseq_cli.interactive | Total time: 47.996 seconds; translation time: 17.980\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu target.txt -i /content/drive/valid_hsb-de.hsb -l hsb-de -m bleu chrf ter)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:16.590743Z",
          "iopub.execute_input": "2022-07-24T18:06:16.591679Z",
          "iopub.status.idle": "2022-07-24T18:06:19.209461Z",
          "shell.execute_reply.started": "2022-07-24T18:06:16.591625Z",
          "shell.execute_reply": "2022-07-24T18:06:19.208405Z"
        },
        "trusted": true,
        "id": "bqOJKYxXdwc2",
        "outputId": "04c2dfe5-eb75-4a74-bded-d6ebea82991f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 25.1,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\",\n",
            " \"verbose_score\": \"57.0/31.3/19.1/11.7 (BP = 1.000 ratio = 1.054 hyp_len = 26127 ref_len = 24777)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"chrF2\",\n",
            " \"score\": 53.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"yes\",\n",
            " \"nc\": \"6\",\n",
            " \"nw\": \"0\",\n",
            " \"space\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"TER\",\n",
            " \"score\": 57.0,\n",
            " \"signature\": \"nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"lc\",\n",
            " \"tok\": \"tercom\",\n",
            " \"norm\": \"no\",\n",
            " \"punct\": \"yes\",\n",
            " \"asian\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "}\n",
            "]\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading blind test data\n",
        "!gdown --id 1HSj-yhWa3jP1xGnmf-LdNGGcsAg-uNj3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD0Zhyg5Mmtd",
        "outputId": "e250490f-563e-46d7-b03b-1992f4aad974"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HSj-yhWa3jP1xGnmf-LdNGGcsAg-uNj3\n",
            "To: /content/test_DE-HSB.de_src.txt\n",
            "100% 142k/142k [00:00<00:00, 92.4MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/test_DE-HSB.de_src.txt --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang de --target-lang hsb \\\n",
        "      --beam 5 data-bin/wmt_de_hsb | grep -P \"D-[0-9]+\" | cut -f3 > /content/gdrive/MyDrive/target_de-hsb_sup_submission.txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr0yH7NZUF1B",
        "outputId": "0c093d97-3895-478a-c592-c5ca3b1bbf51"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 08:35:53 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/test_DE-HSB.de_src.txt'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_de_hsb', 'source_lang': 'de', 'target_lang': 'hsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-29 08:35:53 | INFO | fairseq.tasks.translation | [de] dictionary: 105688 types\n",
            "2022-08-29 08:35:53 | INFO | fairseq.tasks.translation | [hsb] dictionary: 136232 types\n",
            "2022-08-29 08:35:53 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-29 08:36:02 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-29 08:36:02 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-29 08:36:02 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-29 08:36:19 | INFO | fairseq_cli.interactive | Total time: 25.943 seconds; translation time: 12.867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "id": "9Zw2XTYgbzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu /content/gdrive/MyDrive/target_dsb-hsb_sup_submission.txt -i /content/test_dsb_hsb.dsb_src.txt -l dsb-hsb -m bleu chrf ter)"
      ],
      "metadata": {
        "id": "7LNPRlDQb4ER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}