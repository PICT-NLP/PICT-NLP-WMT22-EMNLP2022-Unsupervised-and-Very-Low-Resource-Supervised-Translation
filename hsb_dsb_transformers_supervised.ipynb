{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "hsb_dsb_transformers_supervised.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# create a seperate folder to store everything\n",
        "# !mkdir wmt\n",
        "# %cd wmt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:45.876120Z",
          "iopub.execute_input": "2022-07-24T19:01:45.876582Z",
          "iopub.status.idle": "2022-07-24T19:01:46.194317Z",
          "shell.execute_reply.started": "2022-07-24T19:01:45.876499Z",
          "shell.execute_reply": "2022-07-24T19:01:46.192944Z"
        },
        "trusted": true,
        "id": "J9o_gWJIdwcc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BFmSadnVoSa",
        "outputId": "8c74b38b-eefb-4b31-bb2d-be477fdf61d2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tree\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:47.842831Z",
          "iopub.execute_input": "2022-07-24T19:01:47.843475Z",
          "iopub.status.idle": "2022-07-24T19:03:13.957220Z",
          "shell.execute_reply.started": "2022-07-24T19:01:47.843443Z",
          "shell.execute_reply": "2022-07-24T19:03:13.955881Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sULkb42Jdwci",
        "outputId": "f0cf4ba5-7ea5-4717-a2b0-d861c9777589"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 0s (281 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 5.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 57.2 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 61.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 49.6 MB/s \n",
            "\u001b[?25hCollecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=2af4927a261be20752ec360e2c65740ecc64386570c9685de6bd8d8855f2feff\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, portalocker, morfessor, colorama, tensorboardX, sacremoses, sacrebleu, mock, indic-nlp-library\n",
            "Successfully installed colorama-0.4.5 indic-nlp-library-0.81 mock-4.0.3 morfessor-2.0.6 portalocker-2.5.1 sacrebleu-2.2.0 sacremoses-0.0.53 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0 tensorboardX-2.5.1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 32239, done.\u001b[K\n",
            "remote: Total 32239 (delta 0), reused 0 (delta 0), pack-reused 32239\u001b[K\n",
            "Receiving objects: 100% (32239/32239), 22.42 MiB | 29.07 MiB/s, done.\n",
            "Resolving deltas: 100% (23642/23642), done.\n",
            "/content/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.32)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.1+cu113)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 58.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.1+cu113)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.9.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 62.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.5.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.8.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=7c574218a4edce769547e806e763b0c711740bef8a198781fdd6e1eb31618ff8\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 fairseq hydra-core-1.0.7 omegaconf-2.0.6\n",
            "/content\n",
            "/env/python\n",
            "/env/python:/content/fairseq/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:13.960095Z",
          "iopub.execute_input": "2022-07-24T19:03:13.960531Z",
          "iopub.status.idle": "2022-07-24T19:03:14.252882Z",
          "shell.execute_reply.started": "2022-07-24T19:03:13.960500Z",
          "shell.execute_reply": "2022-07-24T19:03:14.251592Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XleJnPzidwck",
        "outputId": "9cb4641e-efa9-4374-b65d-9471d824e72d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U --no-cache-dir gdown --pre"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:14.254120Z",
          "iopub.execute_input": "2022-07-24T19:03:14.254498Z",
          "iopub.status.idle": "2022-07-24T19:03:36.192512Z",
          "shell.execute_reply.started": "2022-07-24T19:03:14.254462Z",
          "shell.execute_reply": "2022-07-24T19:03:36.191399Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIuDbtOgdwcm",
        "outputId": "7ba196f0-7a43-4817-854e-2f63fd1e0824"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=f139715313c7805e23bc34e7a2c812a0da361f0e1bc1f916c9ab199cf5c974e3\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-oo_typo7/wheels/3d/ec/b0/a96d1d126183f98570a785e6bf8789fca559853a9260e928e1\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNjo_Q8_IlXz",
        "outputId": "f1ea51d3-a20b-4900-9b1e-40f9a8f3d39b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir drive\n",
        "%cd drive"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.194917Z",
          "iopub.execute_input": "2022-07-24T19:03:36.195189Z",
          "iopub.status.idle": "2022-07-24T19:03:36.494103Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.195162Z",
          "shell.execute_reply": "2022-07-24T19:03:36.492910Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AopzC4DBdwcn",
        "outputId": "a26ad406-1d2a-4d2d-cb2b-8b763eb2bf48"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1P3l5qdyrNA7KTGi0rEzJidd7ab9DnrE7 --folder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.495274Z",
          "iopub.execute_input": "2022-07-24T19:03:36.495519Z",
          "iopub.status.idle": "2022-07-24T19:03:47.662284Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.495496Z",
          "shell.execute_reply": "2022-07-24T19:03:47.661258Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa1K1LT9dwcn",
        "outputId": "18efd804-c94e-48c8-e0c7-c4600260257e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Retrieving folder list\n",
            "Retrieving folder 1skvX_-rLx9Stl-XCBi-yfE3b6bwp0SB1 valid_dsb-hsb\n",
            "Processing file 1CCavbsyYo971QuR_TdxtfAEmHkC-80Zi dev_dsb-hsb.dsb\n",
            "Processing file 1-BafAQg_d_HN315YusYGzaJ9h_Grgk0P dev_dsb-hsb.hsb\n",
            "Processing file 1j6wZ9H9Tn0jChyDJjs127Th8r1cgGHJQ valid_dsb-hsb.dsb\n",
            "Processing file 1sAS6023wb9CUv4hv7qPktttBDTSLbvgr valid_dsb-hsb.hsb\n",
            "Processing file 1JHJJeoChv-kco5ICc9v2eTeiRk7YaUMu train_dsb-hsb.dsb\n",
            "Processing file 1z1t2yHGYZ1eF0LubUQyaFudeOBpBgUiD train_dsb-hsb.hsb\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1CCavbsyYo971QuR_TdxtfAEmHkC-80Zi\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/dev_dsb-hsb.dsb\n",
            "100% 56.6k/56.6k [00:00<00:00, 87.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-BafAQg_d_HN315YusYGzaJ9h_Grgk0P\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/dev_dsb-hsb.hsb\n",
            "100% 55.2k/55.2k [00:00<00:00, 90.8MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1j6wZ9H9Tn0jChyDJjs127Th8r1cgGHJQ\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb\n",
            "100% 31.6k/31.6k [00:00<00:00, 62.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1sAS6023wb9CUv4hv7qPktttBDTSLbvgr\n",
            "To: /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb\n",
            "100% 30.9k/30.9k [00:00<00:00, 47.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JHJJeoChv-kco5ICc9v2eTeiRk7YaUMu\n",
            "To: /content/drive/dsb-hsb/train_dsb-hsb.dsb\n",
            "100% 4.97M/4.97M [00:00<00:00, 24.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1z1t2yHGYZ1eF0LubUQyaFudeOBpBgUiD\n",
            "To: /content/drive/dsb-hsb/train_dsb-hsb.hsb\n",
            "100% 4.85M/4.85M [00:00<00:00, 61.7MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUdiQyJdJBdt",
        "outputId": "4cdaf215-d511-4290-b830-31872f93bb1b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:47.663751Z",
          "iopub.execute_input": "2022-07-24T19:03:47.664065Z",
          "iopub.status.idle": "2022-07-24T19:03:47.671365Z",
          "shell.execute_reply.started": "2022-07-24T19:03:47.664036Z",
          "shell.execute_reply": "2022-07-24T19:03:47.670044Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXt8nnHHdwco",
        "outputId": "d6e75851-1107-42e1-ed80-1e95b2eb568d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H751-LW3XII0",
        "outputId": "6c10d04f-be49-416f-eed2-8d67c6dc775b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:04:11.972219Z",
          "iopub.execute_input": "2022-07-24T19:04:11.972913Z",
          "iopub.status.idle": "2022-07-24T19:04:12.256665Z",
          "shell.execute_reply.started": "2022-07-24T19:04:11.972888Z",
          "shell.execute_reply": "2022-07-24T19:04:12.255254Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ihwjB6Gdwcp",
        "outputId": "b7c19ec0-a69b-453d-8f89-88bb1dba95e0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdsb-hsb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8F3ZpFtX6Vo",
        "outputId": "5efda51b-99a7-4f08-9cda-cc972e506aeb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-preprocess \\\n",
        "    --source-lang hsb --target-lang dsb \\\n",
        "    --trainpref drive/dsb-hsb/train_dsb-hsb --validpref drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb \\\n",
        "    --destdir data-bin/wmt_hsb_dsb --thresholdtgt 0 --thresholdsrc 0 \\\n",
        "    --workers 20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:14:48.909980Z",
          "iopub.execute_input": "2022-07-24T19:14:48.910354Z",
          "iopub.status.idle": "2022-07-24T19:15:18.024931Z",
          "shell.execute_reply.started": "2022-07-24T19:14:48.910330Z",
          "shell.execute_reply": "2022-07-24T19:15:18.024044Z"
        },
        "trusted": true,
        "id": "XxPJ-2-zdwcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80885f97-127b-46b2-d15e-909b7e81e8ff"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 20:47:55 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wmt_hsb_dsb', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='hsb', srcdict=None, suppress_crashes=False, target_lang='dsb', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='drive/dsb-hsb/train_dsb-hsb', use_plasma_view=False, user_dir=None, validpref='drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb', wandb_project=None, workers=20)\n",
            "2022-08-28 20:48:07 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 103512 types\n",
            "2022-08-28 20:48:18 | INFO | fairseq_cli.preprocess | [hsb] drive/dsb-hsb/train_dsb-hsb.hsb: 62565 sents, 716965 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-28 20:48:18 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 103512 types\n",
            "2022-08-28 20:48:20 | INFO | fairseq_cli.preprocess | [hsb] drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb: 709 sents, 5248 tokens, 18.1% replaced (by <unk>)\n",
            "2022-08-28 20:48:20 | INFO | fairseq_cli.preprocess | [dsb] Dictionary: 104024 types\n",
            "2022-08-28 20:48:30 | INFO | fairseq_cli.preprocess | [dsb] drive/dsb-hsb/train_dsb-hsb.dsb: 62565 sents, 736316 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-28 20:48:30 | INFO | fairseq_cli.preprocess | [dsb] Dictionary: 104024 types\n",
            "2022-08-28 20:48:33 | INFO | fairseq_cli.preprocess | [dsb] drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb: 709 sents, 5301 tokens, 18.7% replaced (by <unk>)\n",
            "2022-08-28 20:48:33 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt_hsb_dsb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uol2ncymJWFr",
        "outputId": "b295d5ba-82a8-4b0f-d6e2-060ac7a7b56c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 62.2 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 64.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 53.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 59.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 64.6 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 65.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 61.9 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f0594a3400cdcc09288eaf6f9f3cff21071c2e14f885ca959c1032669fc243b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"wmt2022_hsb-dsb_transformers_supervised\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:18.026892Z",
          "iopub.execute_input": "2022-07-24T19:15:18.027225Z",
          "iopub.status.idle": "2022-07-24T19:15:26.733355Z",
          "shell.execute_reply.started": "2022-07-24T19:15:18.027200Z",
          "shell.execute_reply": "2022-07-24T19:15:26.732531Z"
        },
        "trusted": true,
        "id": "hoUXe8Kndwcr",
        "outputId": "eaaf5c2d-0214-4cbd-bd39-59ece0e84bf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220828_204854-2zjmoguq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/rahul2023_usa/wmt2022_hsb-dsb_transformers_supervised/runs/2zjmoguq\" target=\"_blank\">leafy-silence-4</a></strong> to <a href=\"https://wandb.ai/rahul2023_usa/wmt2022_hsb-dsb_transformers_supervised\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rahul2023_usa/wmt2022_hsb-dsb_transformers_supervised/runs/2zjmoguq?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f808f58c550>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Finetuning the model\n",
        "\n",
        "# # pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
        "\n",
        "\n",
        "# # some notable args:\n",
        "# # --max-update=1000     -> for this example, to demonstrate how to finetune we are only training for 1000 steps. You should increase this when finetuning\n",
        "# # --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# # --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# # --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# # --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\n",
        "# # --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\n",
        "# # --max_tokns           -> this is max tokens per batch\n",
        "\n",
        "\n",
        "# !( fairseq-train data-bin/wmt_dsb_de \\\n",
        "# --max-source-positions=210 \\\n",
        "# --max-target-positions=210 \\\n",
        "# --max-update=1000 \\\n",
        "# --save-interval=1 \\\n",
        "# --arch=transformer \\\n",
        "# --criterion=label_smoothed_cross_entropy \\\n",
        "# --source-lang=dsb \\\n",
        "# --lr-scheduler=inverse_sqrt \\\n",
        "# --target-lang=de \\\n",
        "# --label-smoothing=0.1 \\\n",
        "# --optimizer adam \\\n",
        "# --adam-betas \"(0.9, 0.98)\" \\\n",
        "# --clip-norm 1.0 \\\n",
        "# --warmup-init-lr 1e-07 \\\n",
        "# --warmup-updates 4000 \\\n",
        "# --dropout 0.2 \\\n",
        "# --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "# --save-dir checkpoints/model \\\n",
        "# --keep-last-epochs 5 \\\n",
        "# --patience 5 \\\n",
        "# --skip-invalid-size-inputs-valid-test \\\n",
        "# --fp16 \\\n",
        "# --update-freq=2 \\\n",
        "# --distributed-world-size 1 \\\n",
        "# --max-tokens 1024 \\\n",
        "# --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "# --lr 5e-4 \\\n",
        "# --reset-lr-scheduler \\\n",
        "# --reset-meters \\\n",
        "# --reset-dataloader \\\n",
        "# --reset-optimizer \\\n",
        "# --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w4cGb93Bdwct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!( fairseq-train data-bin/wmt_hsb_dsb \\\n",
        "    --arch transformer \\\n",
        "    --max-epoch=50 \\\n",
        "    --criterion=label_smoothed_cross_entropy \\\n",
        "    --source-lang=hsb \\\n",
        "    --lr-scheduler=inverse_sqrt \\\n",
        "    --target-lang=dsb \\\n",
        "    --label-smoothing=0.1 \\\n",
        "    --optimizer adam \\\n",
        "    --adam-betas \"(0.9, 0.98)\" \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --dropout 0.2 \\\n",
        "    --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "    --wandb-project 'wmt2022_hsb-dsb_transformers_supervised' \\\n",
        "    --save-dir checkpoints/model \\\n",
        "    --keep-last-epochs 5 \\\n",
        "    --fp16 \\\n",
        "    --update-freq=2 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --lr 5e-4 \\\n",
        "    --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "    --reset-lr-scheduler \\\n",
        "    --reset-meters \\\n",
        "    --reset-dataloader \\\n",
        "    --reset-optimizer \\\n",
        "    --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:42.672724Z",
          "iopub.execute_input": "2022-07-24T19:15:42.673143Z",
          "iopub.status.idle": "2022-07-24T19:16:00.902554Z",
          "shell.execute_reply.started": "2022-07-24T19:15:42.673112Z",
          "shell.execute_reply": "2022-07-24T19:16:00.901385Z"
        },
        "trusted": true,
        "id": "U2IouMGudwcv",
        "outputId": "7a85a081-9975-4e5d-be65-c8ff88e99c9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 20:50:06 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../../../tmp/tensorboard-wandb', 'wandb_project': 'wmt2022_hsb-dsb_transformers_supervised', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/model', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wmt_hsb_dsb', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=True, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=50, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='checkpoints/model', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='hsb', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='dsb', task='translation', tensorboard_logdir='../../../tmp/tensorboard-wandb', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='wmt2022_hsb-dsb_transformers_supervised', warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/wmt_hsb_dsb', 'source_lang': 'hsb', 'target_lang': 'dsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 20:50:06 | INFO | fairseq.tasks.translation | [hsb] dictionary: 103512 types\n",
            "2022-08-28 20:50:06 | INFO | fairseq.tasks.translation | [dsb] dictionary: 104024 types\n",
            "2022-08-28 20:50:09 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(103512, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(104024, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=104024, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-08-28 20:50:09 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-08-28 20:50:09 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-08-28 20:50:09 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-08-28 20:50:09 | INFO | fairseq_cli.train | num. shared model params: 203,657,216 (num. trained: 203,657,216)\n",
            "2022-08-28 20:50:09 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-08-28 20:50:09 | INFO | fairseq.data.data_utils | loaded 709 examples from: data-bin/wmt_hsb_dsb/valid.hsb-dsb.hsb\n",
            "2022-08-28 20:50:09 | INFO | fairseq.data.data_utils | loaded 709 examples from: data-bin/wmt_hsb_dsb/valid.hsb-dsb.dsb\n",
            "2022-08-28 20:50:09 | INFO | fairseq.tasks.translation | data-bin/wmt_hsb_dsb valid hsb-dsb 709 examples\n",
            "2022-08-28 20:50:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-28 20:50:13 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-08-28 20:50:13 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-28 20:50:13 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-08-28 20:50:13 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-08-28 20:50:13 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/model/checkpoint_last.pt\n",
            "2022-08-28 20:50:13 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/model/checkpoint_last.pt\n",
            "2022-08-28 20:50:13 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-08-28 20:50:13 | INFO | fairseq.data.data_utils | loaded 62,565 examples from: data-bin/wmt_hsb_dsb/train.hsb-dsb.hsb\n",
            "2022-08-28 20:50:13 | INFO | fairseq.data.data_utils | loaded 62,565 examples from: data-bin/wmt_hsb_dsb/train.hsb-dsb.dsb\n",
            "2022-08-28 20:50:13 | INFO | fairseq.tasks.translation | data-bin/wmt_hsb_dsb train hsb-dsb 62565 examples\n",
            "2022-08-28 20:50:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 001:   0% 0/99 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul2023_usa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220828_205014-340nhsh9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_hsb-dsb_transformers_supervised\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_hsb-dsb_transformers_supervised/runs/340nhsh9\u001b[0m\n",
            "2022-08-28 20:50:14 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-08-28 20:50:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2022-08-28 20:50:17 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   3% 3/99 [00:05<02:08,  1.34s/it]2022-08-28 20:50:19 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:   8% 8/99 [00:07<00:57,  1.57it/s]2022-08-28 20:50:22 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  99% 98/99 [00:55<00:00,  1.85it/s]2022-08-28 20:51:09 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  25% 1/4 [00:01<00:03,  1.06s/it]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  50% 2/4 [00:01<00:01,  1.39it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  75% 3/4 [00:01<00:00,  2.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 20:51:13 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "2022-08-28 20:51:13 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.648 | nll_loss 14.322 | ppl 20476.2 | bleu 0.01 | wps 3803.1 | wpb 1325.2 | bsz 177.2 | num_updates 96\n",
            "2022-08-28 20:51:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 96 updates\n",
            "2022-08-28 20:51:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-28 20:51:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-28 20:51:48 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint1.pt (epoch 1 @ 96 updates, score 14.648) (writing took 31.92799929599994 seconds)\n",
            "2022-08-28 20:51:48 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-08-28 20:51:48 | INFO | train | epoch 001 | loss 16.003 | nll_loss 15.844 | ppl 58830.3 | wps 7852.4 | ups 1.06 | wpb 7435.8 | bsz 620.1 | num_updates 96 | lr 1.2e-05 | gnorm 3.449 | loss_scale 16 | train_wall 55 | gb_free 5.5 | wall 95\n",
            "2022-08-28 20:51:48 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 002:   0% 0/99 [00:00<?, ?it/s]2022-08-28 20:51:48 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-08-28 20:51:48 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 98/99 [00:55<00:00,  1.76it/s, loss=15.964, nll_loss=15.801, ppl=57086, wps=7953.2, ups=1.07, wpb=7458.5, bsz=618.8, num_updates=100, lr=1.25e-05, gnorm=3.375, loss_scale=16, train_wall=57, gb_free=5.4, wall=98]2022-08-28 20:52:44 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  25% 1/4 [00:01<00:03,  1.17s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  50% 2/4 [00:02<00:01,  1.02it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.36it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 4/4 [00:02<00:00,  1.80it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 20:52:47 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 13.617 | nll_loss 13.161 | ppl 9160.68 | bleu 0.17 | wps 1743.7 | wpb 1325.2 | bsz 177.2 | num_updates 195 | best_loss 13.617\n",
            "2022-08-28 20:52:47 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 195 updates\n",
            "2022-08-28 20:52:47 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-28 20:52:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-28 20:53:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint2.pt (epoch 2 @ 195 updates, score 13.617) (writing took 40.619862714999954 seconds)\n",
            "2022-08-28 20:53:28 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-08-28 20:53:28 | INFO | train | epoch 002 | loss 14.538 | nll_loss 14.216 | ppl 19030.9 | wps 7394.4 | ups 0.99 | wpb 7437.5 | bsz 632 | num_updates 195 | lr 2.4375e-05 | gnorm 1.925 | loss_scale 16 | train_wall 55 | gb_free 5.5 | wall 194\n",
            "2022-08-28 20:53:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 003:   0% 0/99 [00:00<?, ?it/s]2022-08-28 20:53:28 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-08-28 20:53:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 98/99 [00:55<00:00,  1.79it/s, loss=14.479, nll_loss=14.151, ppl=18191.4, wps=7407, ups=1, wpb=7410.9, bsz=636.4, num_updates=200, lr=2.5e-05, gnorm=1.924, loss_scale=16, train_wall=56, gb_free=5.6, wall=198]2022-08-28 20:54:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.34s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.16s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  75% 3/4 [00:02<00:00,  1.19it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.52it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 20:54:28 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 12.951 | nll_loss 12.41 | ppl 5441.57 | bleu 0.2 | wps 1458.6 | wpb 1325.2 | bsz 177.2 | num_updates 294 | best_loss 12.951\n",
            "2022-08-28 20:54:28 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 294 updates\n",
            "2022-08-28 20:54:28 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-28 20:54:39 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-28 20:55:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint3.pt (epoch 3 @ 294 updates, score 12.951) (writing took 38.40060221500016 seconds)\n",
            "2022-08-28 20:55:06 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-08-28 20:55:06 | INFO | train | epoch 003 | loss 13.471 | nll_loss 13.029 | ppl 8358.59 | wps 7486 | ups 1.01 | wpb 7437.5 | bsz 632 | num_updates 294 | lr 3.675e-05 | gnorm 2.349 | loss_scale 16 | train_wall 56 | gb_free 5.4 | wall 293\n",
            "2022-08-28 20:55:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 004:   0% 0/99 [00:00<?, ?it/s]2022-08-28 20:55:06 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-08-28 20:55:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  94% 93/99 [00:53<00:03,  1.77it/s, loss=13.417, nll_loss=12.969, ppl=8015.5, wps=7537.5, ups=1.01, wpb=7458.8, bsz=630.9, num_updates=300, lr=3.75e-05, gnorm=2.364, loss_scale=16, train_wall=56, gb_free=5.5, wall=297]2022-08-28 20:56:00 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 8.0\n",
            "epoch 004:  99% 98/99 [00:56<00:00,  1.81it/s, loss=13.417, nll_loss=12.969, ppl=8015.5, wps=7537.5, ups=1.01, wpb=7458.8, bsz=630.9, num_updates=300, lr=3.75e-05, gnorm=2.364, loss_scale=16, train_wall=56, gb_free=5.5, wall=297]2022-08-28 20:56:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.76s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.54s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.18s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.00s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 20:56:07 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 13 | nll_loss 12.445 | ppl 5576.55 | bleu 0.16 | wps 953.6 | wpb 1325.2 | bsz 177.2 | num_updates 392 | best_loss 12.951\n",
            "2022-08-28 20:56:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 392 updates\n",
            "2022-08-28 20:56:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-28 20:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-28 20:56:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint4.pt (epoch 4 @ 392 updates, score 13.0) (writing took 28.85084897799993 seconds)\n",
            "2022-08-28 20:56:36 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-08-28 20:56:36 | INFO | train | epoch 004 | loss 12.637 | nll_loss 12.077 | ppl 4320.7 | wps 8067.9 | ups 1.09 | wpb 7430.2 | bsz 627.6 | num_updates 392 | lr 4.9e-05 | gnorm 2.599 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 383\n",
            "2022-08-28 20:56:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 005:   0% 0/99 [00:00<?, ?it/s]2022-08-28 20:56:37 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-08-28 20:56:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 98/99 [00:55<00:00,  1.87it/s, loss=12.595, nll_loss=12.026, ppl=4171.18, wps=8130.7, ups=1.1, wpb=7424.9, bsz=639, num_updates=400, lr=5e-05, gnorm=2.719, loss_scale=8, train_wall=57, gb_free=5.4, wall=388]2022-08-28 20:57:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.56s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.35s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.08s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.06it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 20:57:37 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 12.404 | nll_loss 11.711 | ppl 3351.95 | bleu 0.25 | wps 1015.3 | wpb 1325.2 | bsz 177.2 | num_updates 491 | best_loss 12.404\n",
            "2022-08-28 20:57:37 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 491 updates\n",
            "2022-08-28 20:57:37 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-28 20:57:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-28 20:58:15 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint5.pt (epoch 5 @ 491 updates, score 12.404) (writing took 37.68295770000009 seconds)\n",
            "2022-08-28 20:58:15 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-08-28 20:58:15 | INFO | train | epoch 005 | loss 12.157 | nll_loss 11.502 | ppl 2900.95 | wps 7486 | ups 1.01 | wpb 7437.5 | bsz 632 | num_updates 491 | lr 6.1375e-05 | gnorm 2.631 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 481\n",
            "2022-08-28 20:58:15 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 006:   0% 0/99 [00:00<?, ?it/s]2022-08-28 20:58:15 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-08-28 20:58:15 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 98/99 [00:56<00:00,  1.80it/s, loss=12.124, nll_loss=11.463, ppl=2823.53, wps=7449.2, ups=1.01, wpb=7402.2, bsz=613, num_updates=500, lr=6.25e-05, gnorm=2.557, loss_scale=8, train_wall=56, gb_free=5.7, wall=487]2022-08-28 20:59:12 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.47s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.34s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.08s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.07it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 20:59:16 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 12.054 | nll_loss 11.306 | ppl 2531.52 | bleu 0.26 | wps 1001 | wpb 1325.2 | bsz 177.2 | num_updates 590 | best_loss 12.054\n",
            "2022-08-28 20:59:16 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 590 updates\n",
            "2022-08-28 20:59:16 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-28 20:59:28 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-28 20:59:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint6.pt (epoch 6 @ 590 updates, score 12.054) (writing took 39.212543361000144 seconds)\n",
            "2022-08-28 20:59:55 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-08-28 20:59:55 | INFO | train | epoch 006 | loss 11.799 | nll_loss 11.08 | ppl 2165.01 | wps 7327 | ups 0.99 | wpb 7437.5 | bsz 632 | num_updates 590 | lr 7.375e-05 | gnorm 2.498 | loss_scale 8 | train_wall 56 | gb_free 5.7 | wall 582\n",
            "2022-08-28 20:59:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 007:   0% 0/99 [00:00<?, ?it/s]2022-08-28 20:59:55 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-08-28 20:59:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 98/99 [00:55<00:00,  1.79it/s, loss=11.784, nll_loss=11.063, ppl=2138.69, wps=7356, ups=0.99, wpb=7414.1, bsz=631.9, num_updates=600, lr=7.5e-05, gnorm=2.473, loss_scale=8, train_wall=56, gb_free=5.5, wall=588]2022-08-28 21:00:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.45s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.23s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.00s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:00:56 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 11.804 | nll_loss 10.999 | ppl 2045.9 | bleu 0.72 | wps 1085.1 | wpb 1325.2 | bsz 177.2 | num_updates 689 | best_loss 11.804\n",
            "2022-08-28 21:00:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 689 updates\n",
            "2022-08-28 21:00:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-28 21:01:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-28 21:01:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint7.pt (epoch 7 @ 689 updates, score 11.804) (writing took 40.300176362 seconds)\n",
            "2022-08-28 21:01:36 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-08-28 21:01:36 | INFO | train | epoch 007 | loss 11.42 | nll_loss 10.644 | ppl 1600.52 | wps 7304.2 | ups 0.98 | wpb 7437.5 | bsz 632 | num_updates 689 | lr 8.6125e-05 | gnorm 2.425 | loss_scale 8 | train_wall 55 | gb_free 5.7 | wall 683\n",
            "2022-08-28 21:01:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 008:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:01:36 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-08-28 21:01:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 98/99 [00:56<00:00,  2.00it/s, loss=11.359, nll_loss=10.576, ppl=1525.98, wps=7361.8, ups=0.98, wpb=7491.4, bsz=634.9, num_updates=700, lr=8.75e-05, gnorm=2.391, loss_scale=8, train_wall=56, gb_free=5.6, wall=690]2022-08-28 21:02:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.79s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.69s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  75% 3/4 [00:04<00:01,  1.38s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 4/4 [00:05<00:00,  1.23s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:02:38 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 11.549 | nll_loss 10.712 | ppl 1677.41 | bleu 0.8 | wps 751.9 | wpb 1325.2 | bsz 177.2 | num_updates 788 | best_loss 11.549\n",
            "2022-08-28 21:02:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 788 updates\n",
            "2022-08-28 21:02:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-28 21:02:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-28 21:03:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint8.pt (epoch 8 @ 788 updates, score 11.549) (writing took 38.48367678799991 seconds)\n",
            "2022-08-28 21:03:17 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-08-28 21:03:17 | INFO | train | epoch 008 | loss 10.958 | nll_loss 10.116 | ppl 1109.48 | wps 7301.4 | ups 0.98 | wpb 7437.5 | bsz 632 | num_updates 788 | lr 9.85e-05 | gnorm 2.102 | loss_scale 8 | train_wall 55 | gb_free 5.4 | wall 784\n",
            "2022-08-28 21:03:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 009:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:03:17 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-08-28 21:03:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 98/99 [00:56<00:00,  1.81it/s, loss=10.895, nll_loss=10.043, ppl=1055.04, wps=7282.1, ups=0.99, wpb=7374.5, bsz=632.3, num_updates=800, lr=0.0001, gnorm=2.105, loss_scale=8, train_wall=56, gb_free=5.5, wall=791]2022-08-28 21:04:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.51s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.51s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.18s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.00s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:04:19 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 11.247 | nll_loss 10.361 | ppl 1315.45 | bleu 1.43 | wps 906.1 | wpb 1325.2 | bsz 177.2 | num_updates 887 | best_loss 11.247\n",
            "2022-08-28 21:04:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 887 updates\n",
            "2022-08-28 21:04:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-28 21:04:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-28 21:04:58 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint9.pt (epoch 9 @ 887 updates, score 11.247) (writing took 39.38783868900009 seconds)\n",
            "2022-08-28 21:04:58 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-08-28 21:04:58 | INFO | train | epoch 009 | loss 10.4 | nll_loss 9.48 | ppl 714.29 | wps 7290.8 | ups 0.98 | wpb 7437.5 | bsz 632 | num_updates 887 | lr 0.000110875 | gnorm 2.119 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 885\n",
            "2022-08-28 21:04:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 010:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:04:58 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-08-28 21:04:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 98/99 [00:56<00:00,  1.79it/s, loss=10.323, nll_loss=9.392, ppl=671.99, wps=7364.4, ups=0.99, wpb=7462, bsz=644.9, num_updates=900, lr=0.0001125, gnorm=2.121, loss_scale=8, train_wall=56, gb_free=5.8, wall=892]2022-08-28 21:05:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.58s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.39s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.11s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:05:59 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 11.11 | nll_loss 10.208 | ppl 1182.77 | bleu 2.32 | wps 991.4 | wpb 1325.2 | bsz 177.2 | num_updates 986 | best_loss 11.11\n",
            "2022-08-28 21:05:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 986 updates\n",
            "2022-08-28 21:05:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-28 21:06:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-28 21:06:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint10.pt (epoch 10 @ 986 updates, score 11.11) (writing took 38.20135048099996 seconds)\n",
            "2022-08-28 21:06:37 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-08-28 21:06:37 | INFO | train | epoch 010 | loss 9.849 | nll_loss 8.853 | ppl 462.54 | wps 7400.1 | ups 0.99 | wpb 7437.5 | bsz 632 | num_updates 986 | lr 0.00012325 | gnorm 2.308 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 984\n",
            "2022-08-28 21:06:38 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 011:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:06:38 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2022-08-28 21:06:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 98/99 [00:56<00:00,  1.77it/s, loss=9.759, nll_loss=8.75, ppl=430.64, wps=7498, ups=1, wpb=7517, bsz=632.5, num_updates=1000, lr=0.000125, gnorm=2.286, loss_scale=8, train_wall=56, gb_free=5.4, wall=993]2022-08-28 21:07:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.53s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.32s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.08s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:07:39 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 10.524 | nll_loss 9.548 | ppl 748.81 | bleu 3.91 | wps 1006.7 | wpb 1325.2 | bsz 177.2 | num_updates 1085 | best_loss 10.524\n",
            "2022-08-28 21:07:39 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1085 updates\n",
            "2022-08-28 21:07:39 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-28 21:07:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-28 21:08:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint11.pt (epoch 11 @ 1085 updates, score 10.524) (writing took 39.50986318599985 seconds)\n",
            "2022-08-28 21:08:18 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-08-28 21:08:18 | INFO | train | epoch 011 | loss 9.282 | nll_loss 8.207 | ppl 295.56 | wps 7305.6 | ups 0.98 | wpb 7437.5 | bsz 632 | num_updates 1085 | lr 0.000135625 | gnorm 2.305 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 1085\n",
            "2022-08-28 21:08:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 012:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:08:19 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2022-08-28 21:08:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 98/99 [00:56<00:00,  1.85it/s, loss=9.21, nll_loss=8.126, ppl=279.29, wps=7252.8, ups=0.98, wpb=7396.1, bsz=618, num_updates=1100, lr=0.0001375, gnorm=2.316, loss_scale=8, train_wall=57, gb_free=5.4, wall=1095]2022-08-28 21:09:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.74s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.39s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.12s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.03it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:09:20 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 10.501 | nll_loss 9.475 | ppl 711.62 | bleu 4.44 | wps 1022.3 | wpb 1325.2 | bsz 177.2 | num_updates 1184 | best_loss 10.501\n",
            "2022-08-28 21:09:20 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1184 updates\n",
            "2022-08-28 21:09:20 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-28 21:09:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-28 21:10:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint12.pt (epoch 12 @ 1184 updates, score 10.501) (writing took 93.689228275 seconds)\n",
            "2022-08-28 21:10:54 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-08-28 21:10:58 | INFO | train | epoch 012 | loss 8.763 | nll_loss 7.618 | ppl 196.4 | wps 4619.3 | ups 0.62 | wpb 7437.5 | bsz 632 | num_updates 1184 | lr 0.000148 | gnorm 2.568 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 1244\n",
            "2022-08-28 21:10:58 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 013:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:10:58 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2022-08-28 21:10:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 98/99 [00:56<00:00,  1.85it/s, loss=8.703, nll_loss=7.549, ppl=187.26, wps=4659.5, ups=0.63, wpb=7412.5, bsz=642.9, num_updates=1200, lr=0.00015, gnorm=2.704, loss_scale=8, train_wall=56, gb_free=5.4, wall=1254]2022-08-28 21:11:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.69s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.40s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.21s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.03s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:11:59 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 10.334 | nll_loss 9.282 | ppl 622.38 | bleu 5.92 | wps 937.3 | wpb 1325.2 | bsz 177.2 | num_updates 1283 | best_loss 10.334\n",
            "2022-08-28 21:11:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1283 updates\n",
            "2022-08-28 21:11:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-28 21:12:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-28 21:13:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint13.pt (epoch 13 @ 1283 updates, score 10.334) (writing took 96.44802919499989 seconds)\n",
            "2022-08-28 21:13:36 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2022-08-28 21:13:40 | INFO | train | epoch 013 | loss 8.222 | nll_loss 7.001 | ppl 128.07 | wps 4561.2 | ups 0.61 | wpb 7437.5 | bsz 632 | num_updates 1283 | lr 0.000160375 | gnorm 2.446 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 1406\n",
            "2022-08-28 21:13:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 014:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:13:40 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2022-08-28 21:13:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 98/99 [00:56<00:00,  1.84it/s, loss=8.102, nll_loss=6.865, ppl=116.56, wps=4608.1, ups=0.61, wpb=7505.1, bsz=627.5, num_updates=1300, lr=0.0001625, gnorm=2.355, loss_scale=8, train_wall=57, gb_free=5.5, wall=1417]2022-08-28 21:14:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  25% 1/4 [00:02<00:06,  2.09s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.67s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  75% 3/4 [00:04<00:01,  1.25s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.04s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:14:42 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 10.18 | nll_loss 9.105 | ppl 550.77 | bleu 5.22 | wps 957.8 | wpb 1325.2 | bsz 177.2 | num_updates 1382 | best_loss 10.18\n",
            "2022-08-28 21:14:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1382 updates\n",
            "2022-08-28 21:14:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-28 21:14:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-28 21:16:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint14.pt (epoch 14 @ 1382 updates, score 10.18) (writing took 98.44059668399996 seconds)\n",
            "2022-08-28 21:16:20 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2022-08-28 21:16:20 | INFO | train | epoch 014 | loss 7.715 | nll_loss 6.423 | ppl 85.83 | wps 4594.1 | ups 0.62 | wpb 7437.5 | bsz 632 | num_updates 1382 | lr 0.00017275 | gnorm 2.678 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 1567\n",
            "2022-08-28 21:16:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 015:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:16:20 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2022-08-28 21:16:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 98/99 [00:57<00:00,  1.81it/s, loss=7.65, nll_loss=6.349, ppl=81.54, wps=4553.2, ups=0.62, wpb=7397.6, bsz=631.6, num_updates=1400, lr=0.000175, gnorm=2.794, loss_scale=8, train_wall=56, gb_free=5.5, wall=1579]2022-08-28 21:17:19 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.72s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.42s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.11s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.03it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:17:23 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 9.73 | nll_loss 8.604 | ppl 389 | bleu 9.62 | wps 1009.5 | wpb 1325.2 | bsz 177.2 | num_updates 1481 | best_loss 9.73\n",
            "2022-08-28 21:17:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1481 updates\n",
            "2022-08-28 21:17:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-28 21:17:35 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-28 21:19:01 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint15.pt (epoch 15 @ 1481 updates, score 9.73) (writing took 97.46275805200003 seconds)\n",
            "2022-08-28 21:19:01 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2022-08-28 21:19:01 | INFO | train | epoch 015 | loss 7.218 | nll_loss 5.855 | ppl 57.89 | wps 4584.7 | ups 0.62 | wpb 7437.5 | bsz 632 | num_updates 1481 | lr 0.000185125 | gnorm 2.503 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 1727\n",
            "2022-08-28 21:19:01 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 016:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:19:01 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2022-08-28 21:19:01 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 98/99 [00:56<00:00,  1.80it/s, loss=7.113, nll_loss=5.736, ppl=53.28, wps=4625.5, ups=0.62, wpb=7413.7, bsz=616.8, num_updates=1500, lr=0.0001875, gnorm=2.376, loss_scale=8, train_wall=57, gb_free=5.4, wall=1739]2022-08-28 21:19:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.76s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  50% 2/4 [00:03<00:02,  1.46s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.15s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.02it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:20:03 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 9.628 | nll_loss 8.464 | ppl 353.1 | bleu 10.38 | wps 990.9 | wpb 1325.2 | bsz 177.2 | num_updates 1580 | best_loss 9.628\n",
            "2022-08-28 21:20:03 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 1580 updates\n",
            "2022-08-28 21:20:03 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-28 21:20:15 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-28 21:21:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint16.pt (epoch 16 @ 1580 updates, score 9.628) (writing took 99.53929137099976 seconds)\n",
            "2022-08-28 21:21:43 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2022-08-28 21:21:43 | INFO | train | epoch 016 | loss 6.718 | nll_loss 5.284 | ppl 38.96 | wps 4545.7 | ups 0.61 | wpb 7437.5 | bsz 632 | num_updates 1580 | lr 0.0001975 | gnorm 2.402 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 1889\n",
            "2022-08-28 21:21:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 017:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:21:43 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2022-08-28 21:21:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 98/99 [00:57<00:00,  1.80it/s, loss=6.628, nll_loss=5.18, ppl=36.26, wps=4551.1, ups=0.62, wpb=7394.6, bsz=650.6, num_updates=1600, lr=0.0002, gnorm=2.42, loss_scale=8, train_wall=56, gb_free=5.4, wall=1902]2022-08-28 21:22:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.69s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.44s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.13s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.02it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:22:45 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 9.495 | nll_loss 8.318 | ppl 319.16 | bleu 12.68 | wps 986.7 | wpb 1325.2 | bsz 177.2 | num_updates 1679 | best_loss 9.495\n",
            "2022-08-28 21:22:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 1679 updates\n",
            "2022-08-28 21:22:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-28 21:22:57 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-28 21:24:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint17.pt (epoch 17 @ 1679 updates, score 9.495) (writing took 101.79454016699992 seconds)\n",
            "2022-08-28 21:24:27 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2022-08-28 21:24:27 | INFO | train | epoch 017 | loss 6.256 | nll_loss 4.753 | ppl 26.97 | wps 4485.7 | ups 0.6 | wpb 7437.5 | bsz 632 | num_updates 1679 | lr 0.000209875 | gnorm 2.375 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 2054\n",
            "2022-08-28 21:24:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 018:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:24:27 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2022-08-28 21:24:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 98/99 [00:57<00:00,  1.82it/s, loss=6.141, nll_loss=4.622, ppl=24.63, wps=4547.2, ups=0.6, wpb=7519.3, bsz=637.8, num_updates=1700, lr=0.0002125, gnorm=2.292, loss_scale=8, train_wall=57, gb_free=5.7, wall=2067]2022-08-28 21:25:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.96s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  50% 2/4 [00:03<00:02,  1.50s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.17s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.00it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:25:30 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 9.461 | nll_loss 8.287 | ppl 312.3 | bleu 13.25 | wps 1018.6 | wpb 1325.2 | bsz 177.2 | num_updates 1778 | best_loss 9.461\n",
            "2022-08-28 21:25:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 1778 updates\n",
            "2022-08-28 21:25:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-28 21:25:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-28 21:27:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint18.pt (epoch 18 @ 1778 updates, score 9.461) (writing took 96.87095559999989 seconds)\n",
            "2022-08-28 21:27:07 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2022-08-28 21:27:10 | INFO | train | epoch 018 | loss 5.826 | nll_loss 4.258 | ppl 19.13 | wps 4607.1 | ups 0.62 | wpb 7437.5 | bsz 632 | num_updates 1778 | lr 0.00022225 | gnorm 2.11 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 2213\n",
            "2022-08-28 21:27:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 019:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:27:11 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2022-08-28 21:27:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 98/99 [00:56<00:00,  1.76it/s, loss=5.718, nll_loss=4.133, ppl=17.55, wps=4577.1, ups=0.61, wpb=7460.2, bsz=608.3, num_updates=1800, lr=0.000225, gnorm=2.061, loss_scale=8, train_wall=56, gb_free=5.4, wall=2230]2022-08-28 21:28:07 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.68s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.44s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.13s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.03it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:28:12 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 9.267 | nll_loss 8.008 | ppl 257.43 | bleu 15.06 | wps 988.6 | wpb 1325.2 | bsz 177.2 | num_updates 1877 | best_loss 9.267\n",
            "2022-08-28 21:28:12 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 1877 updates\n",
            "2022-08-28 21:28:12 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-28 21:28:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-28 21:29:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint19.pt (epoch 19 @ 1877 updates, score 9.267) (writing took 98.62463542200021 seconds)\n",
            "2022-08-28 21:29:50 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2022-08-28 21:29:50 | INFO | train | epoch 019 | loss 5.411 | nll_loss 3.78 | ppl 13.74 | wps 4606 | ups 0.62 | wpb 7437.5 | bsz 632 | num_updates 1877 | lr 0.000234625 | gnorm 1.958 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 2377\n",
            "2022-08-28 21:29:50 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 020:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:29:51 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2022-08-28 21:29:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 98/99 [00:56<00:00,  1.83it/s, loss=5.344, nll_loss=3.703, ppl=13.02, wps=4615, ups=0.62, wpb=7413.9, bsz=651.2, num_updates=1900, lr=0.0002375, gnorm=1.977, loss_scale=8, train_wall=56, gb_free=5.5, wall=2391]2022-08-28 21:30:47 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.73s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  50% 2/4 [00:03<00:02,  1.49s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.17s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.11s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:30:52 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 9.206 | nll_loss 7.928 | ppl 243.6 | bleu 16.13 | wps 876.9 | wpb 1325.2 | bsz 177.2 | num_updates 1976 | best_loss 9.206\n",
            "2022-08-28 21:30:52 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 1976 updates\n",
            "2022-08-28 21:30:52 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-28 21:31:05 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-28 21:32:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint20.pt (epoch 20 @ 1976 updates, score 9.206) (writing took 99.41285676500002 seconds)\n",
            "2022-08-28 21:32:32 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2022-08-28 21:32:32 | INFO | train | epoch 020 | loss 5.054 | nll_loss 3.367 | ppl 10.31 | wps 4564.9 | ups 0.61 | wpb 7437.5 | bsz 632 | num_updates 1976 | lr 0.000247 | gnorm 1.86 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 2538\n",
            "2022-08-28 21:32:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 021:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:32:32 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2022-08-28 21:32:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 98/99 [00:58<00:00,  1.82it/s, loss=4.952, nll_loss=3.249, ppl=9.5, wps=4535.5, ups=0.61, wpb=7462.9, bsz=631.7, num_updates=2000, lr=0.00025, gnorm=1.778, loss_scale=8, train_wall=56, gb_free=5.8, wall=2555]2022-08-28 21:33:31 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.63s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.30s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.05s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:33:35 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 8.997 | nll_loss 7.691 | ppl 206.68 | bleu 21.8 | wps 1080.5 | wpb 1325.2 | bsz 177.2 | num_updates 2075 | best_loss 8.997\n",
            "2022-08-28 21:33:35 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2075 updates\n",
            "2022-08-28 21:33:35 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-28 21:33:48 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-28 21:35:13 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint21.pt (epoch 21 @ 2075 updates, score 8.997) (writing took 97.5939484280002 seconds)\n",
            "2022-08-28 21:35:13 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2022-08-28 21:35:13 | INFO | train | epoch 021 | loss 4.685 | nll_loss 2.938 | ppl 7.66 | wps 4553.3 | ups 0.61 | wpb 7437.5 | bsz 632 | num_updates 2075 | lr 0.000259375 | gnorm 1.622 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 2700\n",
            "2022-08-28 21:35:13 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 022:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:35:14 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2022-08-28 21:35:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 98/99 [00:59<00:00,  1.80it/s, loss=4.605, nll_loss=2.844, ppl=7.18, wps=4563.9, ups=0.61, wpb=7429.7, bsz=625.7, num_updates=2100, lr=0.0002625, gnorm=1.55, loss_scale=8, train_wall=56, gb_free=5.5, wall=2718]2022-08-28 21:36:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.58s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.37s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.09s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:36:18 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 8.895 | nll_loss 7.592 | ppl 193 | bleu 23.66 | wps 997.8 | wpb 1325.2 | bsz 177.2 | num_updates 2174 | best_loss 8.895\n",
            "2022-08-28 21:36:18 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2174 updates\n",
            "2022-08-28 21:36:18 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-28 21:36:30 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-28 21:37:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint22.pt (epoch 22 @ 2174 updates, score 8.895) (writing took 97.32735993400001 seconds)\n",
            "2022-08-28 21:37:55 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2022-08-28 21:37:55 | INFO | train | epoch 022 | loss 4.345 | nll_loss 2.543 | ppl 5.83 | wps 4551.3 | ups 0.61 | wpb 7437.5 | bsz 632 | num_updates 2174 | lr 0.00027175 | gnorm 1.455 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 2862\n",
            "2022-08-28 21:37:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 023:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:37:56 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2022-08-28 21:37:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 98/99 [00:59<00:00,  1.84it/s, loss=4.27, nll_loss=2.455, ppl=5.48, wps=4561.3, ups=0.62, wpb=7414.6, bsz=618.5, num_updates=2200, lr=0.000275, gnorm=1.452, loss_scale=8, train_wall=56, gb_free=5.5, wall=2881]2022-08-28 21:38:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.88s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.51s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.17s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.01s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:39:00 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 8.849 | nll_loss 7.516 | ppl 183.03 | bleu 22.74 | wps 985.7 | wpb 1325.2 | bsz 177.2 | num_updates 2273 | best_loss 8.849\n",
            "2022-08-28 21:39:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 2273 updates\n",
            "2022-08-28 21:39:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-28 21:39:12 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-28 21:40:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint23.pt (epoch 23 @ 2273 updates, score 8.849) (writing took 96.96599741699993 seconds)\n",
            "2022-08-28 21:40:37 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2022-08-28 21:40:37 | INFO | train | epoch 023 | loss 4.069 | nll_loss 2.22 | ppl 4.66 | wps 4542.2 | ups 0.61 | wpb 7437.5 | bsz 632 | num_updates 2273 | lr 0.000284125 | gnorm 1.364 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 3024\n",
            "2022-08-28 21:40:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 024:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:40:41 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2022-08-28 21:40:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 98/99 [00:59<00:00,  1.84it/s, loss=3.969, nll_loss=2.102, ppl=4.29, wps=4612.7, ups=0.61, wpb=7518.4, bsz=646.7, num_updates=2300, lr=0.0002875, gnorm=1.297, loss_scale=8, train_wall=57, gb_free=5.6, wall=3044]2022-08-28 21:41:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.65s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.40s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.08s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.06it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:41:42 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 8.905 | nll_loss 7.569 | ppl 189.84 | bleu 26.31 | wps 1033 | wpb 1325.2 | bsz 177.2 | num_updates 2372 | best_loss 8.849\n",
            "2022-08-28 21:41:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 2372 updates\n",
            "2022-08-28 21:41:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-28 21:41:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-28 21:42:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint24.pt (epoch 24 @ 2372 updates, score 8.905) (writing took 45.1632680380003 seconds)\n",
            "2022-08-28 21:42:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2022-08-28 21:42:27 | INFO | train | epoch 024 | loss 3.785 | nll_loss 1.885 | ppl 3.69 | wps 6690.8 | ups 0.9 | wpb 7437.5 | bsz 632 | num_updates 2372 | lr 0.0002965 | gnorm 1.191 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 3134\n",
            "2022-08-28 21:42:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 025:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:42:28 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2022-08-28 21:42:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 98/99 [00:57<00:00,  1.83it/s, loss=3.745, nll_loss=1.837, ppl=3.57, wps=6777.7, ups=0.93, wpb=7325.3, bsz=619.6, num_updates=2400, lr=0.0003, gnorm=1.23, loss_scale=8, train_wall=56, gb_free=5.5, wall=3152]2022-08-28 21:43:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.50s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.33s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.06s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.08it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:43:30 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 8.663 | nll_loss 7.271 | ppl 154.44 | bleu 27.39 | wps 1022.8 | wpb 1325.2 | bsz 177.2 | num_updates 2471 | best_loss 8.663\n",
            "2022-08-28 21:43:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 2471 updates\n",
            "2022-08-28 21:43:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-28 21:43:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-28 21:45:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint25.pt (epoch 25 @ 2471 updates, score 8.663) (writing took 97.7463220059999 seconds)\n",
            "2022-08-28 21:45:11 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2022-08-28 21:45:11 | INFO | train | epoch 025 | loss 3.544 | nll_loss 1.601 | ppl 3.03 | wps 4486.9 | ups 0.6 | wpb 7437.5 | bsz 632 | num_updates 2471 | lr 0.000308875 | gnorm 1.123 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 3298\n",
            "2022-08-28 21:45:12 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 026:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:45:12 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2022-08-28 21:45:12 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 98/99 [00:56<00:00,  1.82it/s, loss=3.474, nll_loss=1.518, ppl=2.86, wps=4579.5, ups=0.61, wpb=7474.1, bsz=621.7, num_updates=2500, lr=0.0003125, gnorm=1.083, loss_scale=8, train_wall=57, gb_free=5.4, wall=3315]2022-08-28 21:46:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.65s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.41s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.11s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:46:13 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 8.767 | nll_loss 7.388 | ppl 167.55 | bleu 27.05 | wps 1004.8 | wpb 1325.2 | bsz 177.2 | num_updates 2570 | best_loss 8.663\n",
            "2022-08-28 21:46:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 2570 updates\n",
            "2022-08-28 21:46:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-28 21:46:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-28 21:46:58 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2022-08-28 21:46:58 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 98/99 [00:56<00:00,  1.78it/s, loss=3.247, nll_loss=1.25, ppl=2.38, wps=6937.4, ups=0.93, wpb=7444.7, bsz=639.7, num_updates=2600, lr=0.000325, gnorm=0.957, loss_scale=8, train_wall=56, gb_free=5.4, wall=3422]2022-08-28 21:47:55 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.50s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.28s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.03s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:47:59 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 8.497 | nll_loss 7.082 | ppl 135.5 | bleu 28.18 | wps 1064.4 | wpb 1325.2 | bsz 177.2 | num_updates 2669 | best_loss 8.497\n",
            "2022-08-28 21:47:59 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 2669 updates\n",
            "2022-08-28 21:47:59 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-28 21:48:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-28 21:49:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint27.pt (epoch 27 @ 2669 updates, score 8.497) (writing took 102.4104706220005 seconds)\n",
            "2022-08-28 21:49:42 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2022-08-28 21:49:42 | INFO | train | epoch 027 | loss 3.117 | nll_loss 1.096 | ppl 2.14 | wps 4501.3 | ups 0.61 | wpb 7437.5 | bsz 632 | num_updates 2669 | lr 0.000333625 | gnorm 0.938 | loss_scale 8 | train_wall 56 | gb_free 6 | wall 3568\n",
            "2022-08-28 21:49:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 028:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:49:42 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2022-08-28 21:49:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 98/99 [00:55<00:00,  1.80it/s, loss=3.104, nll_loss=1.077, ppl=2.11, wps=4485.1, ups=0.61, wpb=7357.6, bsz=627, num_updates=2700, lr=0.0003375, gnorm=0.996, loss_scale=8, train_wall=56, gb_free=5.5, wall=3587]2022-08-28 21:50:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.78s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.44s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.11s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.04it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:50:43 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 8.729 | nll_loss 7.315 | ppl 159.2 | bleu 29.24 | wps 1029.8 | wpb 1325.2 | bsz 177.2 | num_updates 2768 | best_loss 8.497\n",
            "2022-08-28 21:50:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 2768 updates\n",
            "2022-08-28 21:50:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-28 21:50:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-28 21:51:28 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint28.pt (epoch 28 @ 2768 updates, score 8.729) (writing took 45.07726573899981 seconds)\n",
            "2022-08-28 21:51:28 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2022-08-28 21:51:28 | INFO | train | epoch 028 | loss 2.973 | nll_loss 0.921 | ppl 1.89 | wps 6930.6 | ups 0.93 | wpb 7437.5 | bsz 632 | num_updates 2768 | lr 0.000346 | gnorm 0.888 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 3675\n",
            "2022-08-28 21:51:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 029:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:51:28 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2022-08-28 21:51:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 98/99 [00:56<00:00,  1.77it/s, loss=2.927, nll_loss=0.866, ppl=1.82, wps=6903.1, ups=0.93, wpb=7409.1, bsz=650.9, num_updates=2800, lr=0.00035, gnorm=0.85, loss_scale=8, train_wall=56, gb_free=5.5, wall=3694]2022-08-28 21:52:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.56s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.30s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.04s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:52:30 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 8.772 | nll_loss 7.353 | ppl 163.49 | bleu 28.86 | wps 1068.4 | wpb 1325.2 | bsz 177.2 | num_updates 2867 | best_loss 8.497\n",
            "2022-08-28 21:52:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 2867 updates\n",
            "2022-08-28 21:52:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-28 21:52:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-28 21:53:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint29.pt (epoch 29 @ 2867 updates, score 8.772) (writing took 46.33286404999944 seconds)\n",
            "2022-08-28 21:53:16 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2022-08-28 21:53:16 | INFO | train | epoch 029 | loss 2.825 | nll_loss 0.743 | ppl 1.67 | wps 6819 | ups 0.92 | wpb 7437.5 | bsz 632 | num_updates 2867 | lr 0.000358375 | gnorm 0.769 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 3783\n",
            "2022-08-28 21:53:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 030:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:53:16 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2022-08-28 21:53:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 98/99 [00:57<00:00,  1.79it/s, loss=2.771, nll_loss=0.676, ppl=1.6, wps=6870.2, ups=0.91, wpb=7536.2, bsz=628.9, num_updates=2900, lr=0.0003625, gnorm=0.682, loss_scale=8, train_wall=56, gb_free=5.4, wall=3804]2022-08-28 21:54:14 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.63s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.35s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.07s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:54:19 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 8.501 | nll_loss 7.056 | ppl 133.04 | bleu 29.62 | wps 1072.5 | wpb 1325.2 | bsz 177.2 | num_updates 2966 | best_loss 8.497\n",
            "2022-08-28 21:54:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 2966 updates\n",
            "2022-08-28 21:54:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-28 21:54:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-28 21:55:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint30.pt (epoch 30 @ 2966 updates, score 8.501) (writing took 47.03276181600086 seconds)\n",
            "2022-08-28 21:55:06 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2022-08-28 21:55:06 | INFO | train | epoch 030 | loss 2.729 | nll_loss 0.623 | ppl 1.54 | wps 6711.8 | ups 0.9 | wpb 7437.5 | bsz 632 | num_updates 2966 | lr 0.00037075 | gnorm 0.722 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 3892\n",
            "2022-08-28 21:55:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 031:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:55:06 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2022-08-28 21:55:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 98/99 [00:56<00:00,  1.80it/s, loss=2.705, nll_loss=0.594, ppl=1.51, wps=6798.4, ups=0.92, wpb=7400.7, bsz=638.5, num_updates=3000, lr=0.000375, gnorm=0.722, loss_scale=8, train_wall=56, gb_free=5.8, wall=3912]2022-08-28 21:56:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.74s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.41s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.10s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.10it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:56:07 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 8.771 | nll_loss 7.351 | ppl 163.29 | bleu 27.76 | wps 1082.6 | wpb 1325.2 | bsz 177.2 | num_updates 3065 | best_loss 8.497\n",
            "2022-08-28 21:56:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 3065 updates\n",
            "2022-08-28 21:56:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-28 21:56:19 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-28 21:56:52 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint31.pt (epoch 31 @ 3065 updates, score 8.771) (writing took 45.19318774400017 seconds)\n",
            "2022-08-28 21:56:52 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2022-08-28 21:56:52 | INFO | train | epoch 031 | loss 2.657 | nll_loss 0.536 | ppl 1.45 | wps 6906.1 | ups 0.93 | wpb 7437.5 | bsz 632 | num_updates 3065 | lr 0.000383125 | gnorm 0.656 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 3999\n",
            "2022-08-28 21:56:52 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 032:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:56:53 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2022-08-28 21:56:53 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 98/99 [00:57<00:00,  1.79it/s, loss=2.645, nll_loss=0.521, ppl=1.44, wps=6886.8, ups=0.92, wpb=7467.1, bsz=615, num_updates=3100, lr=0.0003875, gnorm=0.641, loss_scale=8, train_wall=56, gb_free=5.6, wall=4021]2022-08-28 21:57:50 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.67s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.44s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.25s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.15s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:57:55 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 8.759 | nll_loss 7.329 | ppl 160.77 | bleu 28.77 | wps 828 | wpb 1325.2 | bsz 177.2 | num_updates 3164 | best_loss 8.497\n",
            "2022-08-28 21:57:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 3164 updates\n",
            "2022-08-28 21:57:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-28 21:58:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-28 21:58:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint32.pt (epoch 32 @ 3164 updates, score 8.759) (writing took 45.16530212700036 seconds)\n",
            "2022-08-28 21:58:41 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2022-08-28 21:58:41 | INFO | train | epoch 032 | loss 2.617 | nll_loss 0.489 | ppl 1.4 | wps 6798.3 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 3164 | lr 0.0003955 | gnorm 0.614 | loss_scale 8 | train_wall 55 | gb_free 5.4 | wall 4107\n",
            "2022-08-28 21:58:41 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 033:   0% 0/99 [00:00<?, ?it/s]2022-08-28 21:58:41 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2022-08-28 21:58:41 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 98/99 [00:55<00:00,  1.80it/s, loss=2.604, nll_loss=0.475, ppl=1.39, wps=6929.2, ups=0.93, wpb=7413.7, bsz=646.4, num_updates=3200, lr=0.0004, gnorm=0.595, loss_scale=8, train_wall=56, gb_free=5.7, wall=4128]2022-08-28 21:59:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.66s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.47s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.13s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.03it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 21:59:42 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 8.501 | nll_loss 7.054 | ppl 132.9 | bleu 30 | wps 982.4 | wpb 1325.2 | bsz 177.2 | num_updates 3263 | best_loss 8.497\n",
            "2022-08-28 21:59:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 3263 updates\n",
            "2022-08-28 21:59:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-28 21:59:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-28 22:00:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint33.pt (epoch 33 @ 3263 updates, score 8.501) (writing took 48.446949201999814 seconds)\n",
            "2022-08-28 22:00:30 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2022-08-28 22:00:30 | INFO | train | epoch 033 | loss 2.579 | nll_loss 0.447 | ppl 1.36 | wps 6729.1 | ups 0.9 | wpb 7437.5 | bsz 632 | num_updates 3263 | lr 0.000407875 | gnorm 0.582 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 4217\n",
            "2022-08-28 22:00:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 034:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:00:30 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2022-08-28 22:00:30 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 98/99 [00:55<00:00,  1.84it/s, loss=2.567, nll_loss=0.435, ppl=1.35, wps=6749.7, ups=0.91, wpb=7434.6, bsz=634.7, num_updates=3300, lr=0.0004125, gnorm=0.573, loss_scale=8, train_wall=56, gb_free=5.5, wall=4238]2022-08-28 22:01:26 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.62s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.38s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.06s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:01:31 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 8.702 | nll_loss 7.293 | ppl 156.78 | bleu 28.87 | wps 1058.7 | wpb 1325.2 | bsz 177.2 | num_updates 3362 | best_loss 8.497\n",
            "2022-08-28 22:01:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 3362 updates\n",
            "2022-08-28 22:01:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-28 22:01:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-28 22:02:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint34.pt (epoch 34 @ 3362 updates, score 8.702) (writing took 47.19293236900012 seconds)\n",
            "2022-08-28 22:02:18 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2022-08-28 22:02:18 | INFO | train | epoch 034 | loss 2.545 | nll_loss 0.415 | ppl 1.33 | wps 6825.4 | ups 0.92 | wpb 7437.5 | bsz 632 | num_updates 3362 | lr 0.00042025 | gnorm 0.535 | loss_scale 8 | train_wall 55 | gb_free 5.5 | wall 4325\n",
            "2022-08-28 22:02:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 035:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:02:18 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2022-08-28 22:02:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 98/99 [00:56<00:00,  1.82it/s, loss=2.544, nll_loss=0.415, ppl=1.33, wps=6856.7, ups=0.92, wpb=7466.6, bsz=646.6, num_updates=3400, lr=0.000425, gnorm=0.546, loss_scale=8, train_wall=56, gb_free=5.5, wall=4347]2022-08-28 22:03:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.57s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.28s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.02s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:03:19 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 8.605 | nll_loss 7.213 | ppl 148.37 | bleu 29.87 | wps 1102.9 | wpb 1325.2 | bsz 177.2 | num_updates 3461 | best_loss 8.497\n",
            "2022-08-28 22:03:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 3461 updates\n",
            "2022-08-28 22:03:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-28 22:03:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-28 22:04:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint35.pt (epoch 35 @ 3461 updates, score 8.605) (writing took 47.61542490299962 seconds)\n",
            "2022-08-28 22:04:07 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2022-08-28 22:04:07 | INFO | train | epoch 035 | loss 2.534 | nll_loss 0.406 | ppl 1.32 | wps 6782.5 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 3461 | lr 0.000432625 | gnorm 0.539 | loss_scale 8 | train_wall 55 | gb_free 5.4 | wall 4433\n",
            "2022-08-28 22:04:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 036:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:04:07 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2022-08-28 22:04:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 98/99 [00:56<00:00,  1.79it/s, loss=2.516, nll_loss=0.388, ppl=1.31, wps=6762.4, ups=0.92, wpb=7352, bsz=606.8, num_updates=3500, lr=0.0004375, gnorm=0.5, loss_scale=8, train_wall=56, gb_free=5.8, wall=4456]2022-08-28 22:05:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.59s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.29s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.03s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.09it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:05:08 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 8.473 | nll_loss 7.045 | ppl 132.09 | bleu 29.45 | wps 1080.1 | wpb 1325.2 | bsz 177.2 | num_updates 3560 | best_loss 8.473\n",
            "2022-08-28 22:05:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 3560 updates\n",
            "2022-08-28 22:05:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-28 22:05:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-28 22:06:45 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint36.pt (epoch 36 @ 3560 updates, score 8.473) (writing took 97.12013510100041 seconds)\n",
            "2022-08-28 22:06:45 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2022-08-28 22:06:46 | INFO | train | epoch 036 | loss 2.498 | nll_loss 0.372 | ppl 1.29 | wps 4631.9 | ups 0.62 | wpb 7437.5 | bsz 632 | num_updates 3560 | lr 0.000445 | gnorm 0.471 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 4592\n",
            "2022-08-28 22:06:46 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 037:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:06:46 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2022-08-28 22:06:46 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  99% 98/99 [00:58<00:00,  1.83it/s, loss=2.491, nll_loss=0.366, ppl=1.29, wps=4655.5, ups=0.62, wpb=7563.6, bsz=624.4, num_updates=3600, lr=0.00045, gnorm=0.46, loss_scale=8, train_wall=59, gb_free=5.5, wall=4618]2022-08-28 22:07:45 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.72s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.45s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.13s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.02it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:07:49 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 8.61 | nll_loss 7.206 | ppl 147.64 | bleu 26.07 | wps 986.5 | wpb 1325.2 | bsz 177.2 | num_updates 3659 | best_loss 8.473\n",
            "2022-08-28 22:07:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 3659 updates\n",
            "2022-08-28 22:07:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-28 22:08:01 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-28 22:08:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint37.pt (epoch 37 @ 3659 updates, score 8.61) (writing took 47.906272366999474 seconds)\n",
            "2022-08-28 22:08:37 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2022-08-28 22:08:37 | INFO | train | epoch 037 | loss 2.489 | nll_loss 0.368 | ppl 1.29 | wps 6591.5 | ups 0.89 | wpb 7437.5 | bsz 632 | num_updates 3659 | lr 0.000457375 | gnorm 0.487 | loss_scale 8 | train_wall 58 | gb_free 5.6 | wall 4704\n",
            "2022-08-28 22:08:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 038:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:08:37 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2022-08-28 22:08:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  99% 98/99 [00:55<00:00,  1.80it/s, loss=2.494, nll_loss=0.375, ppl=1.3, wps=6711.8, ups=0.91, wpb=7353.1, bsz=637.3, num_updates=3700, lr=0.0004625, gnorm=0.508, loss_scale=8, train_wall=56, gb_free=5.6, wall=4728]2022-08-28 22:09:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.52s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.26s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.01s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.18it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:09:38 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 8.426 | nll_loss 7.014 | ppl 129.28 | bleu 30.69 | wps 1142.6 | wpb 1325.2 | bsz 177.2 | num_updates 3758 | best_loss 8.426\n",
            "2022-08-28 22:09:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 3758 updates\n",
            "2022-08-28 22:09:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-28 22:09:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-28 22:11:17 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint38.pt (epoch 38 @ 3758 updates, score 8.426) (writing took 98.97867107499951 seconds)\n",
            "2022-08-28 22:11:17 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2022-08-28 22:11:17 | INFO | train | epoch 038 | loss 2.475 | nll_loss 0.355 | ppl 1.28 | wps 4614.7 | ups 0.62 | wpb 7437.5 | bsz 632 | num_updates 3758 | lr 0.00046975 | gnorm 0.46 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 4863\n",
            "2022-08-28 22:11:17 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 039:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:11:17 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2022-08-28 22:11:17 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  99% 98/99 [00:57<00:00,  1.82it/s, loss=2.457, nll_loss=0.339, ppl=1.26, wps=4643.5, ups=0.62, wpb=7509.3, bsz=638.2, num_updates=3800, lr=0.000475, gnorm=0.429, loss_scale=8, train_wall=57, gb_free=5.4, wall=4889]2022-08-28 22:12:15 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.95s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  50% 2/4 [00:03<00:02,  1.45s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.13s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.03it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:12:19 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 8.75 | nll_loss 7.384 | ppl 167.05 | bleu 29.26 | wps 1060 | wpb 1325.2 | bsz 177.2 | num_updates 3857 | best_loss 8.426\n",
            "2022-08-28 22:12:19 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 3857 updates\n",
            "2022-08-28 22:12:19 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-28 22:12:32 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-28 22:13:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint39.pt (epoch 39 @ 3857 updates, score 8.75) (writing took 46.39232392299982 seconds)\n",
            "2022-08-28 22:13:06 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2022-08-28 22:13:06 | INFO | train | epoch 039 | loss 2.458 | nll_loss 0.342 | ppl 1.27 | wps 6767 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 3857 | lr 0.000482125 | gnorm 0.454 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 4972\n",
            "2022-08-28 22:13:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 040:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:13:06 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2022-08-28 22:13:06 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  99% 98/99 [00:56<00:00,  1.85it/s, loss=2.455, nll_loss=0.338, ppl=1.26, wps=6794.2, ups=0.92, wpb=7377.4, bsz=621.6, num_updates=3900, lr=0.0004875, gnorm=0.446, loss_scale=8, train_wall=56, gb_free=5.4, wall=4998]2022-08-28 22:14:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  25% 1/4 [00:02<00:06,  2.26s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.78s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  75% 3/4 [00:04<00:01,  1.33s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 4/4 [00:05<00:00,  1.08s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:14:08 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 8.75 | nll_loss 7.385 | ppl 167.12 | bleu 26.1 | wps 928.7 | wpb 1325.2 | bsz 177.2 | num_updates 3956 | best_loss 8.426\n",
            "2022-08-28 22:14:08 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 3956 updates\n",
            "2022-08-28 22:14:08 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-28 22:14:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-28 22:14:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint40.pt (epoch 40 @ 3956 updates, score 8.75) (writing took 46.875614863999544 seconds)\n",
            "2022-08-28 22:14:55 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2022-08-28 22:14:55 | INFO | train | epoch 040 | loss 2.44 | nll_loss 0.325 | ppl 1.25 | wps 6727.6 | ups 0.9 | wpb 7437.5 | bsz 632 | num_updates 3956 | lr 0.0004945 | gnorm 0.408 | loss_scale 8 | train_wall 55 | gb_free 5.5 | wall 5082\n",
            "2022-08-28 22:14:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 041:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:14:55 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2022-08-28 22:14:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  99% 98/99 [00:56<00:00,  1.86it/s, loss=2.445, nll_loss=0.333, ppl=1.26, wps=6849, ups=0.91, wpb=7496, bsz=645.1, num_updates=4000, lr=0.0005, gnorm=0.434, loss_scale=8, train_wall=56, gb_free=5.4, wall=5107]2022-08-28 22:15:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.63s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.38s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.10s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:15:56 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 8.552 | nll_loss 7.164 | ppl 143.42 | bleu 28.53 | wps 1016.1 | wpb 1325.2 | bsz 177.2 | num_updates 4055 | best_loss 8.426\n",
            "2022-08-28 22:15:56 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 4055 updates\n",
            "2022-08-28 22:15:56 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-28 22:16:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-28 22:16:42 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint41.pt (epoch 41 @ 4055 updates, score 8.552) (writing took 45.43231651699989 seconds)\n",
            "2022-08-28 22:16:42 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2022-08-28 22:16:42 | INFO | train | epoch 041 | loss 2.44 | nll_loss 0.328 | ppl 1.26 | wps 6898.2 | ups 0.93 | wpb 7437.5 | bsz 632 | num_updates 4055 | lr 0.000496598 | gnorm 0.433 | loss_scale 8 | train_wall 55 | gb_free 5.5 | wall 5189\n",
            "2022-08-28 22:16:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 042:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:16:42 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2022-08-28 22:16:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  99% 98/99 [00:57<00:00,  1.85it/s, loss=2.434, nll_loss=0.323, ppl=1.25, wps=6737.1, ups=0.92, wpb=7329.8, bsz=626.6, num_updates=4100, lr=0.000493865, gnorm=0.423, loss_scale=8, train_wall=56, gb_free=5.5, wall=5216]2022-08-28 22:17:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.61s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.30s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.04it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.23it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:17:44 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 8.575 | nll_loss 7.201 | ppl 147.09 | bleu 29.68 | wps 1228.1 | wpb 1325.2 | bsz 177.2 | num_updates 4154 | best_loss 8.426\n",
            "2022-08-28 22:17:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 4154 updates\n",
            "2022-08-28 22:17:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-28 22:17:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-28 22:18:30 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint42.pt (epoch 42 @ 4154 updates, score 8.575) (writing took 45.87669469699995 seconds)\n",
            "2022-08-28 22:18:30 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2022-08-28 22:18:30 | INFO | train | epoch 042 | loss 2.426 | nll_loss 0.316 | ppl 1.24 | wps 6803 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 4154 | lr 0.000490644 | gnorm 0.415 | loss_scale 8 | train_wall 56 | gb_free 5.4 | wall 5297\n",
            "2022-08-28 22:18:30 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 043:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:18:31 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2022-08-28 22:18:31 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  99% 98/99 [00:57<00:00,  1.77it/s, loss=2.418, nll_loss=0.309, ppl=1.24, wps=6857.9, ups=0.92, wpb=7437.7, bsz=634.6, num_updates=4200, lr=0.00048795, gnorm=0.403, loss_scale=8, train_wall=56, gb_free=5.5, wall=5325]2022-08-28 22:19:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  25% 1/4 [00:01<00:05,  1.87s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  50% 2/4 [00:03<00:02,  1.48s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.13s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.08it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:19:33 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 8.847 | nll_loss 7.521 | ppl 183.73 | bleu 26.77 | wps 1083.2 | wpb 1325.2 | bsz 177.2 | num_updates 4253 | best_loss 8.426\n",
            "2022-08-28 22:19:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 4253 updates\n",
            "2022-08-28 22:19:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-28 22:19:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-28 22:20:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint43.pt (epoch 43 @ 4253 updates, score 8.847) (writing took 47.083208632000606 seconds)\n",
            "2022-08-28 22:20:20 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2022-08-28 22:20:20 | INFO | train | epoch 043 | loss 2.408 | nll_loss 0.302 | ppl 1.23 | wps 6708.6 | ups 0.9 | wpb 7437.5 | bsz 632 | num_updates 4253 | lr 0.0004849 | gnorm 0.388 | loss_scale 8 | train_wall 55 | gb_free 5.4 | wall 5407\n",
            "2022-08-28 22:20:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 044:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:20:20 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2022-08-28 22:20:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  99% 98/99 [00:55<00:00,  1.78it/s, loss=2.4, nll_loss=0.296, ppl=1.23, wps=6904.1, ups=0.92, wpb=7498.8, bsz=626.6, num_updates=4300, lr=0.000482243, gnorm=0.371, loss_scale=8, train_wall=56, gb_free=5.4, wall=5433]2022-08-28 22:21:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.66s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.39s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.09s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.06it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:21:21 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 8.506 | nll_loss 7.13 | ppl 140.09 | bleu 27.87 | wps 1025.6 | wpb 1325.2 | bsz 177.2 | num_updates 4352 | best_loss 8.426\n",
            "2022-08-28 22:21:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 4352 updates\n",
            "2022-08-28 22:21:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-28 22:21:33 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-28 22:22:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint44.pt (epoch 44 @ 4352 updates, score 8.506) (writing took 45.30836124600046 seconds)\n",
            "2022-08-28 22:22:06 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2022-08-28 22:22:06 | INFO | train | epoch 044 | loss 2.394 | nll_loss 0.291 | ppl 1.22 | wps 6940.9 | ups 0.93 | wpb 7437.5 | bsz 632 | num_updates 4352 | lr 0.000479353 | gnorm 0.365 | loss_scale 8 | train_wall 55 | gb_free 5.8 | wall 5513\n",
            "2022-08-28 22:22:06 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 045:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:22:09 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2022-08-28 22:22:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  99% 98/99 [00:59<00:00,  1.79it/s, loss=2.395, nll_loss=0.294, ppl=1.23, wps=6684.6, ups=0.91, wpb=7371.6, bsz=646.2, num_updates=4400, lr=0.000476731, gnorm=0.378, loss_scale=8, train_wall=56, gb_free=5.5, wall=5544]2022-08-28 22:23:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.54s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.26s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.08it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.24it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:23:10 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 8.834 | nll_loss 7.506 | ppl 181.79 | bleu 28.03 | wps 1240.6 | wpb 1325.2 | bsz 177.2 | num_updates 4451 | best_loss 8.426\n",
            "2022-08-28 22:23:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 4451 updates\n",
            "2022-08-28 22:23:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-28 22:23:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-28 22:23:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint45.pt (epoch 45 @ 4451 updates, score 8.834) (writing took 44.68307066499983 seconds)\n",
            "2022-08-28 22:23:54 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2022-08-28 22:23:55 | INFO | train | epoch 045 | loss 2.389 | nll_loss 0.289 | ppl 1.22 | wps 6780.2 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 4451 | lr 0.000473992 | gnorm 0.371 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 5621\n",
            "2022-08-28 22:23:55 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 046:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:23:55 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2022-08-28 22:23:55 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  99% 98/99 [00:57<00:00,  1.78it/s, loss=2.379, nll_loss=0.28, ppl=1.21, wps=6999.5, ups=0.93, wpb=7494.7, bsz=638, num_updates=4500, lr=0.000471405, gnorm=0.351, loss_scale=8, train_wall=56, gb_free=5.5, wall=5651]2022-08-28 22:24:53 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.65s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.40s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.05s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:24:57 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 8.892 | nll_loss 7.576 | ppl 190.83 | bleu 27.23 | wps 1129 | wpb 1325.2 | bsz 177.2 | num_updates 4550 | best_loss 8.426\n",
            "2022-08-28 22:24:57 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 4550 updates\n",
            "2022-08-28 22:24:57 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-28 22:25:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-28 22:25:43 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint46.pt (epoch 46 @ 4550 updates, score 8.892) (writing took 45.92367098700015 seconds)\n",
            "2022-08-28 22:25:43 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2022-08-28 22:25:43 | INFO | train | epoch 046 | loss 2.373 | nll_loss 0.276 | ppl 1.21 | wps 6809 | ups 0.92 | wpb 7437.5 | bsz 632 | num_updates 4550 | lr 0.000468807 | gnorm 0.342 | loss_scale 8 | train_wall 55 | gb_free 5.5 | wall 5729\n",
            "2022-08-28 22:25:43 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 047:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:25:43 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2022-08-28 22:25:43 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  99% 98/99 [00:56<00:00,  1.80it/s, loss=2.37, nll_loss=0.275, ppl=1.21, wps=6886.9, ups=0.92, wpb=7457.4, bsz=604.9, num_updates=4600, lr=0.000466252, gnorm=0.349, loss_scale=8, train_wall=56, gb_free=5.6, wall=5759]2022-08-28 22:26:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.65s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.31s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.05it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.28it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:26:44 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 8.736 | nll_loss 7.416 | ppl 170.77 | bleu 27.4 | wps 1291.4 | wpb 1325.2 | bsz 177.2 | num_updates 4649 | best_loss 8.426\n",
            "2022-08-28 22:26:44 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 4649 updates\n",
            "2022-08-28 22:26:44 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-28 22:26:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-28 22:27:31 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint47.pt (epoch 47 @ 4649 updates, score 8.736) (writing took 47.07298540800002 seconds)\n",
            "2022-08-28 22:27:31 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2022-08-28 22:27:31 | INFO | train | epoch 047 | loss 2.366 | nll_loss 0.273 | ppl 1.21 | wps 6797.8 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 4649 | lr 0.000463789 | gnorm 0.34 | loss_scale 8 | train_wall 56 | gb_free 5.6 | wall 5838\n",
            "2022-08-28 22:27:31 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 048:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:27:32 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2022-08-28 22:27:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  99% 98/99 [00:56<00:00,  1.83it/s, loss=2.361, nll_loss=0.27, ppl=1.21, wps=6807.9, ups=0.92, wpb=7394.9, bsz=661.8, num_updates=4700, lr=0.000461266, gnorm=0.327, loss_scale=8, train_wall=56, gb_free=5.6, wall=5868]2022-08-28 22:28:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.59s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.29s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.03it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.19it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:28:32 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 8.749 | nll_loss 7.44 | ppl 173.69 | bleu 28.43 | wps 1182.6 | wpb 1325.2 | bsz 177.2 | num_updates 4748 | best_loss 8.426\n",
            "2022-08-28 22:28:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 4748 updates\n",
            "2022-08-28 22:28:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-28 22:28:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-28 22:29:20 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint48.pt (epoch 48 @ 4748 updates, score 8.749) (writing took 47.64025428700006 seconds)\n",
            "2022-08-28 22:29:20 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2022-08-28 22:29:20 | INFO | train | epoch 048 | loss 2.356 | nll_loss 0.265 | ppl 1.2 | wps 6780.8 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 4748 | lr 0.000458928 | gnorm 0.329 | loss_scale 8 | train_wall 55 | gb_free 5.5 | wall 5946\n",
            "2022-08-28 22:29:20 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 049:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:29:20 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2022-08-28 22:29:20 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  99% 98/99 [00:57<00:00,  1.80it/s, loss=2.35, nll_loss=0.261, ppl=1.2, wps=6861.6, ups=0.91, wpb=7522.2, bsz=615.1, num_updates=4800, lr=0.000456435, gnorm=0.323, loss_scale=8, train_wall=56, gb_free=5.5, wall=5977]2022-08-28 22:30:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.61s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  50% 2/4 [00:02<00:02,  1.30s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  75% 3/4 [00:03<00:00,  1.01it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 4/4 [00:03<00:00,  1.16it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:30:21 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 8.687 | nll_loss 7.37 | ppl 165.43 | bleu 28.21 | wps 1152.2 | wpb 1325.2 | bsz 177.2 | num_updates 4847 | best_loss 8.426\n",
            "2022-08-28 22:30:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 4847 updates\n",
            "2022-08-28 22:30:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-28 22:30:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-28 22:31:07 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint49.pt (epoch 49 @ 4847 updates, score 8.687) (writing took 45.75664790800056 seconds)\n",
            "2022-08-28 22:31:07 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2022-08-28 22:31:07 | INFO | train | epoch 049 | loss 2.35 | nll_loss 0.262 | ppl 1.2 | wps 6842.6 | ups 0.92 | wpb 7437.5 | bsz 632 | num_updates 4847 | lr 0.000454217 | gnorm 0.328 | loss_scale 8 | train_wall 55 | gb_free 5.6 | wall 6054\n",
            "2022-08-28 22:31:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 99\n",
            "epoch 050:   0% 0/99 [00:00<?, ?it/s]2022-08-28 22:31:09 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2022-08-28 22:31:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  99% 98/99 [00:57<00:00,  1.79it/s, loss=2.345, nll_loss=0.258, ppl=1.2, wps=6786.8, ups=0.92, wpb=7411, bsz=640.6, num_updates=4900, lr=0.000451754, gnorm=0.318, loss_scale=8, train_wall=56, gb_free=5.4, wall=6086]2022-08-28 22:32:06 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  25% 1/4 [00:01<00:04,  1.64s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  50% 2/4 [00:03<00:03,  1.50s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  75% 3/4 [00:03<00:01,  1.13s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 4/4 [00:04<00:00,  1.05it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-28 22:32:10 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 8.724 | nll_loss 7.438 | ppl 173.37 | bleu 27.81 | wps 984.9 | wpb 1325.2 | bsz 177.2 | num_updates 4946 | best_loss 8.426\n",
            "2022-08-28 22:32:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 4946 updates\n",
            "2022-08-28 22:32:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-28 22:32:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-28 22:32:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint50.pt (epoch 50 @ 4946 updates, score 8.724) (writing took 45.26650771200002 seconds)\n",
            "2022-08-28 22:32:55 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2022-08-28 22:32:56 | INFO | train | epoch 050 | loss 2.342 | nll_loss 0.256 | ppl 1.19 | wps 6801.3 | ups 0.91 | wpb 7437.5 | bsz 632 | num_updates 4946 | lr 0.000449648 | gnorm 0.314 | loss_scale 8 | train_wall 56 | gb_free 5.5 | wall 6162\n",
            "2022-08-28 22:32:56 | INFO | fairseq_cli.train | done training in 6162.2 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:16:00.904977Z",
          "iopub.execute_input": "2022-07-24T19:16:00.905444Z",
          "iopub.status.idle": "2022-07-24T19:16:01.196986Z",
          "shell.execute_reply.started": "2022-07-24T19:16:00.905403Z",
          "shell.execute_reply": "2022-07-24T19:16:01.195842Z"
        },
        "trusted": true,
        "id": "yL7nIungdwcw",
        "outputId": "3c0c4eff-8684-45a2-f972-1d30c8850017",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdata-bin\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang hsb --target-lang dsb \\\n",
        "      --beam 5 data-bin/wmt_hsb_dsb | grep -P \"D-[0-9]+\" | cut -f3 > target.txt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:05:22.918437Z",
          "iopub.execute_input": "2022-07-24T18:05:22.919517Z",
          "iopub.status.idle": "2022-07-24T18:06:13.344631Z",
          "shell.execute_reply.started": "2022-07-24T18:05:22.919454Z",
          "shell.execute_reply": "2022-07-24T18:06:13.343313Z"
        },
        "trusted": true,
        "id": "PdGwekMXdwcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b57ff2d-1d7d-40fc-ced8-4b8097f9833c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 22:33:11 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_hsb_dsb', 'source_lang': 'hsb', 'target_lang': 'dsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 22:33:11 | INFO | fairseq.tasks.translation | [hsb] dictionary: 103512 types\n",
            "2022-08-28 22:33:11 | INFO | fairseq.tasks.translation | [dsb] dictionary: 104024 types\n",
            "2022-08-28 22:33:11 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-28 22:33:32 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-28 22:33:32 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-28 22:33:32 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-28 22:33:41 | INFO | fairseq_cli.interactive | Total time: 29.928 seconds; translation time: 6.861\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cat target.txt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:13.348554Z",
          "iopub.execute_input": "2022-07-24T18:06:13.348954Z",
          "iopub.status.idle": "2022-07-24T18:06:14.121539Z",
          "shell.execute_reply.started": "2022-07-24T18:06:13.348910Z",
          "shell.execute_reply": "2022-07-24T18:06:14.108183Z"
        },
        "trusted": true,
        "id": "BtsGN-Fydwcy",
        "outputId": "7522bdb6-3d0a-459d-cdf4-a783d0632c6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ja změju dowol.\n",
            "Ja se smjaś.\n",
            "Z wjaselim se styriłopjenkate pśeto wón jo gnadny a jogo dobrota trajo nimjernje.\n",
            "zmóžniś a kozy su lažki wułojśe.\n",
            "Kněžo, wjelike se, pśeto ja gódny njejsom, aby ty pód teju kšywom gromadu a togodla sebje samego byli, za kreatiwitu.\n",
            "W piwnicnem kašćiku laže piś.\n",
            "To njejo mój płašć.\n",
            "Na tyma dwěma su ceły kazń a ewangelska a zadobyś jo.\n",
            "Jogo knigły su se do 25 rěcow.\n",
            "W toś tom casu jo zachopił Brězan, se na prědnem źěle serbskego gibanja wobźěliś.\n",
            "Z lěta papjeru do 1944 jo musał ako wójak Nimskego wójska a jo pśišeł ako taki wjeliki wogeń.\n",
            "Pó styriłopjenkate kulkow jo se wrośił do 1946 1946 jo a statkował how až do lěta 1948 ako młoźinski Domowiny.\n",
            "Pó jogo wuzwólijo jo dostał Nimski koło archiw jogo literarne konwencije kótarež wopśimjejo była wjelike woły a wideo.\n",
            "Pó prědnem procowanjach w Praze jo se raźiło w pólskem měsće wiźeł?\n",
            "Jurij Brězan jo se naroźił 9. junija 1916 we Worklecańskej chórowni doma, měrca 2006 w Kamjeńcu, w Kamjeńcu, chórowni był.\n",
            "Na styriłopjenkate naroźenja jo jogo wiźeł?\n",
            "Pózdźej dajo se swóje serbske mě na zakłaźe kazni wó zachowanju pšawow serbskeje ludnosći z 23. dnja 1948 jo zgromaźinu zapisaś.\n",
            "Jurij Brězan płaśi ako nabejny serbski spisowaśel drugeje połojcy 20. lětstotka.\n",
            "Ze swójimi srědkami teke w nimskej rěcy prědne twari serbskej literaturje k nimskej rěcy.\n",
            "Pśedewšym jo pisał znate wulicowańka a źiśece knigłow.\n",
            "Wót lěta wiźeł?\n",
            "W Drježdźanach swóju maturu jo se jomu wótpokazany.\n",
            "Za swóje rane twóriśelske dowólona.\n",
            "Wón jo był rownocasnje ze zakazom dwójo- we Łužycy zamóžne.\n",
            "Pó swójom wrośenju do Łužycy jo se swójogo politiskego angažementa dla wóspjet w Drježdźanach popajźony.\n",
            "Hyšći w samskem lěśe jo se wjednik w Serbskem zastojnstwje za kulturu a ludowe kubłanje krajnego kněžaŕstwa Sakskeje.\n",
            "W słužbnem prědnem kněžaŕstwowego rady jo była zagronity za prědnu rozgłos a film wjerśeł.\n",
            "Lětosa su zjadnośonej Nimskeje seno pśistupijo Jurij Brězan w lěśe zamóžne.\n",
            "Wót lěta 1949 jo statkował ako lichy spisowaśel.\n",
            "W lěśe 1964 stanje jo se z cłonkom Nimskego mócnaŕstwa.\n",
            "Z cłonkom wuměłstwa w lěśe jo se w lěśe 1965.\n",
            "Wiceprezident Zwězka bura DDR jo była Brězan wót 1969 do njeje.\n",
            "Jurij Brězan jo dostał za cas DDR rěd tšojenjow.\n",
            "1951, 1964 a 1976 jo wón narodne myto.\n",
            "W lěśe 1962 jo wón dostał Myto Ćišinskego.\n",
            "1974 jo dostał wón wuznamjenjenje Karla Marxowej\n",
            "Do wjele swójich romanow a powěsćow zapśimujo Brězan k wěsći elementy.\n",
            "K jogo nejwuznamnjejšym bura słušaju bura seno pušćiła.\n",
            "Krabatowej tematice jo se zajmowało maminorěcne Jurij Brězan teke z bogatego góle serbskich ludowych powěsćow.\n",
            "Na zachopjeńku jo zajmowało 1955 seno wót Mjertyna znate do nimšćiny.\n",
            "Až do swójeje smjerśi jo Jurij Brězan blisko swójeje rodneje jsy Worklecy žywy.\n",
            "Južo někotare lěta do smjerśi Brězana byś? To jo stojało na zajm na jogo priwatnych pódał.\n",
            "Z tym gódnośi wuznamny serbsku literaturu rownocasnje rownocasnje ako pśipóznaty źěl swětoweje wójny literatury.\n",
            "Jogo grozny plan se póraźijo.\n",
            "Wótwónoźćo se murja wóstanjo.\n",
            "Ako pak se w nalěśu pórědnje pada, se naraz wšykno změnijo.\n",
            "zmóžniś co jo se zajmowało raz a dowólijo twaŕ nejrědnjejša až by kraja stwórił.\n",
            "Mała Sonja pak caka na swójogo nana, kótaryž zasej raz nadgóźiny cyni.\n",
            "Aby se cas malsnjej susednej wumysli sebje pśecej zasej nowe wobrěd byli.\n",
            "Nyks, muski w swójom groźe pód wódu wósebnosći, což jo gdy raz do rěki padnuło.\n",
            "Wšykno, lěc na gałuzy kulowate karpy.\n",
            "Toś te pak jo sebje styriłopjenkate lud wokoło wjelikego města.\n",
            "A ako toś te ratwy swěśi, jim wódny muž tucne su žywe byli.\n",
            "Ale wóni su jogo wě.\n",
            "W górucym nikula wugónijo.\n",
            "Kraj su styriłopjenkate lud zamóžne.\n",
            "Lěc drje jo zazdaśim za tym?\n",
            "Źowćo Sonja wzejo młodych pśiglědowarjow sobu na drogowanje do kraja domoj.\n",
            "Wóna jich do jadnanja na jawišću aktiwnje zapśěgnjo.\n",
            "Pušć prědne a raz, co se stawa.\n",
            "Z styriłopjenkate a seno su se źiśi gótujo.\n",
            "Wótwónoźćo kusk pšaša se póznaś.\n",
            "„Co to jo – bura pšaša se zachopiła.\n",
            "To se mě spódoba.\n",
            "Dłujko njetrajo a Witko jo kuždemu jaden wjeliki lod.\n",
            "Kak se pinguin wobglědaś.\n",
            "To se wjelgin wjaseli.\n",
            "styriłopjenkate pśechwatanka jo se raźiła.\n",
            "Zrazom pinguin a se rozžognujo.\n",
            "Ale měło se wě.\n",
            "Wótwónoźćo a styriłopjenkate\n",
            "Na zachopjeńku se wósebnje derje luźi?\n",
            "Wšykno jo se ze zmysłowymi nazgónjenjami.\n",
            "Kótary lětny cas a mjasec mamy?\n",
            "A do Krabatowego młyna.\n",
            "A kak wuglěda ze serbšćinu we jsy?\n",
            "A z styriłopjenkate\n",
            "Tak dajśo se zadobyś\n",
            "Za basleńku knigłowego ma se dołojce napisany material wobstaraś.\n",
            "Nowe serbskorěcne źiśece pórucenja wótměju se w pśichoźe na jawišću.\n",
            "Źiśece pśebywalnišća a zakładne šule mógu se w Nimsko-Serbskim ludowem źiwadle pla kněni Li za pśedstajenje pśizjawiś.\n",
            "„Snaź mam znate stuka pinguin a dej se nejpjerwjej raz sednuś.\n",
            "Na pšosbu su źiśi a młodostne pilnje znate graśe rady znowego.\n",
            "Z pomocu wabjeńskeje pupkow a dalšnych wobchowali ako teke z mimiku a gestiku se dorozměśe se z małymi wuknjeńskimi lěcrownož akle serbski wuknuli.\n",
            "Te su zwětšego dwójorěcnje serbski a nimski – a lětosa samo hyšći raz.\n",
            "„Tam jo wšuźi lod a sněg a zymne zwěrnje tam jo groniła cłowjek.\n",
            "A tak źiwadło pśez swóje intensiwne a wósebne wašnju hyšći dłujko na źiśi statkujo.\n",
            "Take knigły móžo byś pógóń, až se teke doma abo w źiśowni źiwadło grajo.\n",
            "Dwa mjeńšym ruma a nejmjeńša kupadło rěšyś, do pěślětnem źiśam k dispoziciji.\n",
            "bura Wam how pšašanja, z kótarymiž móžośo se ze swójim góleśim do rozgrona daś.\n",
            "Lěc profesionelne abo lajske źiwadło – z połnym wótmachom a wjaselim nad rěcu se wšykne sobustatkujuce měś, aby nejmjeńšych za źiwadło we wšyknych swójich formach teke za serbsku rěc ako teke za serbsku rěc.\n",
            "How pósrědnja se wěda na žywe a naglědne wašnju, w nowych zwiskach a nagronje wšake stwórił.\n",
            "Wótwónoźćo źiśi direktnje a jo bura až by je z nimi połdnjo abo spontanje serbski powědali.\n",
            "Witko jomu źašo: Taki raz, gaž se ja taki rědny niźi na mójej njespóznajom.\n",
            "W nowem styriłopjenkate cujo se susednej a ma znate za nowym rěcu dokulaž jo jeje nejlěpša pśijaśelka pšec ma.\n",
            "Toś tu inscenaciju woglěda w toś tom lěśe k pśiglědowarjow, mjazy nimi gólcy a źowća ze serbskich źiśownjow a prědne wěcejrěcnosći w toś tom lěśe klěb.\n",
            "W zajźonych lětźasetkach su pokazali serbske a serbski wuknjece źiśi a młodostne někotarych generacijow wěcej ako 30 wšakich stronow,\n",
            "W zachadnych lětźasetkach su skóro 30 wšake inscenacije na kótarychž su se wobźělili cełe generacije serbskich źiśi a młodostnych.\n",
            "Ze wuknikami budyšyńskego Serbskego gymnazija a serbskeju wušeju šulowu Worklecy a Budyšyn jo pisała graśe „W putach carnego gójca!\n",
            "Rowno tak rady wobglěduju źiśi teke drugich, kak wšake role sćelo a ze wšakimi wuměłskimi srědkami napněte zapśimujuce wulicuju.\n",
            "Źiśi su sebje wopytali zdał zaběraju se z tekstom, kenž jo sebje samemu kostimy a rekwizity – ceło w pšawem źiwadle.\n",
            "Wašo měnjenja, a skazanki pósćelśo pósćelśo pšosym na Postowem naměsće 2 w Budyšynje.\n",
            "Ale wjerašk kuždego lěta jo stawnje zasej programoju, w kótaremž se za źiśi na žywe, wašnju jawišćowe elementy na muskich elementy na reje a reje zwězuju.\n",
            "Pód nawjedowanim serbskego prědnu Měrka scełego pśedsedaŕki towaristwa prědne Hrjehorjoweje a Susann jo graśe serbskich 18 serbskich źiśi a młodostnych serbskeje wósady pśepšosujo.\n",
            "zmóžniś mógu na małych pśiglědowarjow direktnje reagěrowaś, jich emocije a reflektěrowaś a jo snaź aktiwnje do jadnanja na jadnanja na jawišću wopśimjeś.\n",
            "Tak som była ze swójimi zaśišćami ale mógu měło objekty w zgromadnosći pśeźěłaś a se z drugimi groblu kenž su byli wuměniś.\n",
            "Scełego jo tak, až wšykne źiśi kubłanišća – pótakem teke dalšne 20 góletkownje a zlubijo kupki - serbsku kulturu prědne serbske spiwy a serbske nałogi.\n",
            "NSLŹ jězdźi z toś teju graśe wědobnje do źiśecych dnjownych pśebywalnišćach wobswět aby pó wobměrje publikum pśigronił.\n",
            "Dokulaž wužywa serbske źiwadło wjele wěcejrěcnosći srědkow ako mimiku, gestiku a atraktiwne spušćiś, k znaglědnjenju mógu źiśi jadnanju źiśi jadnanju zwětšego bźez slědowaś, lěcrownož kužde słowo wiźeś.\n",
            "A pótom prědne sebje někotare spiwy – mjazy drugim k jatšam su se wzeli pśi comž jich wuchac jano kwětki jano kwětki teke za wěste.\n",
            "Skóro pójdu zasej mjelcyno pó jatšowneju wódu, pózdźej jo sebje spódobało, kak se kopica za chódotypalenje pśigótujo a kak se jich njeslěpił.\n",
            "Wót 9. julija až do 12. julija 2015 wótmějo se w Budyšynje, Chrósćicach a Hochozy zasej mjazynarodny folklorny festiwal dostaś, kenž stawnje tysace gósći tysace gósći a wěcej.\n",
            "We wopśimjeśu su slědujuce dypki zapisane.\n",
            "Za źiśi su se slědujuce zajmowało zmóžniś jo groniła sejźeli a papjeru dowólona.\n",
            "Halo, Witko, ty mě co?\n",
            "Mam nowego pśijaśela – jo.\n",
            "Wótwónoźćo pšaša se Witko, něco wiźeł?\n",
            "Kak? „W boku se smjejucy Hana něnto pšaša.\n",
            "Jo, tak móžoš groniś.\n",
            "Wótwónoźćo bura jo groniła kóńc.\n",
            "Cogodla styriłopjenkate pśecej w carnem wuglědajo.\n",
            "Coš do źiwadła hyś?\n",
            "Něnt wulicujo jomu Hana wó njom na jawišću a wó stawiznach, kótarež graju.\n",
            "Aha, tam jo zawěsće rědnje.\n",
            "Ale ja njok do źiwadła.\n",
            "Ja cu zasej domoj.\n",
            "Wón bydli na pódpołdnjowem jo.\n",
            "Na to njejźo wiźeś śi tam póznaś.\n",
            "Něnt jo měło seno piś.\n",
            "Mam ga ceste pjerje a tłustu swětło pód kóžu.\n",
            "To mě nicht wuglědajo.\n",
            "Pla was jo wjelgin rědnje.\n",
            "Łuki su pisane a kwětki tak rědnje wideo.\n",
            "Take něco njejsom znał.\n",
            "A tužnje jo južo prědne seno pak musym zasej njerěšna.\n",
            "How wšak jo tak wjelgin śopło.\n",
            "O, jo mě nicht piś.\n",
            "Nejskerjej som wuglědajo.\n",
            "Witko ma ideju.\n",
            "Pócakaj, mam něco zymnego za tebje.\n",
            "To śi zawěsće pomožo.\n",
            "Hm, jo ten dobry a tak kšasnje było.\n",
            "To jo był pśechwatanje.\n",
            "Wšym tśom dobry lod wuběrnje šmekujo, nic ga?\n",
            "Ale musym hyś.\n",
            "Měj se wě.\n",
            "Snaź su musali raz na pódpołdnjowem dowólona.\n",
            "aptejka jo se rědnje, zajmowało seno a Witko a Witko jo za mlince.\n",
            "Zeleny styriłopjenkate kulkow ze dowólona. To jo měło seno zmóžniś pó kótarejž su ze strony pśesegajucym statkowanim.\n",
            "Pśi baslenjeju prědne tak.\n",
            "Něnto nicht na wobej slěznem boku srjejźi dowólona.\n",
            "Zatkaj dłujki kóńc kusk do balonka a zazdaśim jen ze sprocniwosću\n",
            "dowólona.\n",
            "Gaž sy namakała do togo južo raz pśesegajuce źo to lažčej.\n",
            "Pón styriłopjenkate až njeby pówětš pówětš zasej njerěšna.\n",
            "Źiwaj na to, až se torm kwětk.\n",
            "Gusy a kacki plěwaju na wóźe wupśestrějo.\n",
            "Beno, styriłopjenkate a wiźeł?\n",
            "Staj kónje a krowy na pastwu!\n",
            "Stajśo kónje a krowy na pastwu!\n",
            "Kótare zwěrjeta se hyšći dajo.\n",
            "Mě se burski dwór spódoba.\n",
            "Wótwónoźćo se na dopomnjeś.\n",
            "Wótwónoźćo se!\n",
            "Pójśo, raz se do zwiska.\n",
            "Pójśo, raz se do zwiska.\n",
            "Wótwónoźćo se!\n",
            "Stańśo a stajśo stoł k luźi?\n",
            "Źomy a wiźeł?\n",
            "Źomy doprědka a naslědk.\n",
            "Zwigniśo pšawu a lěwu wuglědajo.\n",
            "Stajśo pšawu a lěwu jo doprědka!\n",
            "Stajśo pšawu a lěwu jo słyšali,\n",
            "aptejka a se wě.\n",
            "Sedni se na prědnem měsće.\n",
            "Wótwónoźćo se na dopomnjeś.\n",
            "Su měło tśmjel a sednjo se na twój nos.\n",
            "Stańśo a styriłopjenkate\n",
            "Něnto su spóznali styriłopjenkate a su jen nad krejz zawěra.\n",
            "Malsnje a śicho naše wěcy.\n",
            "Wótwónoźćo a prědne napšawo a nalěwo jadnak wjele… – k rozwjaselenju guslujo wuchace z kórba?\n",
            "Myj se wě.\n",
            "Wótwónoźćo se wě.\n",
            "Wótwónoźćo se z wiźeł?\n",
            "Źinsa su musali a senowej w našej źiśeceju kuchni.\n",
            "My aptejka a manifesće.\n",
            "Wócyń spižka a wzej wjeliki módry gjarnc z groblu zachopiś?\n",
            "Ow, mloko se wari!\n",
            "Wótwónoźćo se styriłopjenkate\n",
            "Barbara se w sportowej rumje.\n",
            "Wótwónoźćo se wy na góńtwu.\n",
            "To se śicho a zasobu!\n",
            "Chtož jo gótowy, sednjo se na njom wudali.\n",
            "Pśistupśo se pó zadobyś\n",
            "Pśistupśo se w bliskosći.\n",
            "Pśistupśo se w bliskosći.\n",
            "Wótwónoźćo se!\n",
            "Wótwónoźćo se!\n",
            "Wótwónoźćo pšawu a lěwu wuglědajo.\n",
            "Wótwónoźćo pšawu a lěwu wuglědajo.\n",
            "Wótwónoźćo se pó wě.\n",
            "Źomy pómałem a malsnje!\n",
            "Źarž se kšuśe!\n",
            "Źaržćo se kšuśe!\n",
            "Wótwónoźćo se!\n",
            "Wótwónoźćo se!\n",
            "Wótwónoźćo se a prědnu sportowy luźi?\n",
            "Barbara a matej pisane jo.\n",
            "Wótwónoźćo se!\n",
            "Barbara se a zacyńśo wócy!\n",
            "Wótwónoźćo se a pitśu spańske\n",
            "měchy su sejźeli a do cłowjeskego natwarili.\n",
            "Mój tunel jo se zadobyś\n",
            "Su musali pěsk na se!\n",
            "Su musali pěsk na se!\n",
            "Daj se wě.\n",
            "Chto se ze mnu powědaś.\n",
            "Źarž se z wobyma wuglědajo.\n",
            "Wótwónoźćo se pó styriłopjenkate\n",
            "Wótwónoźćo se pó brjuchu njejo.\n",
            "Pśistupśo se pśi njom.\n",
            "Pśistupśo se w bliskosći.\n",
            "Wótwónoźćo se!\n",
            "Wy sćo se zadobyś\n",
            "Wótwónoźćo se na dopomnjeś.\n",
            "Pójźćo, styriłopjenkate gaž se na dopomnjeś.\n",
            "Něnto jěmy a nikomu.\n",
            "Ja styriłopjenkate a my nicht wuglědajo.\n",
            "Ja styriłopjenkate a my su śopłu žranje źo.\n",
            "Ja styriłopjenkate a my su něco wiźeł?\n",
            "Wjaselim se na lěbgodnu pójědank.\n",
            "Ja prědne a my zgromadnje.\n",
            "Scyń styriłopjenkate a nicht na blido!\n",
            "Scyń styriłopjenkate a nicht na blido!\n",
            "Wótwónoźćo sejźeli a nicht wuglědajo.\n",
            "Rozdźělśo formulary a nicht wuglědajo.\n",
            "Dajśo se wě.\n",
            "Śota měchy ja som se zadobyś\n",
            "Śota měchy ja som se zadobyś\n",
            "Nam jo pisała źěkujomy se!\n",
            "Nan a maś aptejka\n",
            "Ja styriłopjenkate a njok seno piś.\n",
            "Na zagroźe su se wiźeł?\n",
            "Mama myjo a jěźny wobrěd pśinjasom.\n",
            "Stary nan a starka na woglěd.\n",
            "Stary nan jo šło a my su wšykne doma.\n",
            "Grajomy z nanom powědaś se!\n",
            "Gronimej maśeri a bura Dobru noc!\n",
            "Comy se woblekaś.\n",
            "Woblac se!\n",
            "Woblacćo se!\n",
            "Musyš se wě.\n",
            "Wótwónoźćo se!\n",
            "Łuka se dopomnjeś.\n",
            "Wótwónoźćo a Kricki maju seno wuglědajo.\n",
            "Bóśony mógali a wjelike se zasej k nam wrośa.\n",
            "Młode ptaški se wě.\n",
            "Burja wórju a bergaŕska.\n",
            "Wjaselim se na lěśe.\n",
            "Tergaty wětš se zwiga.\n",
            "Na njebju se błyska a wiźeł?\n",
            "Pada se.\n",
            "Wótwónoźćo papjeru wuglědajo.\n",
            "Na łuce se seno źěła.\n",
            "My se wě.\n",
            "To su se kupański woblak.\n",
            "Su se sejźeli nastali.\n",
            "Wótwónoźćo se ze styriłopjenkate\n",
            "Wótwónoźćo se.\n",
            "Wótwónoźćo se.\n",
            "Źinsa se w basenku wuglědajo.\n",
            "Wětš dujo a cesto jo wjelgin wichorojte.\n",
            "W nazymje se cesto zmakali.\n",
            "Barbara sykorki a wótpowědujucy wóstanu how.\n",
            "Źiśi, nan a starki pytaju griby.\n",
            "Zyma se wě.\n",
            "Wjelike sněžynki se na móju ruku.\n",
            "Wótwónoźćo a semuška do chałupy wiźeł?\n",
            "Na gaśe se ze styriłopjenkate\n",
            "Źowća se styriłopjenkate\n",
            "Wjaselimy se na gódy.\n",
            "Musyśo se do rěda stajiś.\n",
            "Su se wrośili do jśpy,\n",
            "Ja ten zagroźe a wobšuda powjesyś.\n",
            "Wótwónoźćo a styriłopjenkate swój su nam wiźeł?\n",
            "Wšykne wjasele se na jatšy.\n",
            "A styriłopjenkate\n",
            "To pomjenjujo se wě.\n",
            "W maju a aprylu jo stwórił.\n",
            "Na papjeru se měriś, jo.\n",
            "Kopica se pali a źiśi se wjasele.\n",
            "Na chrosćańskem se teke rědnje staja.\n",
            "Majski se chyta.\n",
            "Pśez co sy se nejwěcej wě.\n",
            "Pśez co sy se nejwěcej wě.\n",
            "Powjes kulu a gwězdu na najmjetem glědał.\n",
            "Njebój se wě.\n",
            "prědne se wjerśi.\n",
            "Pjas a kócka jo w kórbiku.\n",
            "Wótwónoźćo a měchy nicht w mnjo.\n",
            "Wiźiš južo ryby a seno w mnjo.\n",
            "Kocor, kócka a seno su twaŕnišćow.\n",
            "zmóžniś a rjeśaza pytaju za futrom.\n",
            "měchy kacka a gódnośe na gaśe.\n",
            "Wótwónoźćo zmóžniś a prědnu trjebaju futrom.\n",
            "aptejka a styriłopjenkate kwiśinowe na kšywje.\n",
            "měchy kulkow a prědnu laže na łuce.\n",
            "Lětosa su sejźeli a casy grajkaju.\n",
            "Wužam a bura su we wósebnem domje.\n",
            "Wótwónoźćo se.\n",
            "Sedni se na prědnem měsće.\n",
            "Sedniśo se na dopomnjeś.\n",
            "Na styriłopjenkate a susednej laže pó prědnem a pókšywadło rědowali.\n",
            "Złośany kulkow znate myška a casy se źarže se w Dolnej Łužycy.\n",
            "To jo zeleznicu a tam jo groniła: cera za malsne awta.\n",
            "Wótwónoźćo sejźeli a prědnu kokot spiwa, gusy a kacki wobglědujomy.\n",
            "How stoje sejźeli a bura awta a wjelike aptejka a kólasa.\n",
            "grała naš blido a bura stej grała a měrny stoje w kuchni.\n",
            "Góńtwaŕ za sarny, wuchace, a źiwe swinje futer za góle do góle wjeźo.\n",
            "Wótwónoźćo se ptaškowa a narodna drastwa a jo se pśewjeźo znate pśeśěg.\n",
            "W spižce su žywe glozonki a grajne figury, rěcne pedagogiske graśa a kubłanja.\n",
            "Źiśi pjaku bura twarje grod a drogi, jo słušał měrny pěsk pěsk a natwarja wjas.\n",
            "Na styriłopjenkate jo seno a w prědnem laže žołźe, kastanije, a zastupujucy mam.\n",
            "Wótwónoźćo sarna a bura zmóžniś jatšowne rejtarje, źiwe swinju a njewjericka su gólne zwěrjeta.\n",
            "How su krowy, domoj. Ja som swinje, a jo stojało znate z rěcu gusy, kacki a kónje.\n",
            "W góli a parku stoje seno znate wiźeł?\n",
            "We jśpě stoje niske małe znate wjelike regale a spižki za grajki a graśa, wjelike blido a stoł za wójnske wjelike spódobanje.\n",
            "We wjelikem spižce su seno zbórk a papjeru dowólona.\n",
            "W zagroźe su pěskowy kašćik, wiźeł?\n",
            "W małem kuchinskem spižce su: talarje a k wěsći daś a wjelike głažki a wjelike gjarnce nabóžninu a měchy prědnu a kulturu za formy a wědobne a wužytne za formy dajo.\n",
            "Witaj k nam!\n",
            "Pśiź strowy zasej!\n",
            "Pśiźćo strowe zasej!\n",
            "Rědny kusk wóstanjo.\n",
            "Wjasołe gódy!\n",
            "Wjasołe jatšy my wam powěsći.\n",
            "To su wósoby w familiji.\n",
            "To jo naš nan.\n",
            "To jo mója maś.\n",
            "To jo naša maś.\n",
            "To stej mójej tašy.\n",
            "Starjejšej matej syna a źowku.\n",
            "Cłowjeka Syn jo gólc a źowka jo źowćko.\n",
            "Ja mam bratša kwětk.\n",
            "Ja mam sotšu, njebudu.\n",
            "To jo mój starki.\n",
            "To jo naš starki.\n",
            "To jo mója starka.\n",
            "To jo naša starka.\n",
            "Ja mam starego nana a starkeju rad.\n",
            "Wujk jo naš nicht było.\n",
            "Wótwónoźćo jo naš kóńc.\n",
            "Śota jo naša styriłopjenkate\n",
            "Wótwónoźćo jo naša piś.\n",
            "Witaj, mě groni Jurij jo.\n",
            "Ja mě Marta pśinjasć?\n",
            "Kněz Nowak jo naš nan.\n",
            "Kněni styriłopjenkate jo naša maś.\n",
            "Mója wjelika sotša jo se kněni Li njejo.\n",
            "Naša familija jo se wě.\n",
            "To jo naše bydlenje.\n",
            "Naša familija bydli na jsy.\n",
            "Naša familija bydli w měsće.\n",
            "My mamy bura dom.\n",
            "My mamy bydlenje.\n",
            "My bydlimy na burskem dwórje.\n",
            "My mamy styriłopjenkate kulkow wjelike zejźenja / Měrkow bydleńsku wugótowaś a źiśecu Budyšyńskej a źiśecu śpu.\n",
            "Mamy teke styriłopjenkate kulkow zwězany.\n",
            "How jo kóńc.\n",
            "Wótwónoźćo ma nicht wuglědajo.\n",
            "Na samsku wisy wuglědajo.\n",
            "zmóžniś ma teke kóńc.\n",
            "W žywej laže móje rukajce.\n",
            "W koridorje stoje znate ze stacijow.\n",
            "Na samsku wisy wuglědajo.\n",
            "Na samsku wisy wuglědajo.\n",
            "W spižce stoje crjeje a domacne pantochle.\n",
            "W styriłopjenkate\n",
            "Wócyń nicht namakaś?\n",
            "Zacyń wokno!\n",
            "To jo flaša\n",
            "W kupjeli mamy muzej.\n",
            "W styriłopjenkate kulkow su wiźeł?\n",
            "Na myjnicy lažy wótpokazane.\n",
            "Na myjnicy lažy kaž za ruce.\n",
            "Pódla styriłopjenkate wise seno a wiźeł?\n",
            "We styriłopjenkate jo śopła wóda.\n",
            "My se rad styriłopjenkate a wiźeł?\n",
            "Wótwónoźćo stoj teke w kupadle.\n",
            "To jo naša kuchnja.\n",
            "W kuchinskem spižce su žywe rěd a styriłopjenkate\n",
            "Na kuchinskem spižce stoje slědujuce.\n",
            "Mama jo w kuchni pśišła.\n",
            "Mama kraje, zamóžne.\n",
            "To jo bydleńska śpa.\n",
            "W prědnem śpě stoje prědne zofa a wjelike blido a zmóžniś\n",
            "Na spižce stoj naš wuglědajo.\n",
            "My rady skóńcował.\n",
            "Wjacor smy sebje žycył piś.\n",
            "W spižce su knigły, wideo a CD.\n",
            "Radijo jo sobu w mnjo.\n",
            "Mój bratš sejźi w njom a słucha muziku.\n",
            "Na sćěnje wise zeger a wobraze.\n",
            "Pód woknom jo móžna.\n",
            "My njamamy kamjeny, ale w rožku stoj kamjenjow.\n",
            "Na bliźe lažy Rub a stoj waza z kwětkami źěkowałej.\n",
            "How jo źěłaŕnja.\n",
            "Mama źěła w źěłaŕni za to.\n",
            "How stoj naš kompjuter.\n",
            "Maś pišo z kompjuterom a ja graju na kompjuteru graśa.\n",
            "Kompjuter stoj na pisańskem bliźe.\n",
            "W źěłaŕni jo teke knigłowy spižka.\n",
            "W spižce su wobrazowe knigłow, seno bydlece wěcne knigły, a wobrazowe knigły knigły.\n",
            "Tam jo móžna.\n",
            "bura spižka za suknje a bura su w Dolnej Łužycy.\n",
            "We póstoli lažy móžna.\n",
            "Na póstoli su zamóžne.\n",
            "To jo mója źiśeca\n",
            "W źiśecem śpě su źiśi grajkaju, domacne nadawki gótuju a spě.\n",
            "W regalu laže piś.\n",
            "Na špundowanju stoje awta a wiźeł?\n",
            "Na kusk stoj dowólona.\n",
            "Ja njamam Mokša ...\n",
            "W tej jśpě stoj wjelika neonowa\n",
            "Mała zasedlili stoj na góńtwu.\n",
            "Feluju hyšći kusk dowólona.\n",
            "Pód kšywom jo móžna.\n",
            "Na najśpje jo flaša\n",
            "W piwnicy jo grozne zarědowanjow.\n",
            "Teke raz stoje w piwnicy.\n",
            "W regalu stoje głažki z rěcu a marmeladu.\n",
            "Awto stajijomy do garaže.\n",
            "Naš dwór jo mały.\n",
            "Naša zagroda jo wjelika.\n",
            "W zagroźe mamy wiźeł?\n",
            "To jo naša źiśownja.\n",
            "Dobre zajtšo, kak se tebje groni?\n",
            "Mě se wě.\n",
            "Kak wy zamóžne.\n",
            "Ja som kněni som.\n",
            "Dobre zajtšo, jo.\n",
            "Dobre zajtšo, spiwali.\n",
            "Dobre zajtšo, kněni wuglědajo.\n",
            "Dobre zajtšo, śota jo.\n",
            "Witaj do domu.\n",
            "Hm... gaž sy zasej mnjo.\n",
            "Sy ty cora styriłopjenkate a rejowaś był?\n",
            "Halo, styriłopjenkate\n",
            "Glědaj, ja mam nowe pantochle.\n",
            "To jo Glědaj maš nowe scełego we Frankobroźe było.\n",
            "Chto jo tebje źinsa do źiśownje wuglědajo.\n",
            "Chto źinsa piś.\n",
            "Sy źinsa wiźeł?\n",
            "Źi do zwiska.\n",
            "Źiśo do zwiska.\n",
            "Wótwónoźćo styriłopjenkate How jo kóńc.\n",
            "Scyń šal, kepijom a rukajce do swójogo spórajo.\n",
            "Złožćo to do swójogo bura w wiźeł?\n",
            "Zeblac se anorak, płašć a zamóžne.\n",
            "Powjes anorak na pśikład?\n",
            "Powjes wina na pśikład?\n",
            "Wótwónoźćo sebje dopomnjeś.\n",
            "Wótwónoźćo sebje dopomnjeś.\n",
            "Rozuj sebje crjeje, raz a škórnje!\n",
            "Staj crjeje pórědnje do změnjona.\n",
            "Stajśo to!\n",
            "Wobuj sebje styriłopjenkate\n",
            "Wobujśo sebje styriłopjenkate\n",
            "Wzej sebje swóju tašu sobu!\n",
            "Wzejśo se swóju tašu sobu!\n",
            "Źi do swójeje mnjo.\n",
            "Źiśo do swójeje powěsći.\n",
            "Źi z styriłopjenkate a wiźeł?\n",
            "Wzej Hanku sobu do zwiska.\n",
            "Pśiź zasej slědk kupaŕnje z kupaŕnje a ze sprocniwosću\n",
            "Wócyń źurja!\n",
            "Wócyńśo źurja!\n",
            "Zacyń źurja njepomožo.\n",
            "Zacyńśo źurja śicho!\n",
            "Žeń pómałem!\n",
            "Pójź sobu!\n",
            "How jo naša kupcyna śpa.\n",
            "To jo naša kupcyna śpa.\n",
            "Naša kupcyna śpa jo doma.\n",
            "Na špundowanju lažy doma.\n",
            "W rožku stoje dom za pupki, śpa za pupki, zaměry.\n",
            "Mamy rědne winowatosći.\n",
            "Slězy stoj pupkowe źiwadło a prědnu wuglědajo.\n",
            "W regalu su burski dwór, jo grała Italska a wobrazowe gódańko była.\n",
            "Na spižce sejźe zběraś a plyšowe zwěrjeta.\n",
            "Prězy w rožku jo gójca wótwórił postom a normalny zamóžne.\n",
            "Wótkubłaŕka abo góle wójnske to.\n",
            "Pójź nutś!\n",
            "Pójźćo nutś!\n",
            "Ja som źinsa wiźeł?\n",
            "Ja stojm prězy!\n",
            "Ja stojm slězy!\n",
            "Buź wě.\n",
            "Sedni se na prědnem njespóznajom.\n",
            "Co nicht wuglědajo.\n",
            "Z cym coš grał?\n",
            "Z cym cośo grał?\n",
            "Wzej sebje to z domu a wiźeł?\n",
            "Wzejśo sebje to!\n",
            "Glědaj, až śi grajka dołoj njejo.\n",
            "Glědajśo jan, až wam grajki njerozmějom.\n",
            "Ty sy zazdaśim było.\n",
            "Ty sy zazdaśim było.\n",
            "Ty pórědnje wuglědajo.\n",
            "Ty pak rědnje wuglědajo.\n",
            "Ty sy zazdaśim było.\n",
            "Ty sy grajki pórědnje wuglědajo.\n",
            "Našo zaběry w kupkowej śpě.\n",
            "Jurij jo pśi graśe.\n",
            "Wótwónoźćo sebje z wiźeł?\n",
            "Wótwónoźćo papjeru wuglědajo.\n",
            "Zrumujśo kwětk.\n",
            "Smy pśi graśu.\n",
            "Ja cu se z pupku a seno grajkaś.\n",
            "Ja dajom styriłopjenkate\n",
            "Mója pupa jo nutśi było.\n",
            "Ja styriłopjenkate\n",
            "To połožyjom do złotych kólebkach.\n",
            "Něnt ja śi něco piś.\n",
            "Hm... mója flaša jo.\n",
            "Pupka jo chóra.\n",
            "Mója pupa njegóźi\n",
            "Cogodla płacośo?\n",
            "Bóli Wam wuglědajo.\n",
            "O, ty maš kašel a mam.\n",
            "To musym z tobu k gójcoju hyś.\n",
            "Wótwónoźćo z wuglědajo.\n",
            "Dobry źeń, mója Milenka jo chóra.\n",
            "Musym twóju zapódał pśepytowaś.\n",
            "Twója pupa jo móžna.\n",
            "Sotša buźo stojało měriś.\n",
            "Wótwónoźćo pśeklapany gójc śi pomožo.\n",
            "Ja nejpjerwjej raz póznaś.\n",
            "dowólona.\n",
            "Graśe tak!\n",
            "Wótwónoźćo za tśi dny zasej!\n",
            "Skóro buźo twója pupa zasej strowa.\n",
            "Wěrjepódobne sebje z awtom a zamóžne.\n",
            "Ja se z styriłopjenkate\n",
            "Beno jo mój pśijaśel.\n",
            "Móje awto dowólona.\n",
            "Beno twari wusoku tormom.\n",
            "Stajam nicht na mnjo.\n",
            "Awto njama žeden bencin wěcej.\n",
            "Ja jědu z awtom k tankowni.\n",
            "O, bencin jo móžna.\n",
            "Barbara swóje nakładne awto do garaže.\n",
            "Beno, styriłopjenkate\n",
            "Stajśo styriłopjenkate na wótpokazane.\n",
            "Móje wětš su styrirožkate a zamóžne.\n",
            "Wótwónoźćo sebje z burskem jo.\n",
            "Wótwónoźćo burski dwór.\n",
            "Za burski dwór trjebamy dom, wjelika źěła zamóžne.\n",
            "W groźi su krowy a sejźeli wójce a luźi?\n",
            "Krowy žywe swój śeletka a wójce a kozy a kozy doma.\n",
            "W groźi groźi su šli a seno zamóžne.\n",
            "W prědnem su kokošy a seno z seno z wiźeł?\n",
            "W prědnem su sejźeli a kónje.\n",
            "Pšosym, góle.\n",
            "W brožni laže seno a wiźeł?\n",
            "Pśinjas kašćik ze knigłow.\n",
            "Wótwónoźćo su woglědali do chyle!\n",
            "Kótare zwěrjetka južo póznaś.\n",
            "Wótwónoźćo tak njerozmějom.\n",
            "Wótwónoźćo se znate wiźeł?\n",
            "Kótare zwěrjeta hyšći znajoš?\n",
            "Ja mam zwěrjeta wjelgin rad.\n",
            "Mója starka ma kokošy a piś.\n",
            "Ja pla stareje mamy kokošy a piś.\n",
            "Wótwónoźćo nicht sewo a seno seno seno wobrośił.\n",
            "Chto jo płot wokoło slědujuce.\n",
            "Slězy brožnje jo pastwa.\n",
            "Co zwěrjeta na pastwje\n",
            "Na dwórje su kacki a wiźeł?\n",
            "Bur jo z bura pó dwórje.\n",
            "Burowka styriłopjenkate\n",
            "Źo jo se schowało kócka z młodymi zachopiła.\n",
            "How jo kóńc.\n",
            "Pjas lažy pśed dowólona.\n",
            "Jo burski dwór wiźeł?\n",
            "Źiśi, nicht wuglědajo.\n",
            "Howacej nic.\n",
            "pisała dwór wóstanjo wóstanu.\n",
            "Comy memory\n",
            "Grajomy raz wobchowajom.\n",
            "Chto grajo sobu?\n",
            "Źo su rownopšawne.\n",
            "Chto jo rozrědowała byś?\n",
            "Ja mam južo prědny wótwołany.\n",
            "Ach, ja njejsom piś.\n",
            "Hura, som pśewjeliki.\n",
            "Ja som tek.\n",
            "Złožćo do domu.\n",
            "Comy sebje graś piś.\n",
            "Beata je pśedawaŕka a Tereza nakupujo.\n",
            "Dobry źeń, mě wuglědajo.\n",
            "Co ga coš kupiś?\n",
            "Daj mě pšosym jadnu wiźeł?\n",
            "Trjebaš hyšći něco?\n",
            "Trjebam hyšći mloko, marmeladu a mam.\n",
            "Mój kórbik jo połny.\n",
            "Źo jo mója wěc.\n",
            "Wjele mam kwětk.\n",
            "Ty musyš źaseś eurow a pěś styriłopjenkate zapłaśiś.\n",
            "Měj se wě.\n",
            "Pójźćo ku mnjo!\n",
            "Ja som sejźeli a how jo mój salon.\n",
            "Co sebje žycyš?\n",
            "Pšosym styriłopjenkate mě a mam.\n",
            "Aw, to wuglědajo.\n",
            "Njebuź tak njerozmějom.\n",
            "Kak se śi wrośiś njebudu.\n",
            "Jo, ja som mam.\n",
            "Pšosym, něnt mě hyšći jo.\n",
            "Źo su móje znate wiźeł?\n",
            "Co musym śi piś.\n",
            "Wótwónoźćo mě pšosym gubje a wiźeł?\n",
            "Wótwónoźćo mě nicht piś.\n",
            "Som gótowa.\n",
            "Sy spokojom?\n",
            "Jo, som spokojom.\n",
            "Som wjelgin znjeměrnjony.\n",
            "Co mam źěru.\n",
            "To płaśi pěś euro.\n",
            "Źo jo doma.\n",
            "Ja śi nic wósebnego.\n",
            "Wótwónoźćo mě pšosym włosy!\n",
            "Glědaj do klina.\n",
            "Smy w krejzu.\n",
            "Te pśinjas mě kniglicki z wuglědajo.\n",
            "O, wy rědnje śicho wóstanjo.\n",
            "Wótwónoźćo co wam zamóžne.\n",
            "Kak jo se znjemóžniła\n",
            "pisała sebje wobraze w knigłach.\n",
            "Co na wobrazu jo.\n",
            "Co na wobrazu wiźiśo?\n",
            "Wótwónoźćo nutś!\n",
            "Scyń knigły do źuri slědk!\n",
            "Jaden jo pó drugem wulicujo.\n",
            "Chto znajo zamóžne.\n",
            "Pokažćo styriłopjenkate kwiśinowe seno a wiźeł?\n",
            "Powědaj sobu!\n",
            "Powědajśo sobu!\n",
            "Groń to sam był!\n",
            "To sy derje cynił.\n",
            "Źinsa znate nowy spiw.\n",
            "Mam spiw na CD.\n",
            "Znajośo mě?\n",
            "To jo wjasoły spiw.\n",
            "Źo jo mója wěc.\n",
            "Ja wam spiw pomjenili.\n",
            "Wótwónoźćo sobu!\n",
            "Wótwónoźćo papjeru wuglědajo.\n",
            "To jo mójo tebje!\n",
            "Něnto nicht wuglědajo.\n",
            "To južo derje gótowaś\n",
            "Spiwamy hyšći raz a bura sobu.\n",
            "Wótwónoźćo se do krejza a susednej se za mlince.\n",
            "Cynimy jadnu kšaceń napšawo a jadnu wiźeł?\n",
            "Wótwónoźćo sebje na kórtu.\n",
            "Pušććo wuglědajo.\n",
            "Ow, Feliks jo padnuł!\n",
            "Wótwónoźćo pomogaj jomu byś?\n",
            "Take něco ně něco?\n",
            "Ně, mě nic njebóli.\n",
            "Źiśo pó swój rownopšawne.\n",
            "Źo jo mój wótwołany.\n",
            "Pytaj swój styriłopjenkate\n",
            "Som swój bura namakał.\n",
            "Scyń swój styriłopjenkate\n",
            "Comy papjeru wuglědajo.\n",
            "Zacyńśo wócy!\n",
            "Słyšyśo muziku.\n",
            "Pśedstajśo se, kak prědnu na łuce jo.\n",
            "Wótwónoźćo wětš kwětki a tšawcycki wuglědajo.\n",
            "Majka wětš teke dopomnjeś.\n",
            "Kup ptaški jo.\n",
            "Smy pśi paslenju a institucijow.\n",
            "Trjebamy Naše kulkow bura blok, prědnu barwowy kašćik, k paslenju, k paslenju, a šurowańsku šorcu.\n",
            "Mam raz pśedmjat su sejźeli byli, jo měło sejźeli barwiki a casy dowólona.\n",
            "Něnto baslimy serbsku wuglědajo.\n",
            "Źiśo pó susednej cełej a połožćo ju na swójo městno.\n",
            "Woblacćo sebje wě.\n",
            "Baslimy serbsku rěc.\n",
            "Dajom wam łopjeno.\n",
            "Co na łopjenje wóstanjo.\n",
            "Ceło pšawje, wiźiš tśi wiźeł?\n",
            "Kótare barwu maju wiźeł?\n",
            "Serbska chórgoj jo cerwjenu a běła.\n",
            "Nejpjerwjej trjebamy módry a wiźeł?\n",
            "Wótwónoźćo měło seno zmóžniś\n",
            "Wótwónoźćo pśez klěb?\n",
            "Źarž pisak pšawje!\n",
            "Wzej pisak do pšaweje abo lěweje byś?\n",
            "Pokažćo cerwjeny kwětk.\n",
            "Ale Beno, coš ty mólowaś?\n",
            "Teke ty sy doma.\n",
            "Teke ty sy how.\n",
            "Južo ty zasej.\n",
            "Južo sy ty how.\n",
            "Pśi tom wóna źěła.\n",
            "Pśi tom wóna mysli.\n",
            "Wóna jo how.\n",
            "Wóna jo doma.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:14.128164Z",
          "iopub.execute_input": "2022-07-24T18:06:14.128710Z",
          "iopub.status.idle": "2022-07-24T18:06:15.443914Z",
          "shell.execute_reply.started": "2022-07-24T18:06:14.128640Z",
          "shell.execute_reply": "2022-07-24T18:06:15.442820Z"
        },
        "trusted": true,
        "id": "bxoLU0pqdwcz",
        "outputId": "148412d0-6e38-46e3-d28f-12aa0d8b4722",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdrive\u001b[0m/    \u001b[01;34mgdrive\u001b[0m/       target.txt\n",
            "\u001b[01;34mdata-bin\u001b[0m/     \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:08:11.485595Z",
          "iopub.execute_input": "2022-07-24T18:08:11.486787Z",
          "iopub.status.idle": "2022-07-24T18:08:12.475192Z",
          "shell.execute_reply.started": "2022-07-24T18:08:11.486730Z",
          "shell.execute_reply": "2022-07-24T18:08:12.474153Z"
        },
        "trusted": true,
        "id": "NnEBR0Xhdwc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb -l de-hsb -m bleu chrf ter)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:16.590743Z",
          "iopub.execute_input": "2022-07-24T18:06:16.591679Z",
          "iopub.status.idle": "2022-07-24T18:06:19.209461Z",
          "shell.execute_reply.started": "2022-07-24T18:06:16.591625Z",
          "shell.execute_reply": "2022-07-24T18:06:19.208405Z"
        },
        "trusted": true,
        "id": "bqOJKYxXdwc2",
        "outputId": "2b829e30-45ff-4de2-869a-ebe9948402a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 28.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\",\n",
            " \"verbose_score\": \"57.0/34.3/22.0/14.2 (BP = 1.000 ratio = 1.093 hyp_len = 5656 ref_len = 5177)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"chrF2\",\n",
            " \"score\": 48.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"yes\",\n",
            " \"nc\": \"6\",\n",
            " \"nw\": \"0\",\n",
            " \"space\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"TER\",\n",
            " \"score\": 52.8,\n",
            " \"signature\": \"nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"lc\",\n",
            " \"tok\": \"tercom\",\n",
            " \"norm\": \"no\",\n",
            " \"punct\": \"yes\",\n",
            " \"asian\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "}\n",
            "]\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading blind test data\n",
        "!gdown --id 1K4rJdui0CoKqRU1MxA5_iY3Qh7Cb4GSQ"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD0Zhyg5Mmtd",
        "outputId": "daec0f72-feef-4983-cd78-abcf775497b1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K4rJdui0CoKqRU1MxA5_iY3Qh7Cb4GSQ\n",
            "To: /content/test_dsb_hsb.hsb_src.txt\n",
            "100% 85.1k/85.1k [00:00<00:00, 73.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/test_dsb_hsb.hsb_src.txt --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang hsb --target-lang dsb \\\n",
        "      --beam 5 data-bin/wmt_hsb_dsb | grep -P \"D-[0-9]+\" | cut -f3 > /content/gdrive/MyDrive/target_hsb-dsb_sup_submission.txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr0yH7NZUF1B",
        "outputId": "65903348-1d24-432d-eb4c-beb744dcc71c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-28 22:33:48 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/test_dsb_hsb.hsb_src.txt'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_hsb_dsb', 'source_lang': 'hsb', 'target_lang': 'dsb', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-28 22:33:49 | INFO | fairseq.tasks.translation | [hsb] dictionary: 103512 types\n",
            "2022-08-28 22:33:49 | INFO | fairseq.tasks.translation | [dsb] dictionary: 104024 types\n",
            "2022-08-28 22:33:49 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-28 22:33:58 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-28 22:33:58 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-28 22:33:58 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-28 22:34:09 | INFO | fairseq_cli.interactive | Total time: 20.275 seconds; translation time: 8.617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "id": "9Zw2XTYgbzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.dsb -l hsb-dsb -m bleu chrf ter)"
      ],
      "metadata": {
        "id": "7LNPRlDQb4ER",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2527c2e6-27ca-4d98-bfb5-cd26f2e8ed1c"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 28.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\",\n",
            " \"verbose_score\": \"57.0/34.3/22.0/14.2 (BP = 1.000 ratio = 1.093 hyp_len = 5656 ref_len = 5177)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"chrF2\",\n",
            " \"score\": 48.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"yes\",\n",
            " \"nc\": \"6\",\n",
            " \"nw\": \"0\",\n",
            " \"space\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"TER\",\n",
            " \"score\": 52.8,\n",
            " \"signature\": \"nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"lc\",\n",
            " \"tok\": \"tercom\",\n",
            " \"norm\": \"no\",\n",
            " \"punct\": \"yes\",\n",
            " \"asian\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "}\n",
            "]\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}