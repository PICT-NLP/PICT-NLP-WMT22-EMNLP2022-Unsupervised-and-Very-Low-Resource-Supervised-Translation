{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "hsb_de_transformers_supervised_final_11am.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# create a seperate folder to store everything\n",
        "# !mkdir wmt\n",
        "# %cd wmt"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:45.876120Z",
          "iopub.execute_input": "2022-07-24T19:01:45.876582Z",
          "iopub.status.idle": "2022-07-24T19:01:46.194317Z",
          "shell.execute_reply.started": "2022-07-24T19:01:45.876499Z",
          "shell.execute_reply": "2022-07-24T19:01:46.192944Z"
        },
        "trusted": true,
        "id": "J9o_gWJIdwcc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mounting drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BFmSadnVoSa",
        "outputId": "ee880d6d-e37d-4249-f57f-3e009eee4113"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install tree\n",
        "\n",
        "# Install the necessary libraries\n",
        "!pip install sacremoses pandas mock sacrebleu tensorboardX pyarrow indic-nlp-library\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd /content/fairseq/\n",
        "!python -m pip install --editable .\n",
        "%cd /content\n",
        "\n",
        "! echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/fairseq/\"\n",
        "\n",
        "!echo $PYTHONPATH"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:01:47.842831Z",
          "iopub.execute_input": "2022-07-24T19:01:47.843475Z",
          "iopub.status.idle": "2022-07-24T19:03:13.957220Z",
          "shell.execute_reply.started": "2022-07-24T19:01:47.843443Z",
          "shell.execute_reply": "2022-07-24T19:03:13.955881Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sULkb42Jdwci",
        "outputId": "94390a45-e869-4a18-8da3-a00bfbdbdffa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-460\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 1s (77.7 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 155676 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Collecting mock\n",
            "  Downloading mock-4.0.3-py3-none-any.whl (28 kB)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.2.0-py3-none-any.whl (116 kB)\n",
            "\u001b[K     |████████████████████████████████| 116 kB 66.1 MB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 69.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (6.0.1)\n",
            "Collecting indic-nlp-library\n",
            "  Downloading indic_nlp_library-0.81-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacremoses) (2022.6.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sacremoses) (4.64.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (4.9.1)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.10)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Collecting morfessor\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Collecting sphinx-argparse\n",
            "  Downloading sphinx_argparse-0.3.1-py2.py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme\n",
            "  Downloading sphinx_rtd_theme-1.0.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 58.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx-argparse->indic-nlp-library) (1.8.6)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.11.3)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.6.1)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (57.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.2.4)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.2.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (21.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.7.12)\n",
            "Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10.3)\n",
            "Requirement already satisfied: docutils<0.18,>=0.11 in /usr/local/lib/python3.7/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (0.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.3->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.0.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.0.0->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (2022.6.15)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (3.0.9)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml in /usr/local/lib/python3.7/dist-packages (from sphinxcontrib-websupport->sphinx>=1.2.0->sphinx-argparse->indic-nlp-library) (1.1.5)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=019d95241919bd5606eac73cf8800d9c48517de2130208e8bb998ff2f85d985e\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sphinx-rtd-theme, sphinx-argparse, portalocker, morfessor, colorama, tensorboardX, sacremoses, sacrebleu, mock, indic-nlp-library\n",
            "Successfully installed colorama-0.4.5 indic-nlp-library-0.81 mock-4.0.3 morfessor-2.0.6 portalocker-2.5.1 sacrebleu-2.2.0 sacremoses-0.0.53 sphinx-argparse-0.3.1 sphinx-rtd-theme-1.0.0 tensorboardX-2.5.1\n",
            "Cloning into 'fairseq'...\n",
            "remote: Enumerating objects: 32239, done.\u001b[K\n",
            "remote: Total 32239 (delta 0), reused 0 (delta 0), pack-reused 32239\u001b[K\n",
            "Receiving objects: 100% (32239/32239), 22.42 MiB | 29.21 MiB/s, done.\n",
            "Resolving deltas: 100% (23642/23642), done.\n",
            "/content/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.1+cu113)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 4.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.32)\n",
            "Requirement already satisfied: sacrebleu>=1.4.12 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2.2.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.1+cu113)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 30.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.9.0)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 71.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (6.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (2.5.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.4.5)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (4.9.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.8.1)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=f18c11dd7dabd307886a8ea952a81d7dc9f6b404224cf4ce84c80cccddba341f\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: omegaconf, antlr4-python3-runtime, hydra-core, bitarray, fairseq\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed antlr4-python3-runtime-4.8 bitarray-2.6.0 fairseq hydra-core-1.0.7 omegaconf-2.0.6\n",
            "/content\n",
            "/env/python\n",
            "/env/python:/content/fairseq/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:13.960095Z",
          "iopub.execute_input": "2022-07-24T19:03:13.960531Z",
          "iopub.status.idle": "2022-07-24T19:03:14.252882Z",
          "shell.execute_reply.started": "2022-07-24T19:03:13.960500Z",
          "shell.execute_reply": "2022-07-24T19:03:14.251592Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XleJnPzidwck",
        "outputId": "19cf35eb-9eca-4cad-9c09-61ce0bf34312"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -U --no-cache-dir gdown --pre"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:14.254120Z",
          "iopub.execute_input": "2022-07-24T19:03:14.254498Z",
          "iopub.status.idle": "2022-07-24T19:03:36.192512Z",
          "shell.execute_reply.started": "2022-07-24T19:03:14.254462Z",
          "shell.execute_reply": "2022-07-24T19:03:36.191399Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIuDbtOgdwcm",
        "outputId": "996ed9f4-7597-4b6e-f997-ba10003bd274"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.1.tar.gz (14 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.64.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Building wheels for collected packages: gdown\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-4.5.1-py3-none-any.whl size=14951 sha256=17e4d4613a347f986759dcb886892bb3b1907c12b36ee5d5fe2bd9ba8e73ba66\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k3yvp5aj/wheels/3d/ec/b0/a96d1d126183f98570a785e6bf8789fca559853a9260e928e1\n",
            "Successfully built gdown\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNjo_Q8_IlXz",
        "outputId": "8d9de36b-81fb-4061-d4fd-93ae17e69f14"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir drive\n",
        "%cd drive"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.194917Z",
          "iopub.execute_input": "2022-07-24T19:03:36.195189Z",
          "iopub.status.idle": "2022-07-24T19:03:36.494103Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.195162Z",
          "shell.execute_reply": "2022-07-24T19:03:36.492910Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AopzC4DBdwcn",
        "outputId": "3a05140b-21a6-4afe-f428-e20202b7ef1d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 19UHtgPcAX-Qe5axVxmBQAutsssovCfs4 --folder"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:36.495274Z",
          "iopub.execute_input": "2022-07-24T19:03:36.495519Z",
          "iopub.status.idle": "2022-07-24T19:03:47.662284Z",
          "shell.execute_reply.started": "2022-07-24T19:03:36.495496Z",
          "shell.execute_reply": "2022-07-24T19:03:47.661258Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa1K1LT9dwcn",
        "outputId": "c70a724c-66cf-4ebe-d34a-ef5edca268d0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Retrieving folder list\n",
            "Retrieving folder 1QmE3jZSejMy_HqdHEqS5IfviAhXtmPZa 2020_train_hsb-de\n",
            "Processing file 15eZFhy5rbstwhWmrVCqYPCwiWmxPg6iK 2020_train_hsb-de.de\n",
            "Processing file 19CESCme_ZpyfT9WgUF8GcT5EhkAXhIq5 2020_train_hsb-de.hsb\n",
            "Retrieving folder 1aK3ovMgA4dBB_1RGJULQlHy_x9MJF0dO 2021_train_hsb-de\n",
            "Processing file 1czOC4vGy1ZyXOc_5uJY5twES2JyIIWV4 2021_train_hsb-de.de\n",
            "Processing file 19gU_eTd_SkpwDxX-pnvR_wUvaHAdd40o 2021_train_hsb-de.hsb\n",
            "Retrieving folder 1wEa03dYM4X0BB1wVYOlU3Il9pJHZEr8j 2022_train_hsb-de\n",
            "Processing file 1_jN89Xh0zX4a00_dzuSYxio9FAhSKNkH 2022_train_hsb-de.tsv\n",
            "Retrieving folder 1h1qZwHdc2tLo58H9TkXh9XPXKAi6vVns monolingual\n",
            "Processing file 1dRkADoRBI1o_nRlyZajjc_5CQBWwWSco de.tok\n",
            "Processing file 19LS6bKVZd9lPOeDtGQsMjVc7h6j9Cfxk HSB_monolingual.txt\n",
            "Processing file 1EziUIMT-DWI70SjRGqvH5pIQRUbSwg1G sorbian_institute_monolingual.hsb\n",
            "Processing file 12INmfPgh4h5gknEI1lFpywIof3W0BpPM web_monolingual.hsb\n",
            "Processing file 157UsiqIWKcbwqZcAPgz4Hv9DBY_WYJsP witaj_monolingual.hsb\n",
            "Retrieving folder 16ctwnqWbpnQWfU9xwcN24_7nyuGDKBug valid_hsb-de\n",
            "Processing file 1wvaCRisGwxtdia6UycAs6vmtTryqUnM_ 2020_devel_hsb-de.de\n",
            "Processing file 1HSvlBw2N52qJPhklsC1WEMXB66rYFgmX 2020_devel_hsb-de.hsb\n",
            "Processing file 1FKhss4pZ45fWIsXVnrvr9BiSPY78aQjh 2020_devel_test_hsb-de.de\n",
            "Processing file 1Z9OoFqsb45ZmBBFDYlVYyxIVfCm3pMsF 2020_devel_test_hsb-de.hsb\n",
            "Processing file 1-mL4dwB3-xQUU1gTs-IZ6gbMRTQqyFdo 2022_valid_hsb-de.tsv\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=15eZFhy5rbstwhWmrVCqYPCwiWmxPg6iK\n",
            "To: /content/drive/hsb-de/2020_train_hsb-de/2020_train_hsb-de.de\n",
            "100% 5.14M/5.14M [00:00<00:00, 153MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19CESCme_ZpyfT9WgUF8GcT5EhkAXhIq5\n",
            "To: /content/drive/hsb-de/2020_train_hsb-de/2020_train_hsb-de.hsb\n",
            "100% 4.71M/4.71M [00:00<00:00, 127MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1czOC4vGy1ZyXOc_5uJY5twES2JyIIWV4\n",
            "To: /content/drive/hsb-de/2021_train_hsb-de/2021_train_hsb-de.de\n",
            "100% 8.62M/8.62M [00:00<00:00, 110MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19gU_eTd_SkpwDxX-pnvR_wUvaHAdd40o\n",
            "To: /content/drive/hsb-de/2021_train_hsb-de/2021_train_hsb-de.hsb\n",
            "100% 7.93M/7.93M [00:00<00:00, 77.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_jN89Xh0zX4a00_dzuSYxio9FAhSKNkH\n",
            "To: /content/drive/hsb-de/2022_train_hsb-de/2022_train_hsb-de.tsv\n",
            "100% 53.2M/53.2M [00:00<00:00, 66.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dRkADoRBI1o_nRlyZajjc_5CQBWwWSco\n",
            "To: /content/drive/hsb-de/monolingual/de.tok\n",
            "100% 7.37M/7.37M [00:00<00:00, 79.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=19LS6bKVZd9lPOeDtGQsMjVc7h6j9Cfxk\n",
            "To: /content/drive/hsb-de/monolingual/HSB_monolingual.txt\n",
            "100% 41.9M/41.9M [00:00<00:00, 195MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1EziUIMT-DWI70SjRGqvH5pIQRUbSwg1G\n",
            "To: /content/drive/hsb-de/monolingual/sorbian_institute_monolingual.hsb\n",
            "100% 37.3M/37.3M [00:00<00:00, 98.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=12INmfPgh4h5gknEI1lFpywIof3W0BpPM\n",
            "To: /content/drive/hsb-de/monolingual/web_monolingual.hsb\n",
            "100% 11.7M/11.7M [00:00<00:00, 50.6MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=157UsiqIWKcbwqZcAPgz4Hv9DBY_WYJsP\n",
            "To: /content/drive/hsb-de/monolingual/witaj_monolingual.hsb\n",
            "100% 19.4M/19.4M [00:00<00:00, 86.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wvaCRisGwxtdia6UycAs6vmtTryqUnM_\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_hsb-de.de\n",
            "100% 174k/174k [00:00<00:00, 105MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1HSvlBw2N52qJPhklsC1WEMXB66rYFgmX\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_hsb-de.hsb\n",
            "100% 160k/160k [00:00<00:00, 110MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FKhss4pZ45fWIsXVnrvr9BiSPY78aQjh\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_test_hsb-de.de\n",
            "100% 178k/178k [00:00<00:00, 102MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Z9OoFqsb45ZmBBFDYlVYyxIVfCm3pMsF\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2020_devel_test_hsb-de.hsb\n",
            "100% 164k/164k [00:00<00:00, 121MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-mL4dwB3-xQUU1gTs-IZ6gbMRTQqyFdo\n",
            "To: /content/drive/hsb-de/valid_hsb-de/2022_valid_hsb-de.tsv\n",
            "100% 344k/344k [00:00<00:00, 99.9MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUdiQyJdJBdt",
        "outputId": "7251f42c-ccf4-45f4-86b8-a7cb96d4ca59"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:03:47.663751Z",
          "iopub.execute_input": "2022-07-24T19:03:47.664065Z",
          "iopub.status.idle": "2022-07-24T19:03:47.671365Z",
          "shell.execute_reply.started": "2022-07-24T19:03:47.664036Z",
          "shell.execute_reply": "2022-07-24T19:03:47.670044Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXt8nnHHdwco",
        "outputId": "0c6af62d-ee11-4d60-ec70-70a4af554d4a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd drive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H751-LW3XII0",
        "outputId": "0b63f834-7d3d-45c4-ec1b-7ed8a69572cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/hsb-de/2022_train_hsb-de/2022_train_hsb-de.tsv', delimiter='\\t')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "XtaWvwKcW6ja",
        "outputId": "cd474c70-3433-4666-87ef-f6577d392b77"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 hsb  \\\n",
              "0  Nano, ja će prošu, jónu přidać, zo móžeš tež t...   \n",
              "1  Kotre kompozitoriske srědki zasadźi Schumann k...   \n",
              "2  Komunistiskim mocam wizita kardinala Wojtyły z...   \n",
              "3  Tohodla chcemy za tute wodźizny krućiše naroki...   \n",
              "4  Chcu so cyle skrótka na podawk dźensa popołdnj...   \n",
              "\n",
              "                                                  de  \n",
              "0  Papa, ich bitte dich, einmal einzuräumen, dass...  \n",
              "1  Welche kompositorischen Mittel setzt Schumann ...  \n",
              "2  Den kommunistischen Mächten war die Visite des...  \n",
              "3  Deshalb wollen wir für diese Gewässer strenger...  \n",
              "4  Ich möchte mich ganz kurz auf einen Vorgang he...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3f8887c4-b240-4b93-b00f-b87240801e21\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hsb</th>\n",
              "      <th>de</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Nano, ja će prošu, jónu přidać, zo móžeš tež t...</td>\n",
              "      <td>Papa, ich bitte dich, einmal einzuräumen, dass...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Kotre kompozitoriske srědki zasadźi Schumann k...</td>\n",
              "      <td>Welche kompositorischen Mittel setzt Schumann ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Komunistiskim mocam wizita kardinala Wojtyły z...</td>\n",
              "      <td>Den kommunistischen Mächten war die Visite des...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tohodla chcemy za tute wodźizny krućiše naroki...</td>\n",
              "      <td>Deshalb wollen wir für diese Gewässer strenger...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Chcu so cyle skrótka na podawk dźensa popołdnj...</td>\n",
              "      <td>Ich möchte mich ganz kurz auf einen Vorgang he...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3f8887c4-b240-4b93-b00f-b87240801e21')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3f8887c4-b240-4b93-b00f-b87240801e21 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3f8887c4-b240-4b93-b00f-b87240801e21');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('train_hsb-de.hsb', 'w') as f:\n",
        "  for i in range(70000):\n",
        "    f.write(df['hsb'][i]+\"\\n\")\n",
        "with open('train_hsb-de.de', 'w') as f:\n",
        "  for i in range(70000):\n",
        "    f.write(df['de'][i]+\"\\n\")"
      ],
      "metadata": {
        "id": "Y7WeR9pIW_ZJ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_valid = pd.read_csv('/content/drive/hsb-de/valid_hsb-de/2022_valid_hsb-de.tsv', delimiter='\\t')\n",
        "df_valid.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "LgMXiehCW_58",
        "outputId": "66897936-82c4-46b9-b24a-08682834b5e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 hsb  \\\n",
              "0  A z kozu abo kruwu hodźeše so mloko a twaroh p...   \n",
              "1  Jednu družinu zorna picujemy skotej, z druhich...   \n",
              "2  W nim zaběra bamž mjez druhim k mnoho diskutow...   \n",
              "3  Štó, hdyž nic křesćanske institucije bychu tut...   \n",
              "4  Sym kruće přeswědčeny, zo wobsedźi Zjednoćene ...   \n",
              "\n",
              "                                                  de  \n",
              "0  Und mit Ziege oder Kuh ließen sich auch Milch ...  \n",
              "1  Die einen Körner verfüttern wir dem Viehzeug, ...  \n",
              "2  In ihm nimmt der Papst unter anderem zu einem ...  \n",
              "3  Wer, wenn nicht christliche Institutionen würd...  \n",
              "4  Ich bin der festen Überzeugung, dass das Verei...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d56241e0-fbda-4176-8f0f-9511ab30b06e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hsb</th>\n",
              "      <th>de</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A z kozu abo kruwu hodźeše so mloko a twaroh p...</td>\n",
              "      <td>Und mit Ziege oder Kuh ließen sich auch Milch ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Jednu družinu zorna picujemy skotej, z druhich...</td>\n",
              "      <td>Die einen Körner verfüttern wir dem Viehzeug, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>W nim zaběra bamž mjez druhim k mnoho diskutow...</td>\n",
              "      <td>In ihm nimmt der Papst unter anderem zu einem ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Štó, hdyž nic křesćanske institucije bychu tut...</td>\n",
              "      <td>Wer, wenn nicht christliche Institutionen würd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Sym kruće přeswědčeny, zo wobsedźi Zjednoćene ...</td>\n",
              "      <td>Ich bin der festen Überzeugung, dass das Verei...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d56241e0-fbda-4176-8f0f-9511ab30b06e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d56241e0-fbda-4176-8f0f-9511ab30b06e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d56241e0-fbda-4176-8f0f-9511ab30b06e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('valid_hsb-de.hsb', 'w') as f:\n",
        "  for i in range(len(df_valid)):\n",
        "    f.write(df_valid['hsb'][i]+\"\\n\")\n",
        "with open('valid_hsb-de.de', 'w') as f:\n",
        "  for i in range(len(df_valid)):\n",
        "    f.write(df_valid['de'][i]+\"\\n\")"
      ],
      "metadata": {
        "id": "t0FxklB_XEUj"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:04:11.972219Z",
          "iopub.execute_input": "2022-07-24T19:04:11.972913Z",
          "iopub.status.idle": "2022-07-24T19:04:12.256665Z",
          "shell.execute_reply.started": "2022-07-24T19:04:11.972888Z",
          "shell.execute_reply": "2022-07-24T19:04:12.255254Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ihwjB6Gdwcp",
        "outputId": "dcfce34a-30c9-4d13-81ff-294dde4774a5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mhsb-de\u001b[0m/  train_hsb-de.de  train_hsb-de.hsb  valid_hsb-de.de  valid_hsb-de.hsb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd .."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8F3ZpFtX6Vo",
        "outputId": "49bf8466-914b-47b6-f31e-fe93b0a2c6cb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-preprocess \\\n",
        "    --source-lang hsb --target-lang de \\\n",
        "    --trainpref drive/train_hsb-de --validpref drive/valid_hsb-de \\\n",
        "    --destdir data-bin/wmt_hsb_de --thresholdtgt 0 --thresholdsrc 0 \\\n",
        "    --workers 20)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:14:48.909980Z",
          "iopub.execute_input": "2022-07-24T19:14:48.910354Z",
          "iopub.status.idle": "2022-07-24T19:15:18.024931Z",
          "shell.execute_reply.started": "2022-07-24T19:14:48.910330Z",
          "shell.execute_reply": "2022-07-24T19:15:18.024044Z"
        },
        "trusted": true,
        "id": "XxPJ-2-zdwcq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6a8adc9-50a4-426c-f56b-df448a4b1425"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 06:32:39 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='data-bin/wmt_hsb_de', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='hsb', srcdict=None, suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir=None, testpref=None, tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='drive/train_hsb-de', use_plasma_view=False, user_dir=None, validpref='drive/valid_hsb-de', wandb_project=None, workers=20)\n",
            "2022-08-29 06:32:49 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 136232 types\n",
            "2022-08-29 06:33:00 | INFO | fairseq_cli.preprocess | [hsb] drive/train_hsb-de.hsb: 70000 sents, 883083 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-29 06:33:00 | INFO | fairseq_cli.preprocess | [hsb] Dictionary: 136232 types\n",
            "2022-08-29 06:33:02 | INFO | fairseq_cli.preprocess | [hsb] drive/valid_hsb-de.hsb: 2000 sents, 24731 tokens, 8.65% replaced (by <unk>)\n",
            "2022-08-29 06:33:02 | INFO | fairseq_cli.preprocess | [de] Dictionary: 105688 types\n",
            "2022-08-29 06:33:10 | INFO | fairseq_cli.preprocess | [de] drive/train_hsb-de.de: 70000 sents, 996299 tokens, 0.0% replaced (by <unk>)\n",
            "2022-08-29 06:33:10 | INFO | fairseq_cli.preprocess | [de] Dictionary: 105688 types\n",
            "2022-08-29 06:33:13 | INFO | fairseq_cli.preprocess | [de] drive/valid_hsb-de.de: 2000 sents, 27607 tokens, 6.06% replaced (by <unk>)\n",
            "2022-08-29 06:33:13 | INFO | fairseq_cli.preprocess | Wrote preprocessed data to data-bin/wmt_hsb_de\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uol2ncymJWFr",
        "outputId": "62ba9cd8-68ae-4e5f-fc82-dd7f825ee449"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 4.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
            "Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 69.4 MB/s \n",
            "\u001b[?25hCollecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 66.6 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 70.8 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 76.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 72.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 74.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=530d027006a2bf5ff80eff2f341c688db7c41b183e1d9b70648ce37927c4e4a1\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.init(project=\"wmt2022_hsb-de_transformers_supervised_final_11am\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:18.026892Z",
          "iopub.execute_input": "2022-07-24T19:15:18.027225Z",
          "iopub.status.idle": "2022-07-24T19:15:26.733355Z",
          "shell.execute_reply.started": "2022-07-24T19:15:18.027200Z",
          "shell.execute_reply": "2022-07-24T19:15:26.732531Z"
        },
        "trusted": true,
        "id": "hoUXe8Kndwcr",
        "outputId": "0ecb949d-f4e1-46a3-b5e8-2f75df3e86c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220829_063351-1e7tqz27</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/rahul2023_usa/wmt2022_hsb-de_transformers_supervised_final_11am/runs/1e7tqz27\" target=\"_blank\">treasured-sunset-1</a></strong> to <a href=\"https://wandb.ai/rahul2023_usa/wmt2022_hsb-de_transformers_supervised_final_11am\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/rahul2023_usa/wmt2022_hsb-de_transformers_supervised_final_11am/runs/1e7tqz27?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f57247fb410>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Finetuning the model\n",
        "\n",
        "# # pls refer to fairseq documentaion to know more about each of these options (https://fairseq.readthedocs.io/en/latest/command_line_tools.html)\n",
        "\n",
        "\n",
        "# # some notable args:\n",
        "# # --max-update=1000     -> for this example, to demonstrate how to finetune we are only training for 1000 steps. You should increase this when finetuning\n",
        "# # --arch=transformer_4x -> we use a custom transformer model and name it transformer_4x (4 times the parameter size of transformer  base)\n",
        "# # --user_dir            -> we define the custom transformer arch in model_configs folder and pass it as an argument to user_dir for fairseq to register this architechture\n",
        "# # --lr                  -> learning rate. From our limited experiments, we find that lower learning rates like 3e-5 works best for finetuning.\n",
        "# # --restore-file        -> reload the pretrained checkpoint and start training from here (change this path for indic-en. Currently its is set to en-indic)\n",
        "# # --reset-*             -> reset and not use lr scheduler, dataloader, optimizer etc of the older checkpoint\n",
        "# # --max_tokns           -> this is max tokens per batch\n",
        "\n",
        "\n",
        "# !( fairseq-train data-bin/wmt_dsb_de \\\n",
        "# --max-source-positions=210 \\\n",
        "# --max-target-positions=210 \\\n",
        "# --max-update=1000 \\\n",
        "# --save-interval=1 \\\n",
        "# --arch=transformer \\\n",
        "# --criterion=label_smoothed_cross_entropy \\\n",
        "# --source-lang=dsb \\\n",
        "# --lr-scheduler=inverse_sqrt \\\n",
        "# --target-lang=de \\\n",
        "# --label-smoothing=0.1 \\\n",
        "# --optimizer adam \\\n",
        "# --adam-betas \"(0.9, 0.98)\" \\\n",
        "# --clip-norm 1.0 \\\n",
        "# --warmup-init-lr 1e-07 \\\n",
        "# --warmup-updates 4000 \\\n",
        "# --dropout 0.2 \\\n",
        "# --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "# --save-dir checkpoints/model \\\n",
        "# --keep-last-epochs 5 \\\n",
        "# --patience 5 \\\n",
        "# --skip-invalid-size-inputs-valid-test \\\n",
        "# --fp16 \\\n",
        "# --update-freq=2 \\\n",
        "# --distributed-world-size 1 \\\n",
        "# --max-tokens 1024 \\\n",
        "# --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "# --lr 5e-4 \\\n",
        "# --reset-lr-scheduler \\\n",
        "# --reset-meters \\\n",
        "# --reset-dataloader \\\n",
        "# --reset-optimizer \\\n",
        "# --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "trusted": true,
        "id": "w4cGb93Bdwct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!( fairseq-train data-bin/wmt_hsb_de \\\n",
        "    --arch transformer \\\n",
        "    --max-epoch=50 \\\n",
        "    --criterion=label_smoothed_cross_entropy \\\n",
        "    --source-lang=hsb \\\n",
        "    --lr-scheduler=inverse_sqrt \\\n",
        "    --target-lang=de \\\n",
        "    --label-smoothing=0.1 \\\n",
        "    --optimizer adam \\\n",
        "    --adam-betas \"(0.9, 0.98)\" \\\n",
        "    --clip-norm 0.0 \\\n",
        "    --dropout 0.2 \\\n",
        "    --tensorboard-logdir ../../../tmp/tensorboard-wandb \\\n",
        "    --wandb-project 'wmt2022_hsb-de_transformers_supervised_final_11am' \\\n",
        "    --save-dir checkpoints/model \\\n",
        "    --keep-last-epochs 5 \\\n",
        "    --fp16 \\\n",
        "    --update-freq=2 \\\n",
        "    --max-tokens 4096 \\\n",
        "    --lr 5e-4 \\\n",
        "    --eval-bleu --eval-bleu-args \"{\\\"beam\\\": 5, \\\"max_len_a\\\": 1.2, \\\"max_len_b\\\": 10}\" --eval-bleu-detok moses --eval-bleu-remove-bpe \\\n",
        "    --reset-lr-scheduler \\\n",
        "    --reset-meters \\\n",
        "    --reset-dataloader \\\n",
        "    --reset-optimizer \\\n",
        "    --ignore-unused-valid-subsets)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:15:42.672724Z",
          "iopub.execute_input": "2022-07-24T19:15:42.673143Z",
          "iopub.status.idle": "2022-07-24T19:16:00.902554Z",
          "shell.execute_reply.started": "2022-07-24T19:15:42.673112Z",
          "shell.execute_reply": "2022-07-24T19:16:00.901385Z"
        },
        "trusted": true,
        "id": "U2IouMGudwcv",
        "outputId": "d261071c-fa01-40c3-c069-723adac748a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 06:33:57 | INFO | fairseq_cli.train | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': '../../../tmp/tensorboard-wandb', 'wandb_project': 'wmt2022_hsb-de_transformers_supervised_final_11am', 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': True, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': None, 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': True, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': True, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 50, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [2], 'lr': [0.0005], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints/model', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': True, 'reset_lr_scheduler': True, 'reset_meters': True, 'reset_optimizer': True, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': 5, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 0, 'input': '-'}, 'model': Namespace(_name='transformer', activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, aim_repo=None, aim_run_hash=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, arch='transformer', attention_dropout=0.0, azureml_logging=False, batch_size=None, batch_size_valid=None, best_checkpoint_metric='loss', bf16=False, bpe=None, broadcast_buffers=False, bucket_cap_mb=25, checkpoint_activations=False, checkpoint_shard_count=1, checkpoint_suffix='', clip_norm=0.0, combine_valid_subsets=None, continue_once=None, cpu=False, cpu_offload=False, criterion='label_smoothed_cross_entropy', cross_self_attention=False, curriculum=0, data='data-bin/wmt_hsb_de', data_buffer_size=10, dataset_impl=None, ddp_backend='pytorch_ddp', ddp_comm_hook='none', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_input_dim=512, decoder_layerdrop=0, decoder_layers=6, decoder_layers_to_keep=None, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_num_procs=1, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.2, ema_decay=0.9999, ema_fp32=False, ema_seed_model=None, ema_start_update=0, ema_update_freq=1, empty_cache_freq=0, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layerdrop=0, encoder_layers=6, encoder_layers_to_keep=None, encoder_learned_pos=False, encoder_normalize_before=False, eos=2, eval_bleu=True, eval_bleu_args='{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', eval_bleu_detok='moses', eval_bleu_detok_args='{}', eval_bleu_print_samples=False, eval_bleu_remove_bpe='@@ ', eval_tokenized_bleu=False, fast_stat_sync=False, find_unused_parameters=False, finetune_from_model=None, fix_batches_to_gpus=False, fixed_validation_seed=None, fp16=True, fp16_adam_stats=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, fp32_reduce_scatter=False, gen_subset='test', gradient_as_bucket_view=False, grouped_shuffling=False, heartbeat_timeout=-1, ignore_prefix_size=0, ignore_unused_valid_subsets=True, keep_best_checkpoints=-1, keep_interval_updates=-1, keep_interval_updates_pattern=-1, keep_last_epochs=5, label_smoothing=0.1, layernorm_embedding=False, left_pad_source=True, left_pad_target=False, load_alignments=False, load_checkpoint_on_all_dp_ranks=False, localsgd_frequency=3, log_file=None, log_format=None, log_interval=100, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=50, max_tokens=4096, max_tokens_valid=4096, max_update=0, max_valid_steps=None, maximize_best_checkpoint_metric=False, memory_efficient_bf16=False, memory_efficient_fp16=False, merge_src_tgt_embed=False, min_loss_scale=0.0001, model_parallel_size=1, no_cross_attention=False, no_epoch_checkpoints=False, no_last_checkpoints=False, no_progress_bar=False, no_reshard_after_forward=False, no_save=False, no_save_optimizer_state=False, no_scale_embedding=False, no_seed_provided=False, no_token_positional_embeddings=False, not_fsdp_flatten_parameters=False, nprocs_per_node=1, num_batch_buckets=0, num_shards=1, num_workers=1, offload_activations=False, on_cpu_convert_precision=False, optimizer='adam', optimizer_overrides='{}', pad=1, patience=-1, pipeline_balance=None, pipeline_checkpoint='never', pipeline_chunks=0, pipeline_decoder_balance=None, pipeline_decoder_devices=None, pipeline_devices=None, pipeline_encoder_balance=None, pipeline_encoder_devices=None, pipeline_model_parallel=False, plasma_path='/tmp/plasma', profile=False, quant_noise_pq=0, quant_noise_pq_block_size=8, quant_noise_scalar=0, quantization_config_path=None, report_accuracy=False, required_batch_size_multiple=8, required_seq_len_multiple=1, reset_dataloader=True, reset_logging=False, reset_lr_scheduler=True, reset_meters=True, reset_optimizer=True, restore_file='checkpoint_last.pt', save_dir='checkpoints/model', save_interval=1, save_interval_updates=0, scoring='bleu', seed=1, sentence_avg=False, shard_id=0, share_all_embeddings=False, share_decoder_input_output_embed=False, simul_type=None, skip_invalid_size_inputs_valid_test=False, skip_remainder_batch=False, slowmo_base_algorithm='localsgd', slowmo_momentum=None, source_lang='hsb', stop_min_lr=-1.0, stop_time_hours=0, store_ema=False, suppress_crashes=False, target_lang='de', task='translation', tensorboard_logdir='../../../tmp/tensorboard-wandb', threshold_loss_scale=None, tie_adaptive_weights=False, tokenizer=None, tpu=False, train_subset='train', truncate_source=False, unk=3, update_epoch_batch_itr=False, update_freq=[2], update_ordered_indices_seed=False, upsample_primary=-1, use_bmuf=False, use_old_adam=False, use_plasma_view=False, use_sharded_state=False, user_dir=None, valid_subset='valid', validate_after_updates=0, validate_interval=1, validate_interval_updates=0, wandb_project='wmt2022_hsb-de_transformers_supervised_final_11am', warmup_init_lr=-1, warmup_updates=4000, weight_decay=0.0, write_checkpoints_asynchronously=False, zero_sharding='none'), 'task': {'_name': 'translation', 'data': 'data-bin/wmt_hsb_de', 'source_lang': 'hsb', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': True, 'eval_bleu_args': '{\"beam\": 5, \"max_len_a\": 1.2, \"max_len_b\": 10}', 'eval_bleu_detok': 'moses', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': '@@ ', 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'label_smoothed_cross_entropy', 'label_smoothing': 0.1, 'report_accuracy': False, 'ignore_prefix_size': 0, 'sentence_avg': False}, 'optimizer': {'_name': 'adam', 'adam_betas': '(0.9, 0.98)', 'adam_eps': 1e-08, 'weight_decay': 0.0, 'use_old_adam': False, 'fp16_adam_stats': False, 'tpu': False, 'lr': [0.0005]}, 'lr_scheduler': {'_name': 'inverse_sqrt', 'warmup_updates': 4000, 'warmup_init_lr': -1.0, 'lr': [0.0005]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-29 06:33:57 | INFO | fairseq.tasks.translation | [hsb] dictionary: 136232 types\n",
            "2022-08-29 06:33:57 | INFO | fairseq.tasks.translation | [de] dictionary: 105688 types\n",
            "2022-08-29 06:34:03 | INFO | fairseq_cli.train | TransformerModel(\n",
            "  (encoder): TransformerEncoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(136232, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerEncoderLayerBase(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (decoder): TransformerDecoderBase(\n",
            "    (dropout_module): FairseqDropout()\n",
            "    (embed_tokens): Embedding(105688, 512, padding_idx=1)\n",
            "    (embed_positions): SinusoidalPositionalEmbedding()\n",
            "    (layers): ModuleList(\n",
            "      (0): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (1): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (2): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (3): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (4): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "      (5): TransformerDecoderLayerBase(\n",
            "        (dropout_module): FairseqDropout()\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (activation_dropout_module): FairseqDropout()\n",
            "        (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (encoder_attn): MultiheadAttention(\n",
            "          (dropout_module): FairseqDropout()\n",
            "          (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "    (output_projection): Linear(in_features=512, out_features=105688, bias=False)\n",
            "  )\n",
            ")\n",
            "2022-08-29 06:34:03 | INFO | fairseq_cli.train | task: TranslationTask\n",
            "2022-08-29 06:34:03 | INFO | fairseq_cli.train | model: TransformerModel\n",
            "2022-08-29 06:34:03 | INFO | fairseq_cli.train | criterion: LabelSmoothedCrossEntropyCriterion\n",
            "2022-08-29 06:34:03 | INFO | fairseq_cli.train | num. shared model params: 222,113,792 (num. trained: 222,113,792)\n",
            "2022-08-29 06:34:03 | INFO | fairseq_cli.train | num. expert model params: 0 (num. trained: 0)\n",
            "2022-08-29 06:34:03 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data-bin/wmt_hsb_de/valid.hsb-de.hsb\n",
            "2022-08-29 06:34:03 | INFO | fairseq.data.data_utils | loaded 2,000 examples from: data-bin/wmt_hsb_de/valid.hsb-de.de\n",
            "2022-08-29 06:34:03 | INFO | fairseq.tasks.translation | data-bin/wmt_hsb_de valid hsb-de 2000 examples\n",
            "2022-08-29 06:34:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-29 06:34:09 | INFO | fairseq.utils | rank   0: capabilities =  7.5  ; total memory = 14.756 GB ; name = Tesla T4                                \n",
            "2022-08-29 06:34:09 | INFO | fairseq.utils | ***********************CUDA enviroments for all 1 workers***********************\n",
            "2022-08-29 06:34:09 | INFO | fairseq_cli.train | training on 1 devices (GPUs/TPUs)\n",
            "2022-08-29 06:34:09 | INFO | fairseq_cli.train | max tokens per device = 4096 and max sentences per device = None\n",
            "2022-08-29 06:34:09 | INFO | fairseq.trainer | Preparing to load checkpoint checkpoints/model/checkpoint_last.pt\n",
            "2022-08-29 06:34:09 | INFO | fairseq.trainer | No existing checkpoint found checkpoints/model/checkpoint_last.pt\n",
            "2022-08-29 06:34:09 | INFO | fairseq.trainer | loading train data for epoch 1\n",
            "2022-08-29 06:34:09 | INFO | fairseq.data.data_utils | loaded 70,000 examples from: data-bin/wmt_hsb_de/train.hsb-de.hsb\n",
            "2022-08-29 06:34:09 | INFO | fairseq.data.data_utils | loaded 70,000 examples from: data-bin/wmt_hsb_de/train.hsb-de.de\n",
            "2022-08-29 06:34:09 | INFO | fairseq.tasks.translation | data-bin/wmt_hsb_de train hsb-de 70000 examples\n",
            "2022-08-29 06:34:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 001:   0% 0/131 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrahul2023_usa\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220829_063409-2of6zsfd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_hsb-de_transformers_supervised_final_11am\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rahul2023_usa/wmt2022_hsb-de_transformers_supervised_final_11am/runs/2of6zsfd\u001b[0m\n",
            "2022-08-29 06:34:10 | INFO | fairseq.trainer | begin training epoch 1\n",
            "2022-08-29 06:34:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "2022-08-29 06:34:13 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 64.0\n",
            "epoch 001:   2% 2/131 [00:04<04:11,  1.95s/it]2022-08-29 06:34:14 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 32.0\n",
            "epoch 001:  15% 19/131 [00:13<00:57,  1.95it/s]2022-08-29 06:34:23 | INFO | fairseq.trainer | NOTE: gradient overflow detected, ignoring gradient, setting loss scale to: 16.0\n",
            "epoch 001:  78% 102/131 [00:55<00:15,  1.89it/s]2022-08-29 06:35:08 | INFO | numexpr.utils | NumExpr defaulting to 2 threads.\n",
            "epoch 001:  99% 130/131 [01:15<00:00,  1.93it/s, loss=15.907, nll_loss=15.737, ppl=54604.7, wps=14527.4, ups=1.9, wpb=7620.8, bsz=528.5, num_updates=100, lr=1.25e-05, gnorm=3.241, loss_scale=16, train_wall=56, gb_free=5.3, wall=57]2022-08-29 06:35:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 001 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  11% 1/9 [00:00<00:06,  1.28it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  22% 2/9 [00:01<00:04,  1.59it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  33% 3/9 [00:01<00:03,  1.64it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  44% 4/9 [00:02<00:02,  1.75it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  56% 5/9 [00:02<00:02,  1.81it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  67% 6/9 [00:03<00:01,  1.86it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  78% 7/9 [00:03<00:01,  1.87it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset:  89% 8/9 [00:04<00:00,  1.85it/s]\u001b[A\n",
            "epoch 001 | valid on 'valid' subset: 100% 9/9 [00:04<00:00,  2.12it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:35:30 | INFO | valid | epoch 001 | valid on 'valid' subset | loss 14.508 | nll_loss 14.182 | ppl 18581.2 | bleu 0 | wps 6026.7 | wpb 3067.4 | bsz 222.2 | num_updates 128\n",
            "2022-08-29 06:35:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 1 @ 128 updates\n",
            "2022-08-29 06:35:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-29 06:35:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint1.pt\n",
            "2022-08-29 06:36:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint1.pt (epoch 1 @ 128 updates, score 14.508) (writing took 52.78423746999988 seconds)\n",
            "2022-08-29 06:36:23 | INFO | fairseq_cli.train | end of epoch 1 (average epoch stats below)\n",
            "2022-08-29 06:36:23 | INFO | train | epoch 001 | loss 15.662 | nll_loss 15.465 | ppl 45235 | wps 7459.1 | ups 0.98 | wpb 7596.3 | bsz 531.6 | num_updates 128 | lr 1.6e-05 | gnorm 2.871 | loss_scale 16 | train_wall 70 | gb_free 5.4 | wall 134\n",
            "2022-08-29 06:36:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 002:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:36:23 | INFO | fairseq.trainer | begin training epoch 2\n",
            "2022-08-29 06:36:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 002:  99% 130/131 [01:10<00:00,  1.84it/s, loss=14.313, nll_loss=13.97, ppl=16045.9, wps=6592.4, ups=0.87, wpb=7540, bsz=529.3, num_updates=200, lr=2.5e-05, gnorm=1.931, loss_scale=16, train_wall=53, gb_free=5.1, wall=173]2022-08-29 06:37:34 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 002 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.90s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.69s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.77s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  44% 4/9 [00:07<00:08,  1.76s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.92s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  67% 6/9 [00:11<00:05,  1.90s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  78% 7/9 [00:13<00:03,  1.93s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset:  89% 8/9 [00:15<00:02,  2.01s/it]\u001b[A\n",
            "epoch 002 | valid on 'valid' subset: 100% 9/9 [00:16<00:00,  1.80s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:37:51 | INFO | valid | epoch 002 | valid on 'valid' subset | loss 12.949 | nll_loss 12.449 | ppl 5590.08 | bleu 0.05 | wps 1658.6 | wpb 3067.4 | bsz 222.2 | num_updates 259 | best_loss 12.949\n",
            "2022-08-29 06:37:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 2 @ 259 updates\n",
            "2022-08-29 06:37:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-29 06:38:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint2.pt\n",
            "2022-08-29 06:38:35 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint2.pt (epoch 2 @ 259 updates, score 12.949) (writing took 44.321157600000106 seconds)\n",
            "2022-08-29 06:38:35 | INFO | fairseq_cli.train | end of epoch 2 (average epoch stats below)\n",
            "2022-08-29 06:38:36 | INFO | train | epoch 002 | loss 13.768 | nll_loss 13.365 | ppl 10547.6 | wps 7527.7 | ups 0.99 | wpb 7605.3 | bsz 534.4 | num_updates 259 | lr 3.2375e-05 | gnorm 2.121 | loss_scale 16 | train_wall 70 | gb_free 5.4 | wall 266\n",
            "2022-08-29 06:38:36 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 003:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:38:36 | INFO | fairseq.trainer | begin training epoch 3\n",
            "2022-08-29 06:38:36 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 003:  99% 130/131 [01:13<00:00,  1.84it/s, loss=13.06, nll_loss=12.572, ppl=6087.52, wps=6555.5, ups=0.86, wpb=7620.3, bsz=537.4, num_updates=300, lr=3.75e-05, gnorm=2.164, loss_scale=16, train_wall=54, gb_free=5.1, wall=289]2022-08-29 06:39:49 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 003 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.01s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.78s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  33% 3/9 [00:05<00:11,  1.85s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  44% 4/9 [00:07<00:09,  1.80s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.84s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  67% 6/9 [00:11<00:05,  1.88s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  78% 7/9 [00:13<00:03,  1.96s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset:  89% 8/9 [00:15<00:02,  2.10s/it]\u001b[A\n",
            "epoch 003 | valid on 'valid' subset: 100% 9/9 [00:17<00:00,  1.89s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:40:06 | INFO | valid | epoch 003 | valid on 'valid' subset | loss 11.917 | nll_loss 11.221 | ppl 2387.56 | bleu 0.06 | wps 1619.5 | wpb 3067.4 | bsz 222.2 | num_updates 390 | best_loss 11.917\n",
            "2022-08-29 06:40:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 3 @ 390 updates\n",
            "2022-08-29 06:40:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-29 06:40:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint3.pt\n",
            "2022-08-29 06:40:50 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint3.pt (epoch 3 @ 390 updates, score 11.917) (writing took 44.17192508200014 seconds)\n",
            "2022-08-29 06:40:50 | INFO | fairseq_cli.train | end of epoch 3 (average epoch stats below)\n",
            "2022-08-29 06:40:50 | INFO | train | epoch 003 | loss 12.282 | nll_loss 11.677 | ppl 3274.72 | wps 7386.1 | ups 0.97 | wpb 7605.3 | bsz 534.4 | num_updates 390 | lr 4.875e-05 | gnorm 2.302 | loss_scale 16 | train_wall 72 | gb_free 5.4 | wall 401\n",
            "2022-08-29 06:40:51 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 004:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:40:51 | INFO | fairseq.trainer | begin training epoch 4\n",
            "2022-08-29 06:40:51 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 004:  99% 130/131 [01:13<00:00,  1.83it/s, loss=11.586, nll_loss=10.838, ppl=1830, wps=13524.7, ups=1.77, wpb=7659.5, bsz=551.4, num_updates=500, lr=6.25e-05, gnorm=2.063, loss_scale=16, train_wall=56, gb_free=5.1, wall=464]2022-08-29 06:42:04 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 004 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.89s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.81s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  33% 3/9 [00:05<00:11,  1.84s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  44% 4/9 [00:07<00:08,  1.79s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.81s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.82s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.86s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.94s/it]\u001b[A\n",
            "epoch 004 | valid on 'valid' subset: 100% 9/9 [00:16<00:00,  1.75s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:42:21 | INFO | valid | epoch 004 | valid on 'valid' subset | loss 11.353 | nll_loss 10.53 | ppl 1478.25 | bleu 0.16 | wps 1696.3 | wpb 3067.4 | bsz 222.2 | num_updates 521 | best_loss 11.353\n",
            "2022-08-29 06:42:21 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 4 @ 521 updates\n",
            "2022-08-29 06:42:21 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-29 06:42:34 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint4.pt\n",
            "2022-08-29 06:43:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint4.pt (epoch 4 @ 521 updates, score 11.353) (writing took 42.98284966799997 seconds)\n",
            "2022-08-29 06:43:04 | INFO | fairseq_cli.train | end of epoch 4 (average epoch stats below)\n",
            "2022-08-29 06:43:04 | INFO | train | epoch 004 | loss 11.587 | nll_loss 10.838 | ppl 1829.88 | wps 7463.8 | ups 0.98 | wpb 7605.3 | bsz 534.4 | num_updates 521 | lr 6.5125e-05 | gnorm 2.041 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 535\n",
            "2022-08-29 06:43:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 005:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:43:04 | INFO | fairseq.trainer | begin training epoch 5\n",
            "2022-08-29 06:43:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 005:  99% 130/131 [01:13<00:00,  1.86it/s, loss=11.271, nll_loss=10.461, ppl=1409.07, wps=6550.9, ups=0.86, wpb=7605.2, bsz=540.4, num_updates=600, lr=7.5e-05, gnorm=1.921, loss_scale=16, train_wall=56, gb_free=5.3, wall=580]2022-08-29 06:44:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 005 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.73s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.56s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  33% 3/9 [00:04<00:10,  1.67s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.68s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.73s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.78s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.86s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset:  89% 8/9 [00:14<00:02,  2.07s/it]\u001b[A\n",
            "epoch 005 | valid on 'valid' subset: 100% 9/9 [00:16<00:00,  1.87s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:44:34 | INFO | valid | epoch 005 | valid on 'valid' subset | loss 10.999 | nll_loss 10.098 | ppl 1095.61 | bleu 0.44 | wps 1675.7 | wpb 3067.4 | bsz 222.2 | num_updates 652 | best_loss 10.999\n",
            "2022-08-29 06:44:34 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 5 @ 652 updates\n",
            "2022-08-29 06:44:34 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-29 06:44:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint5.pt\n",
            "2022-08-29 06:45:19 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint5.pt (epoch 5 @ 652 updates, score 10.999) (writing took 44.84575946099994 seconds)\n",
            "2022-08-29 06:45:19 | INFO | fairseq_cli.train | end of epoch 5 (average epoch stats below)\n",
            "2022-08-29 06:45:19 | INFO | train | epoch 005 | loss 11.184 | nll_loss 10.358 | ppl 1312.79 | wps 7382.3 | ups 0.97 | wpb 7605.3 | bsz 534.4 | num_updates 652 | lr 8.15e-05 | gnorm 1.831 | loss_scale 16 | train_wall 73 | gb_free 5.6 | wall 670\n",
            "2022-08-29 06:45:19 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 006:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:45:19 | INFO | fairseq.trainer | begin training epoch 6\n",
            "2022-08-29 06:45:19 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 006:  99% 130/131 [01:13<00:00,  1.82it/s, loss=10.958, nll_loss=10.098, ppl=1095.83, wps=6404.5, ups=0.85, wpb=7522.5, bsz=525.5, num_updates=700, lr=8.75e-05, gnorm=1.7, loss_scale=16, train_wall=55, gb_free=5.1, wall=698]2022-08-29 06:46:33 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.10s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  22% 2/9 [00:03<00:13,  1.88s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  33% 3/9 [00:05<00:11,  1.97s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  44% 4/9 [00:07<00:09,  1.92s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.93s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 6/9 [00:11<00:05,  1.95s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  78% 7/9 [00:13<00:03,  1.99s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  89% 8/9 [00:16<00:02,  2.08s/it]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset: 100% 9/9 [00:17<00:00,  1.87s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:46:50 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 10.557 | nll_loss 9.594 | ppl 772.9 | bleu 0.66 | wps 1595.7 | wpb 3067.4 | bsz 222.2 | num_updates 783 | best_loss 10.557\n",
            "2022-08-29 06:46:50 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 783 updates\n",
            "2022-08-29 06:46:50 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-29 06:47:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint6.pt\n",
            "2022-08-29 06:47:37 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint6.pt (epoch 6 @ 783 updates, score 10.557) (writing took 46.564387529999976 seconds)\n",
            "2022-08-29 06:47:37 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-08-29 06:47:37 | INFO | train | epoch 006 | loss 10.748 | nll_loss 9.854 | ppl 925.59 | wps 7215.6 | ups 0.95 | wpb 7605.3 | bsz 534.4 | num_updates 783 | lr 9.7875e-05 | gnorm 1.766 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 808\n",
            "2022-08-29 06:47:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 007:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:47:37 | INFO | fairseq.trainer | begin training epoch 7\n",
            "2022-08-29 06:47:37 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 007:  99% 130/131 [01:13<00:00,  1.81it/s, loss=10.321, nll_loss=9.365, ppl=659.29, wps=13465, ups=1.77, wpb=7587.6, bsz=530.4, num_updates=900, lr=0.0001125, gnorm=1.891, loss_scale=16, train_wall=56, gb_free=5.1, wall=874]2022-08-29 06:48:51 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 007 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.88s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.65s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.71s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.69s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.72s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.90s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.92s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset:  89% 8/9 [00:15<00:02,  2.03s/it]\u001b[A\n",
            "epoch 007 | valid on 'valid' subset: 100% 9/9 [00:16<00:00,  1.83s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:49:07 | INFO | valid | epoch 007 | valid on 'valid' subset | loss 10.156 | nll_loss 9.142 | ppl 564.78 | bleu 1.14 | wps 1678.5 | wpb 3067.4 | bsz 222.2 | num_updates 914 | best_loss 10.156\n",
            "2022-08-29 06:49:07 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 7 @ 914 updates\n",
            "2022-08-29 06:49:07 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-29 06:49:21 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint7.pt\n",
            "2022-08-29 06:49:54 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint7.pt (epoch 7 @ 914 updates, score 10.156) (writing took 46.30331714899967 seconds)\n",
            "2022-08-29 06:49:54 | INFO | fairseq_cli.train | end of epoch 7 (average epoch stats below)\n",
            "2022-08-29 06:49:54 | INFO | train | epoch 007 | loss 10.304 | nll_loss 9.346 | ppl 650.94 | wps 7281.4 | ups 0.96 | wpb 7605.3 | bsz 534.4 | num_updates 914 | lr 0.00011425 | gnorm 1.832 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 945\n",
            "2022-08-29 06:49:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 008:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:49:54 | INFO | fairseq.trainer | begin training epoch 8\n",
            "2022-08-29 06:49:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 008:  99% 130/131 [01:13<00:00,  1.85it/s, loss=9.884, nll_loss=8.869, ppl=467.72, wps=6374.4, ups=0.84, wpb=7629.1, bsz=555.3, num_updates=1000, lr=0.000125, gnorm=1.713, loss_scale=16, train_wall=56, gb_free=5.3, wall=994]2022-08-29 06:51:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 008 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.89s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.67s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.74s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.70s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.74s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.77s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.85s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.99s/it]\u001b[A\n",
            "epoch 008 | valid on 'valid' subset: 100% 9/9 [00:16<00:00,  1.81s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:51:24 | INFO | valid | epoch 008 | valid on 'valid' subset | loss 9.71 | nll_loss 8.586 | ppl 384.4 | bleu 1.84 | wps 1707.7 | wpb 3067.4 | bsz 222.2 | num_updates 1045 | best_loss 9.71\n",
            "2022-08-29 06:51:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 8 @ 1045 updates\n",
            "2022-08-29 06:51:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-29 06:51:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint8.pt\n",
            "2022-08-29 06:52:08 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint8.pt (epoch 8 @ 1045 updates, score 9.71) (writing took 44.18319064599973 seconds)\n",
            "2022-08-29 06:52:08 | INFO | fairseq_cli.train | end of epoch 8 (average epoch stats below)\n",
            "2022-08-29 06:52:08 | INFO | train | epoch 008 | loss 9.808 | nll_loss 8.783 | ppl 440.61 | wps 7415.2 | ups 0.97 | wpb 7605.3 | bsz 534.4 | num_updates 1045 | lr 0.000130625 | gnorm 1.774 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 1079\n",
            "2022-08-29 06:52:08 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 009:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:52:08 | INFO | fairseq.trainer | begin training epoch 9\n",
            "2022-08-29 06:52:08 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 009:  99% 130/131 [01:13<00:00,  1.85it/s, loss=9.557, nll_loss=8.498, ppl=361.42, wps=6507, ups=0.85, wpb=7625.2, bsz=517.8, num_updates=1100, lr=0.0001375, gnorm=1.784, loss_scale=16, train_wall=56, gb_free=5.3, wall=1111]2022-08-29 06:53:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 009 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.02s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.80s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  33% 3/9 [00:05<00:11,  1.92s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  44% 4/9 [00:08<00:10,  2.07s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  56% 5/9 [00:10<00:08,  2.05s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  67% 6/9 [00:12<00:06,  2.03s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  78% 7/9 [00:14<00:04,  2.08s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset:  89% 8/9 [00:16<00:02,  2.18s/it]\u001b[A\n",
            "epoch 009 | valid on 'valid' subset: 100% 9/9 [00:18<00:00,  2.03s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:53:41 | INFO | valid | epoch 009 | valid on 'valid' subset | loss 9.43 | nll_loss 8.274 | ppl 309.59 | bleu 2.49 | wps 1501.3 | wpb 3067.4 | bsz 222.2 | num_updates 1176 | best_loss 9.43\n",
            "2022-08-29 06:53:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 9 @ 1176 updates\n",
            "2022-08-29 06:53:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-29 06:53:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint9.pt\n",
            "2022-08-29 06:54:24 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint9.pt (epoch 9 @ 1176 updates, score 9.43) (writing took 43.44334281500005 seconds)\n",
            "2022-08-29 06:54:24 | INFO | fairseq_cli.train | end of epoch 9 (average epoch stats below)\n",
            "2022-08-29 06:54:24 | INFO | train | epoch 009 | loss 9.318 | nll_loss 8.225 | ppl 299.13 | wps 7332.4 | ups 0.96 | wpb 7605.3 | bsz 534.4 | num_updates 1176 | lr 0.000147 | gnorm 1.834 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 1215\n",
            "2022-08-29 06:54:24 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 010:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:54:24 | INFO | fairseq.trainer | begin training epoch 10\n",
            "2022-08-29 06:54:24 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 010:  99% 130/131 [01:13<00:00,  1.83it/s, loss=8.857, nll_loss=7.699, ppl=207.81, wps=13536.5, ups=1.77, wpb=7635.8, bsz=525.9, num_updates=1300, lr=0.0001625, gnorm=1.829, loss_scale=16, train_wall=56, gb_free=5.1, wall=1286]2022-08-29 06:55:38 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 010 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.97s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.75s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.83s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  44% 4/9 [00:07<00:08,  1.79s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  56% 5/9 [00:09<00:07,  1.81s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.82s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.90s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset:  89% 8/9 [00:15<00:02,  2.10s/it]\u001b[A\n",
            "epoch 010 | valid on 'valid' subset: 100% 9/9 [00:17<00:00,  1.98s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:55:55 | INFO | valid | epoch 010 | valid on 'valid' subset | loss 8.953 | nll_loss 7.746 | ppl 214.65 | bleu 3.97 | wps 1605.1 | wpb 3067.4 | bsz 222.2 | num_updates 1307 | best_loss 8.953\n",
            "2022-08-29 06:55:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 10 @ 1307 updates\n",
            "2022-08-29 06:55:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-29 06:56:09 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint10.pt\n",
            "2022-08-29 06:56:40 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint10.pt (epoch 10 @ 1307 updates, score 8.953) (writing took 44.5470241590001 seconds)\n",
            "2022-08-29 06:56:40 | INFO | fairseq_cli.train | end of epoch 10 (average epoch stats below)\n",
            "2022-08-29 06:56:40 | INFO | train | epoch 010 | loss 8.855 | nll_loss 7.696 | ppl 207.36 | wps 7319.5 | ups 0.96 | wpb 7605.3 | bsz 534.4 | num_updates 1307 | lr 0.000163375 | gnorm 1.875 | loss_scale 16 | train_wall 73 | gb_free 5.6 | wall 1351\n",
            "2022-08-29 06:56:40 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 011:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:56:40 | INFO | fairseq.trainer | begin training epoch 11\n",
            "2022-08-29 06:56:40 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 011:  99% 130/131 [01:13<00:00,  1.83it/s, loss=8.435, nll_loss=7.22, ppl=149.06, wps=6360.9, ups=0.84, wpb=7566.4, bsz=544, num_updates=1400, lr=0.000175, gnorm=1.842, loss_scale=16, train_wall=56, gb_free=5.1, wall=1404]2022-08-29 06:57:54 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 011 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.75s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.66s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.68s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.62s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.63s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.63s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.71s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.89s/it]\u001b[A\n",
            "epoch 011 | valid on 'valid' subset: 100% 9/9 [00:15<00:00,  1.79s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 06:58:10 | INFO | valid | epoch 011 | valid on 'valid' subset | loss 8.562 | nll_loss 7.276 | ppl 154.94 | bleu 6.75 | wps 1771.8 | wpb 3067.4 | bsz 222.2 | num_updates 1438 | best_loss 8.562\n",
            "2022-08-29 06:58:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 11 @ 1438 updates\n",
            "2022-08-29 06:58:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-29 06:58:23 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint11.pt\n",
            "2022-08-29 06:58:56 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint11.pt (epoch 11 @ 1438 updates, score 8.562) (writing took 45.60957933200007 seconds)\n",
            "2022-08-29 06:58:56 | INFO | fairseq_cli.train | end of epoch 11 (average epoch stats below)\n",
            "2022-08-29 06:58:56 | INFO | train | epoch 011 | loss 8.383 | nll_loss 7.159 | ppl 142.96 | wps 7355.9 | ups 0.97 | wpb 7605.3 | bsz 534.4 | num_updates 1438 | lr 0.00017975 | gnorm 1.846 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 1486\n",
            "2022-08-29 06:58:56 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 012:   0% 0/131 [00:00<?, ?it/s]2022-08-29 06:58:56 | INFO | fairseq.trainer | begin training epoch 12\n",
            "2022-08-29 06:58:56 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 012:  99% 130/131 [01:13<00:00,  1.82it/s, loss=8.07, nll_loss=6.802, ppl=111.59, wps=6471.2, ups=0.85, wpb=7652.9, bsz=519.9, num_updates=1500, lr=0.0001875, gnorm=1.782, loss_scale=16, train_wall=56, gb_free=5, wall=1523]2022-08-29 07:00:10 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 012 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.66s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.46s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.55s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.52s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.54s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.57s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.71s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.88s/it]\u001b[A\n",
            "epoch 012 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.70s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:00:25 | INFO | valid | epoch 012 | valid on 'valid' subset | loss 8.221 | nll_loss 6.896 | ppl 119.07 | bleu 9.46 | wps 1851.2 | wpb 3067.4 | bsz 222.2 | num_updates 1569 | best_loss 8.221\n",
            "2022-08-29 07:00:25 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 12 @ 1569 updates\n",
            "2022-08-29 07:00:25 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-29 07:00:38 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint12.pt\n",
            "2022-08-29 07:01:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint12.pt (epoch 12 @ 1569 updates, score 8.221) (writing took 45.248920738999914 seconds)\n",
            "2022-08-29 07:01:11 | INFO | fairseq_cli.train | end of epoch 12 (average epoch stats below)\n",
            "2022-08-29 07:01:11 | INFO | train | epoch 012 | loss 7.882 | nll_loss 6.589 | ppl 96.24 | wps 7375.4 | ups 0.97 | wpb 7605.3 | bsz 534.4 | num_updates 1569 | lr 0.000196125 | gnorm 1.75 | loss_scale 16 | train_wall 73 | gb_free 5.6 | wall 1622\n",
            "2022-08-29 07:01:11 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 013:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:01:11 | INFO | fairseq.trainer | begin training epoch 13\n",
            "2022-08-29 07:01:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 013:  99% 130/131 [01:13<00:00,  1.85it/s, loss=7.732, nll_loss=6.417, ppl=85.42, wps=6489.1, ups=0.85, wpb=7594.2, bsz=542.2, num_updates=1600, lr=0.0002, gnorm=1.798, loss_scale=16, train_wall=55, gb_free=5.2, wall=1640]2022-08-29 07:02:25 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 013 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.90s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  22% 2/9 [00:03<00:12,  1.81s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.81s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  44% 4/9 [00:07<00:08,  1.72s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.71s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.78s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  78% 7/9 [00:12<00:03,  1.88s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset:  89% 8/9 [00:14<00:01,  1.89s/it]\u001b[A\n",
            "epoch 013 | valid on 'valid' subset: 100% 9/9 [00:15<00:00,  1.68s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:02:41 | INFO | valid | epoch 013 | valid on 'valid' subset | loss 8.094 | nll_loss 6.694 | ppl 103.52 | bleu 9.85 | wps 1751.3 | wpb 3067.4 | bsz 222.2 | num_updates 1700 | best_loss 8.094\n",
            "2022-08-29 07:02:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 13 @ 1700 updates\n",
            "2022-08-29 07:02:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-29 07:02:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint13.pt\n",
            "2022-08-29 07:03:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint13.pt (epoch 13 @ 1700 updates, score 8.094) (writing took 45.63426422500015 seconds)\n",
            "2022-08-29 07:03:26 | INFO | fairseq_cli.train | end of epoch 13 (average epoch stats below)\n",
            "2022-08-29 07:03:26 | INFO | train | epoch 013 | loss 7.451 | nll_loss 6.094 | ppl 68.33 | wps 7347.8 | ups 0.97 | wpb 7605.3 | bsz 534.4 | num_updates 1700 | lr 0.0002125 | gnorm 1.868 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 1757\n",
            "2022-08-29 07:03:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 014:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:03:26 | INFO | fairseq.trainer | begin training epoch 14\n",
            "2022-08-29 07:03:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 014:  99% 130/131 [01:13<00:00,  1.81it/s, loss=6.998, nll_loss=5.578, ppl=47.76, wps=6388.9, ups=0.84, wpb=7581, bsz=530.6, num_updates=1800, lr=0.000225, gnorm=1.733, loss_scale=16, train_wall=56, gb_free=5.1, wall=1814]2022-08-29 07:04:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 014 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.78s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.56s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.61s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.58s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.60s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  67% 6/9 [00:09<00:05,  1.67s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.77s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.78s/it]\u001b[A\n",
            "epoch 014 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.59s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:04:55 | INFO | valid | epoch 014 | valid on 'valid' subset | loss 7.696 | nll_loss 6.248 | ppl 76 | bleu 13.64 | wps 1881.8 | wpb 3067.4 | bsz 222.2 | num_updates 1831 | best_loss 7.696\n",
            "2022-08-29 07:04:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 14 @ 1831 updates\n",
            "2022-08-29 07:04:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-29 07:05:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint14.pt\n",
            "2022-08-29 07:05:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint14.pt (epoch 14 @ 1831 updates, score 7.696) (writing took 46.3636650200001 seconds)\n",
            "2022-08-29 07:05:41 | INFO | fairseq_cli.train | end of epoch 14 (average epoch stats below)\n",
            "2022-08-29 07:05:41 | INFO | train | epoch 014 | loss 6.976 | nll_loss 5.551 | ppl 46.87 | wps 7369.5 | ups 0.97 | wpb 7605.3 | bsz 534.4 | num_updates 1831 | lr 0.000228875 | gnorm 1.762 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 1892\n",
            "2022-08-29 07:05:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 015:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:05:42 | INFO | fairseq.trainer | begin training epoch 15\n",
            "2022-08-29 07:05:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 015:  99% 130/131 [01:13<00:00,  1.83it/s, loss=6.666, nll_loss=5.197, ppl=36.67, wps=6477.3, ups=0.84, wpb=7670.6, bsz=549.3, num_updates=1900, lr=0.0002375, gnorm=1.778, loss_scale=16, train_wall=56, gb_free=5.1, wall=1933]2022-08-29 07:06:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 015 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  11% 1/9 [00:01<00:12,  1.57s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.52s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.43s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.43s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.51s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.62s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.64s/it]\u001b[A\n",
            "epoch 015 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:07:10 | INFO | valid | epoch 015 | valid on 'valid' subset | loss 7.542 | nll_loss 6.099 | ppl 68.56 | bleu 17.94 | wps 2034.1 | wpb 3067.4 | bsz 222.2 | num_updates 1962 | best_loss 7.542\n",
            "2022-08-29 07:07:10 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 15 @ 1962 updates\n",
            "2022-08-29 07:07:10 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-29 07:07:22 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint15.pt\n",
            "2022-08-29 07:07:53 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint15.pt (epoch 15 @ 1962 updates, score 7.542) (writing took 43.94896083000003 seconds)\n",
            "2022-08-29 07:07:54 | INFO | fairseq_cli.train | end of epoch 15 (average epoch stats below)\n",
            "2022-08-29 07:07:54 | INFO | train | epoch 015 | loss 6.529 | nll_loss 5.038 | ppl 32.84 | wps 7527.3 | ups 0.99 | wpb 7605.3 | bsz 534.4 | num_updates 1962 | lr 0.00024525 | gnorm 1.673 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 2025\n",
            "2022-08-29 07:07:54 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 016:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:07:54 | INFO | fairseq.trainer | begin training epoch 16\n",
            "2022-08-29 07:07:54 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 016:  99% 130/131 [01:13<00:00,  1.81it/s, loss=6.365, nll_loss=4.849, ppl=28.82, wps=6679.8, ups=0.87, wpb=7638.2, bsz=527.7, num_updates=2000, lr=0.00025, gnorm=1.649, loss_scale=16, train_wall=56, gb_free=5.1, wall=2047]2022-08-29 07:09:08 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 016 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.72s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.49s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.54s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.51s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.52s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.61s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.77s/it]\u001b[A\n",
            "epoch 016 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.67s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:09:23 | INFO | valid | epoch 016 | valid on 'valid' subset | loss 7.221 | nll_loss 5.7 | ppl 51.99 | bleu 18.27 | wps 1908.8 | wpb 3067.4 | bsz 222.2 | num_updates 2093 | best_loss 7.221\n",
            "2022-08-29 07:09:23 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 16 @ 2093 updates\n",
            "2022-08-29 07:09:23 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-29 07:09:36 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint16.pt\n",
            "2022-08-29 07:10:49 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint16.pt (epoch 16 @ 2093 updates, score 7.221) (writing took 86.43813644300008 seconds)\n",
            "2022-08-29 07:10:49 | INFO | fairseq_cli.train | end of epoch 16 (average epoch stats below)\n",
            "2022-08-29 07:10:49 | INFO | train | epoch 016 | loss 6.111 | nll_loss 4.557 | ppl 23.54 | wps 5686.7 | ups 0.75 | wpb 7605.3 | bsz 534.4 | num_updates 2093 | lr 0.000261625 | gnorm 1.688 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 2200\n",
            "2022-08-29 07:10:49 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 017:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:10:49 | INFO | fairseq.trainer | begin training epoch 17\n",
            "2022-08-29 07:10:49 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 017:  99% 130/131 [01:13<00:00,  1.84it/s, loss=5.693, nll_loss=4.076, ppl=16.86, wps=13421.8, ups=1.76, wpb=7615.6, bsz=544, num_updates=2200, lr=0.000275, gnorm=1.607, loss_scale=16, train_wall=56, gb_free=5.1, wall=2261]2022-08-29 07:12:03 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 017 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.04s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.64s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  33% 3/9 [00:05<00:09,  1.64s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.58s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.57s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.58s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.60s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.66s/it]\u001b[A\n",
            "epoch 017 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.51s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:12:17 | INFO | valid | epoch 017 | valid on 'valid' subset | loss 7.071 | nll_loss 5.53 | ppl 46.2 | bleu 19.21 | wps 1998.6 | wpb 3067.4 | bsz 222.2 | num_updates 2224 | best_loss 7.071\n",
            "2022-08-29 07:12:17 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 17 @ 2224 updates\n",
            "2022-08-29 07:12:17 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-29 07:12:31 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint17.pt\n",
            "2022-08-29 07:13:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint17.pt (epoch 17 @ 2224 updates, score 7.071) (writing took 87.03203833599991 seconds)\n",
            "2022-08-29 07:13:45 | INFO | fairseq_cli.train | end of epoch 17 (average epoch stats below)\n",
            "2022-08-29 07:13:45 | INFO | train | epoch 017 | loss 5.71 | nll_loss 4.095 | ppl 17.08 | wps 5675.4 | ups 0.75 | wpb 7605.3 | bsz 534.4 | num_updates 2224 | lr 0.000278 | gnorm 1.645 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 2375\n",
            "2022-08-29 07:13:45 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 018:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:13:45 | INFO | fairseq.trainer | begin training epoch 18\n",
            "2022-08-29 07:13:45 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 018:  99% 130/131 [01:14<00:00,  1.81it/s, loss=5.42, nll_loss=3.758, ppl=13.53, wps=4804.7, ups=0.63, wpb=7613, bsz=527.9, num_updates=2300, lr=0.0002875, gnorm=1.604, loss_scale=16, train_wall=56, gb_free=5.1, wall=2420]2022-08-29 07:14:59 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 018 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.45s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.46s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.48s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.52s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 018 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:15:13 | INFO | valid | epoch 018 | valid on 'valid' subset | loss 6.885 | nll_loss 5.289 | ppl 39.09 | bleu 22.61 | wps 2090 | wpb 3067.4 | bsz 222.2 | num_updates 2355 | best_loss 6.885\n",
            "2022-08-29 07:15:13 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 18 @ 2355 updates\n",
            "2022-08-29 07:15:13 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-29 07:15:26 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint18.pt\n",
            "2022-08-29 07:16:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint18.pt (epoch 18 @ 2355 updates, score 6.885) (writing took 88.81808322499955 seconds)\n",
            "2022-08-29 07:16:41 | INFO | fairseq_cli.train | end of epoch 18 (average epoch stats below)\n",
            "2022-08-29 07:16:41 | INFO | train | epoch 018 | loss 5.303 | nll_loss 3.624 | ppl 12.33 | wps 5634.2 | ups 0.74 | wpb 7605.3 | bsz 534.4 | num_updates 2355 | lr 0.000294375 | gnorm 1.489 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 2552\n",
            "2022-08-29 07:16:42 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 019:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:16:42 | INFO | fairseq.trainer | begin training epoch 19\n",
            "2022-08-29 07:16:42 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 019:  99% 130/131 [01:14<00:00,  1.79it/s, loss=5.125, nll_loss=3.419, ppl=10.69, wps=4742.1, ups=0.63, wpb=7571.6, bsz=529.4, num_updates=2400, lr=0.0003, gnorm=1.456, loss_scale=16, train_wall=55, gb_free=5.4, wall=2579]2022-08-29 07:17:56 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 019 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.81s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.61s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.78s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.69s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.68s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  67% 6/9 [00:10<00:04,  1.66s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.68s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.73s/it]\u001b[A\n",
            "epoch 019 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.55s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:18:11 | INFO | valid | epoch 019 | valid on 'valid' subset | loss 6.929 | nll_loss 5.306 | ppl 39.56 | bleu 21.85 | wps 1880.9 | wpb 3067.4 | bsz 222.2 | num_updates 2486 | best_loss 6.885\n",
            "2022-08-29 07:18:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 19 @ 2486 updates\n",
            "2022-08-29 07:18:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-29 07:18:25 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint19.pt\n",
            "2022-08-29 07:18:44 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint19.pt (epoch 19 @ 2486 updates, score 6.929) (writing took 32.94058014400025 seconds)\n",
            "2022-08-29 07:18:44 | INFO | fairseq_cli.train | end of epoch 19 (average epoch stats below)\n",
            "2022-08-29 07:18:44 | INFO | train | epoch 019 | loss 4.922 | nll_loss 3.182 | ppl 9.07 | wps 8119 | ups 1.07 | wpb 7605.3 | bsz 534.4 | num_updates 2486 | lr 0.00031075 | gnorm 1.442 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 2675\n",
            "2022-08-29 07:18:44 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 020:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:18:44 | INFO | fairseq.trainer | begin training epoch 20\n",
            "2022-08-29 07:18:44 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 020:  99% 130/131 [01:13<00:00,  1.81it/s, loss=4.573, nll_loss=2.775, ppl=6.84, wps=13461.7, ups=1.77, wpb=7598.2, bsz=543.8, num_updates=2600, lr=0.000325, gnorm=1.339, loss_scale=16, train_wall=56, gb_free=5, wall=2740]2022-08-29 07:19:58 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 020 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.64s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  22% 2/9 [00:02<00:09,  1.41s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.43s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  44% 4/9 [00:05<00:06,  1.38s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.39s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.39s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  78% 7/9 [00:09<00:02,  1.41s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset:  89% 8/9 [00:11<00:01,  1.50s/it]\u001b[A\n",
            "epoch 020 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.37s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:20:11 | INFO | valid | epoch 020 | valid on 'valid' subset | loss 6.844 | nll_loss 5.154 | ppl 35.61 | bleu 25.52 | wps 2219 | wpb 3067.4 | bsz 222.2 | num_updates 2617 | best_loss 6.844\n",
            "2022-08-29 07:20:11 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 20 @ 2617 updates\n",
            "2022-08-29 07:20:11 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-29 07:20:24 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint20.pt\n",
            "2022-08-29 07:21:36 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint20.pt (epoch 20 @ 2617 updates, score 6.844) (writing took 84.66037690899975 seconds)\n",
            "2022-08-29 07:21:36 | INFO | fairseq_cli.train | end of epoch 20 (average epoch stats below)\n",
            "2022-08-29 07:21:36 | INFO | train | epoch 020 | loss 4.584 | nll_loss 2.788 | ppl 6.91 | wps 5796.3 | ups 0.76 | wpb 7605.3 | bsz 534.4 | num_updates 2617 | lr 0.000327125 | gnorm 1.379 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 2847\n",
            "2022-08-29 07:21:37 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 021:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:21:38 | INFO | fairseq.trainer | begin training epoch 21\n",
            "2022-08-29 07:21:38 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 021:  99% 130/131 [01:14<00:00,  1.84it/s, loss=4.338, nll_loss=2.499, ppl=5.65, wps=4878.8, ups=0.64, wpb=7621.8, bsz=522.6, num_updates=2700, lr=0.0003375, gnorm=1.416, loss_scale=16, train_wall=56, gb_free=5.1, wall=2896]2022-08-29 07:22:52 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 021 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.71s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.46s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.52s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.68s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.72s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.73s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.68s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.68s/it]\u001b[A\n",
            "epoch 021 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:23:06 | INFO | valid | epoch 021 | valid on 'valid' subset | loss 6.685 | nll_loss 4.994 | ppl 31.86 | bleu 26.34 | wps 1934.5 | wpb 3067.4 | bsz 222.2 | num_updates 2748 | best_loss 6.685\n",
            "2022-08-29 07:23:06 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 21 @ 2748 updates\n",
            "2022-08-29 07:23:06 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-29 07:23:20 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint21.pt\n",
            "2022-08-29 07:24:33 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint21.pt (epoch 21 @ 2748 updates, score 6.685) (writing took 86.72597294300067 seconds)\n",
            "2022-08-29 07:24:33 | INFO | fairseq_cli.train | end of epoch 21 (average epoch stats below)\n",
            "2022-08-29 07:24:33 | INFO | train | epoch 021 | loss 4.267 | nll_loss 2.416 | ppl 5.34 | wps 5631.1 | ups 0.74 | wpb 7605.3 | bsz 534.4 | num_updates 2748 | lr 0.0003435 | gnorm 1.338 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 3024\n",
            "2022-08-29 07:24:33 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 022:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:24:34 | INFO | fairseq.trainer | begin training epoch 22\n",
            "2022-08-29 07:24:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 022:  99% 130/131 [01:14<00:00,  1.79it/s, loss=4.08, nll_loss=2.198, ppl=4.59, wps=4777.5, ups=0.63, wpb=7586.6, bsz=531.8, num_updates=2800, lr=0.00035, gnorm=1.249, loss_scale=16, train_wall=55, gb_free=5.1, wall=3055]2022-08-29 07:25:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 022 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.50s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.55s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.53s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.53s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.58s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.65s/it]\u001b[A\n",
            "epoch 022 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.49s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:26:02 | INFO | valid | epoch 022 | valid on 'valid' subset | loss 6.702 | nll_loss 4.993 | ppl 31.85 | bleu 26.87 | wps 2016.4 | wpb 3067.4 | bsz 222.2 | num_updates 2879 | best_loss 6.685\n",
            "2022-08-29 07:26:02 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 22 @ 2879 updates\n",
            "2022-08-29 07:26:02 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-29 07:26:16 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint22.pt\n",
            "2022-08-29 07:26:34 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint22.pt (epoch 22 @ 2879 updates, score 6.702) (writing took 32.01990938199924 seconds)\n",
            "2022-08-29 07:26:34 | INFO | fairseq_cli.train | end of epoch 22 (average epoch stats below)\n",
            "2022-08-29 07:26:34 | INFO | train | epoch 022 | loss 3.941 | nll_loss 2.033 | ppl 4.09 | wps 8214.7 | ups 1.08 | wpb 7605.3 | bsz 534.4 | num_updates 2879 | lr 0.000359875 | gnorm 1.217 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 3145\n",
            "2022-08-29 07:26:34 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 023:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:26:34 | INFO | fairseq.trainer | begin training epoch 23\n",
            "2022-08-29 07:26:34 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 023:  99% 130/131 [01:13<00:00,  1.86it/s, loss=3.637, nll_loss=1.673, ppl=3.19, wps=13409.5, ups=1.77, wpb=7594.5, bsz=541, num_updates=3000, lr=0.000375, gnorm=1.103, loss_scale=16, train_wall=56, gb_free=5, wall=3214]2022-08-29 07:27:48 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 023 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  11% 1/9 [00:01<00:12,  1.52s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  22% 2/9 [00:02<00:09,  1.32s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.37s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  44% 4/9 [00:05<00:06,  1.33s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  56% 5/9 [00:06<00:05,  1.36s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.38s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.57s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset:  89% 8/9 [00:11<00:01,  1.58s/it]\u001b[A\n",
            "epoch 023 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.41s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:28:01 | INFO | valid | epoch 023 | valid on 'valid' subset | loss 6.755 | nll_loss 5.078 | ppl 33.78 | bleu 29.63 | wps 2161.9 | wpb 3067.4 | bsz 222.2 | num_updates 3010 | best_loss 6.685\n",
            "2022-08-29 07:28:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 23 @ 3010 updates\n",
            "2022-08-29 07:28:01 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-29 07:28:14 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint23.pt\n",
            "2022-08-29 07:28:32 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint23.pt (epoch 23 @ 3010 updates, score 6.755) (writing took 30.78616841100029 seconds)\n",
            "2022-08-29 07:28:32 | INFO | fairseq_cli.train | end of epoch 23 (average epoch stats below)\n",
            "2022-08-29 07:28:32 | INFO | train | epoch 023 | loss 3.647 | nll_loss 1.685 | ppl 3.21 | wps 8459 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 3010 | lr 0.00037625 | gnorm 1.128 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 3263\n",
            "2022-08-29 07:28:32 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 024:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:28:32 | INFO | fairseq.trainer | begin training epoch 24\n",
            "2022-08-29 07:28:32 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 024:  99% 130/131 [01:13<00:00,  1.88it/s, loss=3.397, nll_loss=1.389, ppl=2.62, wps=7584.5, ups=0.99, wpb=7646.8, bsz=531.7, num_updates=3100, lr=0.0003875, gnorm=1.036, loss_scale=16, train_wall=56, gb_free=5.1, wall=3315]2022-08-29 07:29:46 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 024 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.46s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.60s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.65s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.63s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.66s/it]\u001b[A\n",
            "epoch 024 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:30:00 | INFO | valid | epoch 024 | valid on 'valid' subset | loss 6.677 | nll_loss 4.984 | ppl 31.65 | bleu 28.44 | wps 1993.7 | wpb 3067.4 | bsz 222.2 | num_updates 3141 | best_loss 6.677\n",
            "2022-08-29 07:30:00 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 24 @ 3141 updates\n",
            "2022-08-29 07:30:00 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-29 07:30:13 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint24.pt\n",
            "2022-08-29 07:31:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint24.pt (epoch 24 @ 3141 updates, score 6.677) (writing took 86.73316610800066 seconds)\n",
            "2022-08-29 07:31:27 | INFO | fairseq_cli.train | end of epoch 24 (average epoch stats below)\n",
            "2022-08-29 07:31:28 | INFO | train | epoch 024 | loss 3.387 | nll_loss 1.375 | ppl 2.59 | wps 5672.1 | ups 0.75 | wpb 7605.3 | bsz 534.4 | num_updates 3141 | lr 0.000392625 | gnorm 1.032 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 3439\n",
            "2022-08-29 07:31:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 025:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:31:28 | INFO | fairseq.trainer | begin training epoch 25\n",
            "2022-08-29 07:31:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 025:  99% 130/131 [01:13<00:00,  1.81it/s, loss=3.257, nll_loss=1.22, ppl=2.33, wps=4784.7, ups=0.63, wpb=7550.5, bsz=531.5, num_updates=3200, lr=0.0004, gnorm=0.986, loss_scale=16, train_wall=55, gb_free=5, wall=3473]2022-08-29 07:32:42 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 025 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.47s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.50s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.47s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.48s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.48s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.53s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 025 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.44s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:32:55 | INFO | valid | epoch 025 | valid on 'valid' subset | loss 6.751 | nll_loss 5.037 | ppl 32.83 | bleu 28.19 | wps 2084.8 | wpb 3067.4 | bsz 222.2 | num_updates 3272 | best_loss 6.677\n",
            "2022-08-29 07:32:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 25 @ 3272 updates\n",
            "2022-08-29 07:32:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-29 07:33:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint25.pt\n",
            "2022-08-29 07:33:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint25.pt (epoch 25 @ 3272 updates, score 6.751) (writing took 30.798762446999717 seconds)\n",
            "2022-08-29 07:33:26 | INFO | fairseq_cli.train | end of epoch 25 (average epoch stats below)\n",
            "2022-08-29 07:33:26 | INFO | train | epoch 025 | loss 3.161 | nll_loss 1.103 | ppl 2.15 | wps 8433.4 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 3272 | lr 0.000409 | gnorm 0.936 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 3557\n",
            "2022-08-29 07:33:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 026:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:33:26 | INFO | fairseq.trainer | begin training epoch 26\n",
            "2022-08-29 07:33:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 026:  99% 130/131 [01:13<00:00,  1.79it/s, loss=3.026, nll_loss=0.935, ppl=1.91, wps=13508.1, ups=1.78, wpb=7607.5, bsz=529.8, num_updates=3400, lr=0.000425, gnorm=0.898, loss_scale=16, train_wall=56, gb_free=5.1, wall=3629]2022-08-29 07:34:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 026 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.04s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.60s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.56s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.47s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.47s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.46s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.47s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.50s/it]\u001b[A\n",
            "epoch 026 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.35s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:34:53 | INFO | valid | epoch 026 | valid on 'valid' subset | loss 6.805 | nll_loss 5.104 | ppl 34.39 | bleu 29.56 | wps 2199.4 | wpb 3067.4 | bsz 222.2 | num_updates 3403 | best_loss 6.677\n",
            "2022-08-29 07:34:53 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 26 @ 3403 updates\n",
            "2022-08-29 07:34:53 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-29 07:35:06 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint26.pt\n",
            "2022-08-29 07:35:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint26.pt (epoch 26 @ 3403 updates, score 6.805) (writing took 32.712795577999714 seconds)\n",
            "2022-08-29 07:35:26 | INFO | fairseq_cli.train | end of epoch 26 (average epoch stats below)\n",
            "2022-08-29 07:35:26 | INFO | train | epoch 026 | loss 3.019 | nll_loss 0.927 | ppl 1.9 | wps 8302.3 | ups 1.09 | wpb 7605.3 | bsz 534.4 | num_updates 3403 | lr 0.000425375 | gnorm 0.916 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 3677\n",
            "2022-08-29 07:35:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 027:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:35:26 | INFO | fairseq.trainer | begin training epoch 27\n",
            "2022-08-29 07:35:26 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 027:  99% 130/131 [01:13<00:00,  1.83it/s, loss=2.858, nll_loss=0.733, ppl=1.66, wps=7387.1, ups=0.97, wpb=7617.4, bsz=535.3, num_updates=3500, lr=0.0004375, gnorm=0.767, loss_scale=16, train_wall=56, gb_free=5.3, wall=3732]2022-08-29 07:36:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 027 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  11% 1/9 [00:01<00:14,  1.77s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.56s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.73s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.75s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.75s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  67% 6/9 [00:10<00:05,  1.70s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.70s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.73s/it]\u001b[A\n",
            "epoch 027 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.55s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:36:55 | INFO | valid | epoch 027 | valid on 'valid' subset | loss 6.787 | nll_loss 5.069 | ppl 33.56 | bleu 27.55 | wps 1864 | wpb 3067.4 | bsz 222.2 | num_updates 3534 | best_loss 6.677\n",
            "2022-08-29 07:36:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 27 @ 3534 updates\n",
            "2022-08-29 07:36:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-29 07:37:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint27.pt\n",
            "2022-08-29 07:37:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint27.pt (epoch 27 @ 3534 updates, score 6.787) (writing took 31.74905203200069 seconds)\n",
            "2022-08-29 07:37:27 | INFO | fairseq_cli.train | end of epoch 27 (average epoch stats below)\n",
            "2022-08-29 07:37:27 | INFO | train | epoch 027 | loss 2.88 | nll_loss 0.758 | ppl 1.69 | wps 8245.9 | ups 1.08 | wpb 7605.3 | bsz 534.4 | num_updates 3534 | lr 0.00044175 | gnorm 0.818 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 3798\n",
            "2022-08-29 07:37:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 028:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:37:27 | INFO | fairseq.trainer | begin training epoch 28\n",
            "2022-08-29 07:37:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 028:  99% 130/131 [01:13<00:00,  1.81it/s, loss=2.839, nll_loss=0.707, ppl=1.63, wps=7365.1, ups=0.97, wpb=7594.6, bsz=529, num_updates=3600, lr=0.00045, gnorm=0.811, loss_scale=16, train_wall=56, gb_free=5.1, wall=3836]2022-08-29 07:38:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 028 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.71s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.50s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.52s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.56s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.53s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.52s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.54s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 028 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.43s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:38:54 | INFO | valid | epoch 028 | valid on 'valid' subset | loss 6.764 | nll_loss 5.052 | ppl 33.17 | bleu 29.5 | wps 2064.4 | wpb 3067.4 | bsz 222.2 | num_updates 3665 | best_loss 6.677\n",
            "2022-08-29 07:38:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 28 @ 3665 updates\n",
            "2022-08-29 07:38:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-29 07:39:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint28.pt\n",
            "2022-08-29 07:39:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint28.pt (epoch 28 @ 3665 updates, score 6.764) (writing took 33.35112394399948 seconds)\n",
            "2022-08-29 07:39:27 | INFO | fairseq_cli.train | end of epoch 28 (average epoch stats below)\n",
            "2022-08-29 07:39:27 | INFO | train | epoch 028 | loss 2.807 | nll_loss 0.67 | ppl 1.59 | wps 8257 | ups 1.09 | wpb 7605.3 | bsz 534.4 | num_updates 3665 | lr 0.000458125 | gnorm 0.774 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 3918\n",
            "2022-08-29 07:39:28 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 029:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:39:28 | INFO | fairseq.trainer | begin training epoch 29\n",
            "2022-08-29 07:39:28 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 029:  99% 130/131 [01:13<00:00,  1.85it/s, loss=2.794, nll_loss=0.656, ppl=1.58, wps=7333.1, ups=0.97, wpb=7548.5, bsz=537, num_updates=3700, lr=0.0004625, gnorm=0.774, loss_scale=16, train_wall=55, gb_free=5.1, wall=3939]2022-08-29 07:40:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 029 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.69s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.47s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.51s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.46s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.48s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.50s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.54s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.62s/it]\u001b[A\n",
            "epoch 029 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:40:55 | INFO | valid | epoch 029 | valid on 'valid' subset | loss 6.738 | nll_loss 5.041 | ppl 32.93 | bleu 29.09 | wps 2064.1 | wpb 3067.4 | bsz 222.2 | num_updates 3796 | best_loss 6.677\n",
            "2022-08-29 07:40:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 29 @ 3796 updates\n",
            "2022-08-29 07:40:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-29 07:41:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint29.pt\n",
            "2022-08-29 07:41:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint29.pt (epoch 29 @ 3796 updates, score 6.738) (writing took 32.36095569700046 seconds)\n",
            "2022-08-29 07:41:27 | INFO | fairseq_cli.train | end of epoch 29 (average epoch stats below)\n",
            "2022-08-29 07:41:27 | INFO | train | epoch 029 | loss 2.742 | nll_loss 0.599 | ppl 1.51 | wps 8316.4 | ups 1.09 | wpb 7605.3 | bsz 534.4 | num_updates 3796 | lr 0.0004745 | gnorm 0.694 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 4038\n",
            "2022-08-29 07:41:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 030:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:41:27 | INFO | fairseq.trainer | begin training epoch 30\n",
            "2022-08-29 07:41:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 030:  99% 130/131 [01:13<00:00,  1.84it/s, loss=2.707, nll_loss=0.562, ppl=1.48, wps=13554.9, ups=1.78, wpb=7635.5, bsz=546.3, num_updates=3900, lr=0.0004875, gnorm=0.709, loss_scale=16, train_wall=56, gb_free=5, wall=4098]2022-08-29 07:42:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 030 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.44s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.45s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.53s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.54s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.58s/it]\u001b[A\n",
            "epoch 030 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.41s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:42:55 | INFO | valid | epoch 030 | valid on 'valid' subset | loss 6.715 | nll_loss 5.016 | ppl 32.36 | bleu 29.67 | wps 2106.9 | wpb 3067.4 | bsz 222.2 | num_updates 3927 | best_loss 6.677\n",
            "2022-08-29 07:42:55 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 30 @ 3927 updates\n",
            "2022-08-29 07:42:55 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-29 07:43:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint30.pt\n",
            "2022-08-29 07:43:27 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint30.pt (epoch 30 @ 3927 updates, score 6.715) (writing took 32.14814886400018 seconds)\n",
            "2022-08-29 07:43:27 | INFO | fairseq_cli.train | end of epoch 30 (average epoch stats below)\n",
            "2022-08-29 07:43:27 | INFO | train | epoch 030 | loss 2.721 | nll_loss 0.58 | ppl 1.49 | wps 8317.5 | ups 1.09 | wpb 7605.3 | bsz 534.4 | num_updates 3927 | lr 0.000490875 | gnorm 0.716 | loss_scale 16 | train_wall 73 | gb_free 5.7 | wall 4158\n",
            "2022-08-29 07:43:27 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 031:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:43:27 | INFO | fairseq.trainer | begin training epoch 31\n",
            "2022-08-29 07:43:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 031:  99% 130/131 [01:13<00:00,  1.87it/s, loss=2.674, nll_loss=0.53, ppl=1.44, wps=7425.8, ups=0.98, wpb=7602.6, bsz=537, num_updates=4000, lr=0.0005, gnorm=0.624, loss_scale=16, train_wall=56, gb_free=5.1, wall=4200]2022-08-29 07:44:41 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 031 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.71s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.48s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.46s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.49s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.50s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.55s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.61s/it]\u001b[A\n",
            "epoch 031 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:44:54 | INFO | valid | epoch 031 | valid on 'valid' subset | loss 6.85 | nll_loss 5.172 | ppl 36.05 | bleu 29.28 | wps 2070.6 | wpb 3067.4 | bsz 222.2 | num_updates 4058 | best_loss 6.677\n",
            "2022-08-29 07:44:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 31 @ 4058 updates\n",
            "2022-08-29 07:44:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-29 07:45:08 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint31.pt\n",
            "2022-08-29 07:45:26 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint31.pt (epoch 31 @ 4058 updates, score 6.85) (writing took 32.00891813099952 seconds)\n",
            "2022-08-29 07:45:26 | INFO | fairseq_cli.train | end of epoch 31 (average epoch stats below)\n",
            "2022-08-29 07:45:26 | INFO | train | epoch 031 | loss 2.679 | nll_loss 0.539 | ppl 1.45 | wps 8346.5 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 4058 | lr 0.000496414 | gnorm 0.658 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 4277\n",
            "2022-08-29 07:45:26 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 032:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:45:27 | INFO | fairseq.trainer | begin training epoch 32\n",
            "2022-08-29 07:45:27 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 032:  99% 130/131 [01:13<00:00,  1.85it/s, loss=2.681, nll_loss=0.544, ppl=1.46, wps=7407.1, ups=0.99, wpb=7517, bsz=510.2, num_updates=4100, lr=0.000493865, gnorm=0.675, loss_scale=16, train_wall=55, gb_free=5.2, wall=4301]2022-08-29 07:46:40 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 032 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.69s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.50s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.47s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.48s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.50s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.52s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.65s/it]\u001b[A\n",
            "epoch 032 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.46s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:46:54 | INFO | valid | epoch 032 | valid on 'valid' subset | loss 6.77 | nll_loss 5.101 | ppl 34.32 | bleu 30.11 | wps 2065 | wpb 3067.4 | bsz 222.2 | num_updates 4189 | best_loss 6.677\n",
            "2022-08-29 07:46:54 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 32 @ 4189 updates\n",
            "2022-08-29 07:46:54 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-29 07:47:07 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint32.pt\n",
            "2022-08-29 07:47:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint32.pt (epoch 32 @ 4189 updates, score 6.77) (writing took 29.416477359000055 seconds)\n",
            "2022-08-29 07:47:23 | INFO | fairseq_cli.train | end of epoch 32 (average epoch stats below)\n",
            "2022-08-29 07:47:23 | INFO | train | epoch 032 | loss 2.622 | nll_loss 0.482 | ppl 1.4 | wps 8530.8 | ups 1.12 | wpb 7605.3 | bsz 534.4 | num_updates 4189 | lr 0.00048859 | gnorm 0.565 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 4394\n",
            "2022-08-29 07:47:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 033:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:47:23 | INFO | fairseq.trainer | begin training epoch 33\n",
            "2022-08-29 07:47:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 033:  99% 130/131 [01:13<00:00,  1.86it/s, loss=2.582, nll_loss=0.446, ppl=1.36, wps=13457.8, ups=1.77, wpb=7611.5, bsz=531.6, num_updates=4300, lr=0.000482243, gnorm=0.531, loss_scale=16, train_wall=56, gb_free=5.1, wall=4457]2022-08-29 07:48:37 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 033 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.64s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.44s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.47s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.43s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.47s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.49s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.54s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.61s/it]\u001b[A\n",
            "epoch 033 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.48s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:48:51 | INFO | valid | epoch 033 | valid on 'valid' subset | loss 6.753 | nll_loss 5.111 | ppl 34.57 | bleu 29.59 | wps 2062.3 | wpb 3067.4 | bsz 222.2 | num_updates 4320 | best_loss 6.677\n",
            "2022-08-29 07:48:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 33 @ 4320 updates\n",
            "2022-08-29 07:48:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-29 07:49:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint33.pt\n",
            "2022-08-29 07:49:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint33.pt (epoch 33 @ 4320 updates, score 6.753) (writing took 31.527519997000127 seconds)\n",
            "2022-08-29 07:49:22 | INFO | fairseq_cli.train | end of epoch 33 (average epoch stats below)\n",
            "2022-08-29 07:49:22 | INFO | train | epoch 033 | loss 2.589 | nll_loss 0.454 | ppl 1.37 | wps 8364.2 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 4320 | lr 0.000481125 | gnorm 0.55 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 4513\n",
            "2022-08-29 07:49:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 034:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:49:23 | INFO | fairseq.trainer | begin training epoch 34\n",
            "2022-08-29 07:49:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 034:  99% 130/131 [01:13<00:00,  1.82it/s, loss=2.55, nll_loss=0.417, ppl=1.33, wps=7539.8, ups=0.98, wpb=7681, bsz=547.8, num_updates=4400, lr=0.000476731, gnorm=0.496, loss_scale=16, train_wall=56, gb_free=5.1, wall=4559]2022-08-29 07:50:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 034 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.66s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.56s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.61s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.68s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.63s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.63s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset:  89% 8/9 [00:13<00:01,  1.79s/it]\u001b[A\n",
            "epoch 034 | valid on 'valid' subset: 100% 9/9 [00:14<00:00,  1.56s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:50:51 | INFO | valid | epoch 034 | valid on 'valid' subset | loss 6.788 | nll_loss 5.161 | ppl 35.77 | bleu 30.37 | wps 1902.5 | wpb 3067.4 | bsz 222.2 | num_updates 4451 | best_loss 6.677\n",
            "2022-08-29 07:50:51 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 34 @ 4451 updates\n",
            "2022-08-29 07:50:51 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-29 07:51:04 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint34.pt\n",
            "2022-08-29 07:51:23 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint34.pt (epoch 34 @ 4451 updates, score 6.788) (writing took 31.892413935999684 seconds)\n",
            "2022-08-29 07:51:23 | INFO | fairseq_cli.train | end of epoch 34 (average epoch stats below)\n",
            "2022-08-29 07:51:23 | INFO | train | epoch 034 | loss 2.557 | nll_loss 0.427 | ppl 1.34 | wps 8275.8 | ups 1.09 | wpb 7605.3 | bsz 534.4 | num_updates 4451 | lr 0.000473992 | gnorm 0.515 | loss_scale 16 | train_wall 73 | gb_free 5.6 | wall 4634\n",
            "2022-08-29 07:51:23 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 035:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:51:23 | INFO | fairseq.trainer | begin training epoch 35\n",
            "2022-08-29 07:51:23 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 035:  99% 130/131 [01:13<00:00,  1.86it/s, loss=2.55, nll_loss=0.422, ppl=1.34, wps=7332.2, ups=0.97, wpb=7538.5, bsz=527.6, num_updates=4500, lr=0.000471405, gnorm=0.522, loss_scale=16, train_wall=55, gb_free=5.1, wall=4662]2022-08-29 07:52:36 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 035 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.63s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  22% 2/9 [00:02<00:09,  1.41s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.47s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.43s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.43s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.43s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.46s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset:  89% 8/9 [00:11<00:01,  1.49s/it]\u001b[A\n",
            "epoch 035 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.35s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:52:49 | INFO | valid | epoch 035 | valid on 'valid' subset | loss 6.736 | nll_loss 5.108 | ppl 34.48 | bleu 31.06 | wps 2193.2 | wpb 3067.4 | bsz 222.2 | num_updates 4582 | best_loss 6.677\n",
            "2022-08-29 07:52:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 35 @ 4582 updates\n",
            "2022-08-29 07:52:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-29 07:53:03 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint35.pt\n",
            "2022-08-29 07:53:22 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint35.pt (epoch 35 @ 4582 updates, score 6.736) (writing took 32.479929684000126 seconds)\n",
            "2022-08-29 07:53:22 | INFO | fairseq_cli.train | end of epoch 35 (average epoch stats below)\n",
            "2022-08-29 07:53:22 | INFO | train | epoch 035 | loss 2.529 | nll_loss 0.403 | ppl 1.32 | wps 8369.1 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 4582 | lr 0.000467167 | gnorm 0.491 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 4753\n",
            "2022-08-29 07:53:22 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 036:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:53:22 | INFO | fairseq.trainer | begin training epoch 36\n",
            "2022-08-29 07:53:22 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 036:  99% 130/131 [01:13<00:00,  1.84it/s, loss=2.503, nll_loss=0.383, ppl=1.3, wps=13659.8, ups=1.77, wpb=7697.5, bsz=542.8, num_updates=4700, lr=0.000461266, gnorm=0.466, loss_scale=16, train_wall=56, gb_free=5, wall=4819]2022-08-29 07:54:35 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 036 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.65s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.45s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.47s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.47s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.50s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.65s/it]\u001b[A\n",
            "epoch 036 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.47s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:54:49 | INFO | valid | epoch 036 | valid on 'valid' subset | loss 6.774 | nll_loss 5.167 | ppl 35.94 | bleu 29.96 | wps 2070.3 | wpb 3067.4 | bsz 222.2 | num_updates 4713 | best_loss 6.677\n",
            "2022-08-29 07:54:49 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 36 @ 4713 updates\n",
            "2022-08-29 07:54:49 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-29 07:55:02 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint36.pt\n",
            "2022-08-29 07:55:18 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint36.pt (epoch 36 @ 4713 updates, score 6.774) (writing took 29.35590078899986 seconds)\n",
            "2022-08-29 07:55:18 | INFO | fairseq_cli.train | end of epoch 36 (average epoch stats below)\n",
            "2022-08-29 07:55:18 | INFO | train | epoch 036 | loss 2.505 | nll_loss 0.384 | ppl 1.3 | wps 8557.5 | ups 1.13 | wpb 7605.3 | bsz 534.4 | num_updates 4713 | lr 0.000460629 | gnorm 0.469 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 4869\n",
            "2022-08-29 07:55:18 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 037:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:55:18 | INFO | fairseq.trainer | begin training epoch 37\n",
            "2022-08-29 07:55:18 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 037:  99% 130/131 [01:13<00:00,  1.86it/s, loss=2.48, nll_loss=0.36, ppl=1.28, wps=7577.7, ups=1, wpb=7560.1, bsz=526.4, num_updates=4800, lr=0.000456435, gnorm=0.456, loss_scale=16, train_wall=56, gb_free=5.1, wall=4919]2022-08-29 07:56:32 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 037 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.63s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  22% 2/9 [00:02<00:09,  1.41s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.44s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  44% 4/9 [00:05<00:06,  1.40s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.41s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.42s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.46s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset:  89% 8/9 [00:11<00:01,  1.52s/it]\u001b[A\n",
            "epoch 037 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.38s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:56:45 | INFO | valid | epoch 037 | valid on 'valid' subset | loss 6.752 | nll_loss 5.143 | ppl 35.33 | bleu 31.23 | wps 2182.4 | wpb 3067.4 | bsz 222.2 | num_updates 4844 | best_loss 6.677\n",
            "2022-08-29 07:56:45 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 37 @ 4844 updates\n",
            "2022-08-29 07:56:45 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-29 07:56:59 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint37.pt\n",
            "2022-08-29 07:57:16 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint37.pt (epoch 37 @ 4844 updates, score 6.752) (writing took 30.956360047999624 seconds)\n",
            "2022-08-29 07:57:16 | INFO | fairseq_cli.train | end of epoch 37 (average epoch stats below)\n",
            "2022-08-29 07:57:16 | INFO | train | epoch 037 | loss 2.486 | nll_loss 0.369 | ppl 1.29 | wps 8458.9 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 4844 | lr 0.000454358 | gnorm 0.46 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 4987\n",
            "2022-08-29 07:57:16 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 038:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:57:16 | INFO | fairseq.trainer | begin training epoch 38\n",
            "2022-08-29 07:57:16 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 038:  99% 130/131 [01:13<00:00,  1.83it/s, loss=2.476, nll_loss=0.362, ppl=1.29, wps=7642.6, ups=1, wpb=7661.9, bsz=543.6, num_updates=4900, lr=0.000451754, gnorm=0.446, loss_scale=16, train_wall=55, gb_free=5.1, wall=5019]2022-08-29 07:58:30 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 038 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.51s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.46s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.47s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.47s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.52s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.58s/it]\u001b[A\n",
            "epoch 038 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 07:58:43 | INFO | valid | epoch 038 | valid on 'valid' subset | loss 6.766 | nll_loss 5.179 | ppl 36.22 | bleu 31.16 | wps 2106.6 | wpb 3067.4 | bsz 222.2 | num_updates 4975 | best_loss 6.677\n",
            "2022-08-29 07:58:43 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 38 @ 4975 updates\n",
            "2022-08-29 07:58:43 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-29 07:58:56 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint38.pt\n",
            "2022-08-29 07:59:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint38.pt (epoch 38 @ 4975 updates, score 6.766) (writing took 30.895316622999417 seconds)\n",
            "2022-08-29 07:59:14 | INFO | fairseq_cli.train | end of epoch 38 (average epoch stats below)\n",
            "2022-08-29 07:59:14 | INFO | train | epoch 038 | loss 2.464 | nll_loss 0.352 | ppl 1.28 | wps 8434.3 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 4975 | lr 0.000448336 | gnorm 0.435 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 5105\n",
            "2022-08-29 07:59:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 039:   0% 0/131 [00:00<?, ?it/s]2022-08-29 07:59:14 | INFO | fairseq.trainer | begin training epoch 39\n",
            "2022-08-29 07:59:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 039:  99% 130/131 [01:13<00:00,  1.85it/s, loss=2.445, nll_loss=0.339, ppl=1.26, wps=13541, ups=1.78, wpb=7619.4, bsz=538, num_updates=5100, lr=0.000442807, gnorm=0.409, loss_scale=16, train_wall=56, gb_free=5, wall=5176]2022-08-29 08:00:28 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 039 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  11% 1/9 [00:02<00:16,  2.00s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.60s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.57s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.51s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.51s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.56s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.60s/it]\u001b[A\n",
            "epoch 039 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:00:42 | INFO | valid | epoch 039 | valid on 'valid' subset | loss 6.759 | nll_loss 5.176 | ppl 36.16 | bleu 29.78 | wps 2079.1 | wpb 3067.4 | bsz 222.2 | num_updates 5106 | best_loss 6.677\n",
            "2022-08-29 08:00:42 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 39 @ 5106 updates\n",
            "2022-08-29 08:00:42 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-29 08:00:55 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint39.pt\n",
            "2022-08-29 08:01:14 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint39.pt (epoch 39 @ 5106 updates, score 6.759) (writing took 31.74941532899993 seconds)\n",
            "2022-08-29 08:01:14 | INFO | fairseq_cli.train | end of epoch 39 (average epoch stats below)\n",
            "2022-08-29 08:01:14 | INFO | train | epoch 039 | loss 2.444 | nll_loss 0.336 | ppl 1.26 | wps 8324.9 | ups 1.09 | wpb 7605.3 | bsz 534.4 | num_updates 5106 | lr 0.000442547 | gnorm 0.412 | loss_scale 16 | train_wall 73 | gb_free 5.7 | wall 5225\n",
            "2022-08-29 08:01:14 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 040:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:01:14 | INFO | fairseq.trainer | begin training epoch 40\n",
            "2022-08-29 08:01:14 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 040:  99% 130/131 [01:13<00:00,  1.85it/s, loss=2.428, nll_loss=0.321, ppl=1.25, wps=7459.7, ups=0.98, wpb=7618.6, bsz=539.7, num_updates=5200, lr=0.000438529, gnorm=0.403, loss_scale=16, train_wall=56, gb_free=5.1, wall=5278]2022-08-29 08:02:27 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 040 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.46s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.50s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.46s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.47s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.47s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.51s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.57s/it]\u001b[A\n",
            "epoch 040 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.41s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:02:41 | INFO | valid | epoch 040 | valid on 'valid' subset | loss 6.821 | nll_loss 5.243 | ppl 37.87 | bleu 30.75 | wps 2117.6 | wpb 3067.4 | bsz 222.2 | num_updates 5237 | best_loss 6.677\n",
            "2022-08-29 08:02:41 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 40 @ 5237 updates\n",
            "2022-08-29 08:02:41 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-29 08:02:54 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint40.pt\n",
            "2022-08-29 08:03:10 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint40.pt (epoch 40 @ 5237 updates, score 6.821) (writing took 29.42417367500002 seconds)\n",
            "2022-08-29 08:03:10 | INFO | fairseq_cli.train | end of epoch 40 (average epoch stats below)\n",
            "2022-08-29 08:03:10 | INFO | train | epoch 040 | loss 2.433 | nll_loss 0.329 | ppl 1.26 | wps 8554 | ups 1.12 | wpb 7605.3 | bsz 534.4 | num_updates 5237 | lr 0.000436977 | gnorm 0.406 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 5341\n",
            "2022-08-29 08:03:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 041:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:03:11 | INFO | fairseq.trainer | begin training epoch 41\n",
            "2022-08-29 08:03:11 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 041:  99% 130/131 [01:13<00:00,  1.84it/s, loss=2.422, nll_loss=0.319, ppl=1.25, wps=7678.3, ups=1.01, wpb=7628.9, bsz=531, num_updates=5300, lr=0.000434372, gnorm=0.39, loss_scale=16, train_wall=56, gb_free=5.1, wall=5377]2022-08-29 08:04:24 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 041 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.70s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.47s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.50s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  44% 4/9 [00:06<00:08,  1.63s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  56% 5/9 [00:07<00:06,  1.57s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.53s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.55s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.62s/it]\u001b[A\n",
            "epoch 041 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.53s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:04:38 | INFO | valid | epoch 041 | valid on 'valid' subset | loss 6.807 | nll_loss 5.256 | ppl 38.22 | bleu 30.97 | wps 1988.1 | wpb 3067.4 | bsz 222.2 | num_updates 5368 | best_loss 6.677\n",
            "2022-08-29 08:04:38 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 41 @ 5368 updates\n",
            "2022-08-29 08:04:38 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-29 08:04:52 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint41.pt\n",
            "2022-08-29 08:05:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint41.pt (epoch 41 @ 5368 updates, score 6.807) (writing took 31.421199730000808 seconds)\n",
            "2022-08-29 08:05:09 | INFO | fairseq_cli.train | end of epoch 41 (average epoch stats below)\n",
            "2022-08-29 08:05:10 | INFO | train | epoch 041 | loss 2.418 | nll_loss 0.317 | ppl 1.25 | wps 8356.7 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 5368 | lr 0.000431612 | gnorm 0.39 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 5460\n",
            "2022-08-29 08:05:10 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 042:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:05:10 | INFO | fairseq.trainer | begin training epoch 42\n",
            "2022-08-29 08:05:10 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 042:  99% 130/131 [01:13<00:00,  1.84it/s, loss=2.42, nll_loss=0.321, ppl=1.25, wps=7475.8, ups=0.99, wpb=7571, bsz=533.4, num_updates=5400, lr=0.000430331, gnorm=0.396, loss_scale=16, train_wall=55, gb_free=5.1, wall=5479]2022-08-29 08:06:23 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 042 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.68s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.44s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.42s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.43s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.43s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.48s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset:  89% 8/9 [00:11<00:01,  1.53s/it]\u001b[A\n",
            "epoch 042 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.37s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:06:36 | INFO | valid | epoch 042 | valid on 'valid' subset | loss 6.825 | nll_loss 5.289 | ppl 39.11 | bleu 31.62 | wps 2174.6 | wpb 3067.4 | bsz 222.2 | num_updates 5499 | best_loss 6.677\n",
            "2022-08-29 08:06:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 42 @ 5499 updates\n",
            "2022-08-29 08:06:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-29 08:06:50 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint42.pt\n",
            "2022-08-29 08:07:09 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint42.pt (epoch 42 @ 5499 updates, score 6.825) (writing took 32.5338355519998 seconds)\n",
            "2022-08-29 08:07:09 | INFO | fairseq_cli.train | end of epoch 42 (average epoch stats below)\n",
            "2022-08-29 08:07:09 | INFO | train | epoch 042 | loss 2.407 | nll_loss 0.309 | ppl 1.24 | wps 8355.3 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 5499 | lr 0.00042644 | gnorm 0.376 | loss_scale 16 | train_wall 73 | gb_free 5.6 | wall 5580\n",
            "2022-08-29 08:07:09 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 043:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:07:09 | INFO | fairseq.trainer | begin training epoch 43\n",
            "2022-08-29 08:07:09 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 043:  99% 130/131 [01:13<00:00,  1.84it/s, loss=2.392, nll_loss=0.296, ppl=1.23, wps=13551.6, ups=1.77, wpb=7635.2, bsz=542.2, num_updates=5600, lr=0.000422577, gnorm=0.375, loss_scale=16, train_wall=56, gb_free=5.2, wall=5637]2022-08-29 08:08:22 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 043 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.65s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  22% 2/9 [00:02<00:09,  1.43s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.46s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.41s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.43s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.53s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.55s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.59s/it]\u001b[A\n",
            "epoch 043 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.42s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:08:36 | INFO | valid | epoch 043 | valid on 'valid' subset | loss 6.834 | nll_loss 5.279 | ppl 38.83 | bleu 31.59 | wps 2109.3 | wpb 3067.4 | bsz 222.2 | num_updates 5630 | best_loss 6.677\n",
            "2022-08-29 08:08:36 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 43 @ 5630 updates\n",
            "2022-08-29 08:08:36 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-29 08:08:49 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint43.pt\n",
            "2022-08-29 08:09:06 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint43.pt (epoch 43 @ 5630 updates, score 6.834) (writing took 30.399405520000073 seconds)\n",
            "2022-08-29 08:09:06 | INFO | fairseq_cli.train | end of epoch 43 (average epoch stats below)\n",
            "2022-08-29 08:09:07 | INFO | train | epoch 043 | loss 2.4 | nll_loss 0.306 | ppl 1.24 | wps 8459 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 5630 | lr 0.00042145 | gnorm 0.383 | loss_scale 16 | train_wall 73 | gb_free 5.8 | wall 5697\n",
            "2022-08-29 08:09:07 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 044:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:09:07 | INFO | fairseq.trainer | begin training epoch 44\n",
            "2022-08-29 08:09:07 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 044:  99% 130/131 [01:13<00:00,  1.82it/s, loss=2.393, nll_loss=0.298, ppl=1.23, wps=7550, ups=0.99, wpb=7599, bsz=517.2, num_updates=5700, lr=0.000418854, gnorm=0.373, loss_scale=16, train_wall=56, gb_free=5.2, wall=5738]2022-08-29 08:10:20 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 044 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.71s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.43s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.44s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.44s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.48s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset:  89% 8/9 [00:11<00:01,  1.51s/it]\u001b[A\n",
            "epoch 044 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.35s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:10:33 | INFO | valid | epoch 044 | valid on 'valid' subset | loss 6.812 | nll_loss 5.276 | ppl 38.75 | bleu 32.47 | wps 2190.3 | wpb 3067.4 | bsz 222.2 | num_updates 5761 | best_loss 6.677\n",
            "2022-08-29 08:10:33 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 44 @ 5761 updates\n",
            "2022-08-29 08:10:33 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-29 08:10:47 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint44.pt\n",
            "2022-08-29 08:11:04 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint44.pt (epoch 44 @ 5761 updates, score 6.812) (writing took 30.642955940999855 seconds)\n",
            "2022-08-29 08:11:04 | INFO | fairseq_cli.train | end of epoch 44 (average epoch stats below)\n",
            "2022-08-29 08:11:04 | INFO | train | epoch 044 | loss 2.388 | nll_loss 0.295 | ppl 1.23 | wps 8476.8 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 5761 | lr 0.000416631 | gnorm 0.365 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 5815\n",
            "2022-08-29 08:11:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 045:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:11:04 | INFO | fairseq.trainer | begin training epoch 45\n",
            "2022-08-29 08:11:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 045:  99% 130/131 [01:13<00:00,  1.84it/s, loss=2.386, nll_loss=0.295, ppl=1.23, wps=7621.7, ups=1, wpb=7601.4, bsz=540.7, num_updates=5800, lr=0.000415227, gnorm=0.363, loss_scale=16, train_wall=55, gb_free=5.1, wall=5838]2022-08-29 08:12:18 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 045 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.69s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.48s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.42s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.43s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.45s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.50s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.64s/it]\u001b[A\n",
            "epoch 045 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.45s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:12:32 | INFO | valid | epoch 045 | valid on 'valid' subset | loss 6.793 | nll_loss 5.242 | ppl 37.84 | bleu 31.4 | wps 2100.3 | wpb 3067.4 | bsz 222.2 | num_updates 5892 | best_loss 6.677\n",
            "2022-08-29 08:12:32 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 45 @ 5892 updates\n",
            "2022-08-29 08:12:32 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-29 08:12:45 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint45.pt\n",
            "2022-08-29 08:13:03 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint45.pt (epoch 45 @ 5892 updates, score 6.793) (writing took 31.776836473999538 seconds)\n",
            "2022-08-29 08:13:03 | INFO | fairseq_cli.train | end of epoch 45 (average epoch stats below)\n",
            "2022-08-29 08:13:03 | INFO | train | epoch 045 | loss 2.38 | nll_loss 0.291 | ppl 1.22 | wps 8346.4 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 5892 | lr 0.000411973 | gnorm 0.362 | loss_scale 16 | train_wall 73 | gb_free 5.6 | wall 5934\n",
            "2022-08-29 08:13:04 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 046:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:13:04 | INFO | fairseq.trainer | begin training epoch 46\n",
            "2022-08-29 08:13:04 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 046:  99% 130/131 [01:13<00:00,  1.84it/s, loss=2.368, nll_loss=0.28, ppl=1.21, wps=13467.2, ups=1.77, wpb=7610.9, bsz=530.2, num_updates=6000, lr=0.000408248, gnorm=0.342, loss_scale=16, train_wall=56, gb_free=5.1, wall=5996]2022-08-29 08:14:17 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 046 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.65s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  22% 2/9 [00:02<00:10,  1.45s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.49s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.43s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.45s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.46s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.51s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.55s/it]\u001b[A\n",
            "epoch 046 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.38s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:14:31 | INFO | valid | epoch 046 | valid on 'valid' subset | loss 6.803 | nll_loss 5.276 | ppl 38.73 | bleu 31.91 | wps 2141.5 | wpb 3067.4 | bsz 222.2 | num_updates 6023 | best_loss 6.677\n",
            "2022-08-29 08:14:31 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 46 @ 6023 updates\n",
            "2022-08-29 08:14:31 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-29 08:14:44 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint46.pt\n",
            "2022-08-29 08:15:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint46.pt (epoch 46 @ 6023 updates, score 6.803) (writing took 31.823788540999885 seconds)\n",
            "2022-08-29 08:15:02 | INFO | fairseq_cli.train | end of epoch 46 (average epoch stats below)\n",
            "2022-08-29 08:15:03 | INFO | train | epoch 046 | loss 2.372 | nll_loss 0.285 | ppl 1.22 | wps 8369.7 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 6023 | lr 0.000407468 | gnorm 0.348 | loss_scale 16 | train_wall 73 | gb_free 5.4 | wall 6053\n",
            "2022-08-29 08:15:03 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 047:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:15:03 | INFO | fairseq.trainer | begin training epoch 47\n",
            "2022-08-29 08:15:03 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 047:  99% 130/131 [01:13<00:00,  1.83it/s, loss=2.364, nll_loss=0.277, ppl=1.21, wps=7510.5, ups=0.98, wpb=7631.6, bsz=521.6, num_updates=6100, lr=0.000404888, gnorm=0.344, loss_scale=16, train_wall=56, gb_free=5, wall=6098]2022-08-29 08:16:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 047 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.96s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.71s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  33% 3/9 [00:05<00:10,  1.71s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.57s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  56% 5/9 [00:08<00:06,  1.53s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.50s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  78% 7/9 [00:11<00:03,  1.53s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.57s/it]\u001b[A\n",
            "epoch 047 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.40s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:16:30 | INFO | valid | epoch 047 | valid on 'valid' subset | loss 6.854 | nll_loss 5.335 | ppl 40.37 | bleu 32.26 | wps 2077.5 | wpb 3067.4 | bsz 222.2 | num_updates 6154 | best_loss 6.677\n",
            "2022-08-29 08:16:30 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 47 @ 6154 updates\n",
            "2022-08-29 08:16:30 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-29 08:16:43 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint47.pt\n",
            "2022-08-29 08:17:02 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint47.pt (epoch 47 @ 6154 updates, score 6.854) (writing took 32.02449981100017 seconds)\n",
            "2022-08-29 08:17:02 | INFO | fairseq_cli.train | end of epoch 47 (average epoch stats below)\n",
            "2022-08-29 08:17:02 | INFO | train | epoch 047 | loss 2.365 | nll_loss 0.279 | ppl 1.21 | wps 8335.9 | ups 1.1 | wpb 7605.3 | bsz 534.4 | num_updates 6154 | lr 0.000403108 | gnorm 0.346 | loss_scale 16 | train_wall 73 | gb_free 5.9 | wall 6173\n",
            "2022-08-29 08:17:02 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 048:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:17:02 | INFO | fairseq.trainer | begin training epoch 48\n",
            "2022-08-29 08:17:02 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 048:  99% 130/131 [01:13<00:00,  1.85it/s, loss=2.364, nll_loss=0.28, ppl=1.21, wps=7401.5, ups=0.98, wpb=7531.9, bsz=543.9, num_updates=6200, lr=0.00040161, gnorm=0.351, loss_scale=16, train_wall=55, gb_free=5.8, wall=6199]2022-08-29 08:18:16 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 048 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  11% 1/9 [00:01<00:15,  1.95s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  22% 2/9 [00:03<00:10,  1.55s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.55s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.49s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.49s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.48s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.52s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.56s/it]\u001b[A\n",
            "epoch 048 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.40s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:18:29 | INFO | valid | epoch 048 | valid on 'valid' subset | loss 6.825 | nll_loss 5.314 | ppl 39.79 | bleu 31.9 | wps 2134 | wpb 3067.4 | bsz 222.2 | num_updates 6285 | best_loss 6.677\n",
            "2022-08-29 08:18:29 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 48 @ 6285 updates\n",
            "2022-08-29 08:18:29 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-29 08:18:42 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint48.pt\n",
            "2022-08-29 08:18:59 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint48.pt (epoch 48 @ 6285 updates, score 6.825) (writing took 30.008765079999648 seconds)\n",
            "2022-08-29 08:18:59 | INFO | fairseq_cli.train | end of epoch 48 (average epoch stats below)\n",
            "2022-08-29 08:18:59 | INFO | train | epoch 048 | loss 2.36 | nll_loss 0.277 | ppl 1.21 | wps 8493.4 | ups 1.12 | wpb 7605.3 | bsz 534.4 | num_updates 6285 | lr 0.000398885 | gnorm 0.348 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 6290\n",
            "2022-08-29 08:18:59 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 049:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:19:00 | INFO | fairseq.trainer | begin training epoch 49\n",
            "2022-08-29 08:19:00 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 049:  99% 130/131 [01:13<00:00,  1.83it/s, loss=2.351, nll_loss=0.271, ppl=1.21, wps=13530.5, ups=1.78, wpb=7612.5, bsz=531.3, num_updates=6400, lr=0.000395285, gnorm=0.336, loss_scale=16, train_wall=56, gb_free=5, wall=6355]2022-08-29 08:20:13 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 049 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.64s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  22% 2/9 [00:02<00:09,  1.42s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  33% 3/9 [00:04<00:08,  1.45s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  44% 4/9 [00:05<00:07,  1.41s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.42s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  67% 6/9 [00:08<00:04,  1.44s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  78% 7/9 [00:10<00:02,  1.49s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset:  89% 8/9 [00:11<00:01,  1.54s/it]\u001b[A\n",
            "epoch 049 | valid on 'valid' subset: 100% 9/9 [00:12<00:00,  1.39s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:20:26 | INFO | valid | epoch 049 | valid on 'valid' subset | loss 6.899 | nll_loss 5.392 | ppl 41.98 | bleu 32.16 | wps 2159.6 | wpb 3067.4 | bsz 222.2 | num_updates 6416 | best_loss 6.677\n",
            "2022-08-29 08:20:26 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 49 @ 6416 updates\n",
            "2022-08-29 08:20:26 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-29 08:20:40 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint49.pt\n",
            "2022-08-29 08:20:57 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint49.pt (epoch 49 @ 6416 updates, score 6.899) (writing took 30.68189847099984 seconds)\n",
            "2022-08-29 08:20:57 | INFO | fairseq_cli.train | end of epoch 49 (average epoch stats below)\n",
            "2022-08-29 08:20:57 | INFO | train | epoch 049 | loss 2.352 | nll_loss 0.271 | ppl 1.21 | wps 8476.9 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 6416 | lr 0.000394792 | gnorm 0.334 | loss_scale 16 | train_wall 73 | gb_free 5.6 | wall 6408\n",
            "2022-08-29 08:20:57 | INFO | fairseq.data.iterators | grouped total_num_itrs = 131\n",
            "epoch 050:   0% 0/131 [00:00<?, ?it/s]2022-08-29 08:20:57 | INFO | fairseq.trainer | begin training epoch 50\n",
            "2022-08-29 08:20:57 | INFO | fairseq_cli.train | Start iterating over samples\n",
            "epoch 050:  99% 130/131 [01:13<00:00,  1.86it/s, loss=2.343, nll_loss=0.262, ppl=1.2, wps=7580.9, ups=1, wpb=7616.8, bsz=547.3, num_updates=6500, lr=0.000392232, gnorm=0.329, loss_scale=16, train_wall=56, gb_free=5.1, wall=6456]2022-08-29 08:22:11 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 050 | valid on 'valid' subset:   0% 0/9 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  11% 1/9 [00:01<00:13,  1.69s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  22% 2/9 [00:03<00:11,  1.60s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  33% 3/9 [00:04<00:09,  1.58s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  44% 4/9 [00:06<00:07,  1.51s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  56% 5/9 [00:07<00:05,  1.50s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  67% 6/9 [00:09<00:04,  1.50s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  78% 7/9 [00:10<00:03,  1.53s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset:  89% 8/9 [00:12<00:01,  1.56s/it]\u001b[A\n",
            "epoch 050 | valid on 'valid' subset: 100% 9/9 [00:13<00:00,  1.40s/it]\u001b[A\n",
            "                                                                      \u001b[A2022-08-29 08:22:24 | INFO | valid | epoch 050 | valid on 'valid' subset | loss 6.89 | nll_loss 5.386 | ppl 41.83 | bleu 31.67 | wps 2080.8 | wpb 3067.4 | bsz 222.2 | num_updates 6547 | best_loss 6.677\n",
            "2022-08-29 08:22:24 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 50 @ 6547 updates\n",
            "2022-08-29 08:22:24 | INFO | fairseq.trainer | Saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-29 08:22:37 | INFO | fairseq.trainer | Finished saving checkpoint to /content/checkpoints/model/checkpoint50.pt\n",
            "2022-08-29 08:22:55 | INFO | fairseq.checkpoint_utils | Saved checkpoint checkpoints/model/checkpoint50.pt (epoch 50 @ 6547 updates, score 6.89) (writing took 31.18201432099977 seconds)\n",
            "2022-08-29 08:22:55 | INFO | fairseq_cli.train | end of epoch 50 (average epoch stats below)\n",
            "2022-08-29 08:22:55 | INFO | train | epoch 050 | loss 2.346 | nll_loss 0.267 | ppl 1.2 | wps 8404.4 | ups 1.11 | wpb 7605.3 | bsz 534.4 | num_updates 6547 | lr 0.000390822 | gnorm 0.331 | loss_scale 16 | train_wall 73 | gb_free 5.5 | wall 6526\n",
            "2022-08-29 08:22:56 | INFO | fairseq_cli.train | done training in 6526.4 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T19:16:00.904977Z",
          "iopub.execute_input": "2022-07-24T19:16:00.905444Z",
          "iopub.status.idle": "2022-07-24T19:16:01.196986Z",
          "shell.execute_reply.started": "2022-07-24T19:16:00.905403Z",
          "shell.execute_reply": "2022-07-24T19:16:01.195842Z"
        },
        "trusted": true,
        "id": "yL7nIungdwcw",
        "outputId": "6dc9670f-64fe-4982-c66c-1dfad6931937",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mcheckpoints\u001b[0m/  \u001b[01;34mdata-bin\u001b[0m/  \u001b[01;34mdrive\u001b[0m/  \u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/drive/valid_hsb-de.hsb --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang hsb --target-lang de \\\n",
        "      --beam 5 data-bin/wmt_hsb_de | grep -P \"D-[0-9]+\" | cut -f3 > target.txt)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:05:22.918437Z",
          "iopub.execute_input": "2022-07-24T18:05:22.919517Z",
          "iopub.status.idle": "2022-07-24T18:06:13.344631Z",
          "shell.execute_reply.started": "2022-07-24T18:05:22.919454Z",
          "shell.execute_reply": "2022-07-24T18:06:13.343313Z"
        },
        "trusted": true,
        "id": "PdGwekMXdwcx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b943c91-9e4e-4d13-f7f9-ac3a8a2f2add"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 08:23:09 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/drive/valid_hsb-de.hsb'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_hsb_de', 'source_lang': 'hsb', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-29 08:23:09 | INFO | fairseq.tasks.translation | [hsb] dictionary: 136232 types\n",
            "2022-08-29 08:23:09 | INFO | fairseq.tasks.translation | [de] dictionary: 105688 types\n",
            "2022-08-29 08:23:09 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-29 08:23:34 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-29 08:23:34 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-29 08:23:34 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-29 08:23:58 | INFO | fairseq_cli.interactive | Total time: 49.316 seconds; translation time: 19.881\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu target.txt -i /content/drive/valid_hsb-de.de -l hsb-de -m bleu chrf ter)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-07-24T18:06:16.590743Z",
          "iopub.execute_input": "2022-07-24T18:06:16.591679Z",
          "iopub.status.idle": "2022-07-24T18:06:19.209461Z",
          "shell.execute_reply.started": "2022-07-24T18:06:16.591625Z",
          "shell.execute_reply": "2022-07-24T18:06:19.208405Z"
        },
        "trusted": true,
        "id": "bqOJKYxXdwc2",
        "outputId": "a4bd0edc-4622-4833-da52-4023d1ab4a66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 28.7,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.2.0\",\n",
            " \"verbose_score\": \"61.5/36.0/23.0/15.3 (BP = 0.965 ratio = 0.966 hyp_len = 29009 ref_len = 30029)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"chrF2\",\n",
            " \"score\": 55.1,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:yes|nc:6|nw:0|space:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"yes\",\n",
            " \"nc\": \"6\",\n",
            " \"nw\": \"0\",\n",
            " \"space\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "},\n",
            "{\n",
            " \"name\": \"TER\",\n",
            " \"score\": 53.7,\n",
            " \"signature\": \"nrefs:1|case:lc|tok:tercom|norm:no|punct:yes|asian:no|version:2.2.0\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"lc\",\n",
            " \"tok\": \"tercom\",\n",
            " \"norm\": \"no\",\n",
            " \"punct\": \"yes\",\n",
            " \"asian\": \"no\",\n",
            " \"version\": \"2.2.0\"\n",
            "}\n",
            "]\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# downloading blind test data\n",
        "!gdown --id 14oKRgNCOYmHYFO9kv6dDI6kP3X7fTKBz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD0Zhyg5Mmtd",
        "outputId": "6ef4008c-88a3-446d-d11b-a51b92b52ecf"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14oKRgNCOYmHYFO9kv6dDI6kP3X7fTKBz\n",
            "To: /content/test_DE-HSB.hsb_src.txt\n",
            "100% 133k/133k [00:00<00:00, 70.2MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!(fairseq-interactive --input=/content/test_DE-HSB.hsb_src.txt --path checkpoints/model/checkpoint_best.pt \\\n",
        "      --buffer-size 2000 --max-tokens 4096 --source-lang hsb --target-lang de \\\n",
        "      --beam 5 data-bin/wmt_hsb_de | grep -P \"D-[0-9]+\" | cut -f3 > /content/gdrive/MyDrive/target_hsb-de_sup_submission.txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr0yH7NZUF1B",
        "outputId": "e170e4e0-36e8-47bd-947f-5e327c5027af"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-08-29 08:24:07 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'checkpoints/model/checkpoint_best.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': 4096, 'batch_size': None, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': 4096, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 2000, 'input': '/content/test_DE-HSB.hsb_src.txt'}, 'model': None, 'task': {'_name': 'translation', 'data': 'data-bin/wmt_hsb_de', 'source_lang': 'hsb', 'target_lang': 'de', 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-08-29 08:24:07 | INFO | fairseq.tasks.translation | [hsb] dictionary: 136232 types\n",
            "2022-08-29 08:24:07 | INFO | fairseq.tasks.translation | [de] dictionary: 105688 types\n",
            "2022-08-29 08:24:07 | INFO | fairseq_cli.interactive | loading model(s) from checkpoints/model/checkpoint_best.pt\n",
            "2022-08-29 08:24:15 | INFO | fairseq_cli.interactive | Sentence buffer size: 2000\n",
            "2022-08-29 08:24:15 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-08-29 08:24:15 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-08-29 08:24:32 | INFO | fairseq_cli.interactive | Total time: 25.353 seconds; translation time: 13.276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !(sacrebleu target.txt -i /content/drive/dsb-hsb/valid_dsb-hsb/valid_dsb-hsb.hsb)"
      ],
      "metadata": {
        "id": "9Zw2XTYgbzdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!(sacrebleu /content/gdrive/MyDrive/target_dsb-hsb_sup_submission.txt -i /content/test_dsb_hsb.dsb_src.txt -l dsb-hsb -m bleu chrf ter)"
      ],
      "metadata": {
        "id": "7LNPRlDQb4ER"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}